{"meta":{"title":"滚~韬的博客","subtitle":"滚~","description":"一个普通大学生的博客","author":"滚~韬","url":"https://gwtt.github.io","root":"/"},"pages":[{"title":"关于这个博客","date":"2022-06-11T02:34:12.102Z","updated":"2022-06-11T02:34:12.102Z","comments":true,"path":"about/index.html","permalink":"https://gwtt.github.io/about/index.html","excerpt":"","text":"​ 自我高二学习C#以来，到了大二才开始创建自己的博客，也可能是分手的缘故，让我重新开始审视自己。我学习代码已有5年了，遗憾的是，已经不再是抱着做游戏心态去学习代码，而是为了现实学习后端。加油吧。"},{"title":"分类","date":"2022-06-11T02:24:29.000Z","updated":"2022-06-11T02:34:49.842Z","comments":true,"path":"category/index.html","permalink":"https://gwtt.github.io/category/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-06-11T02:24:27.000Z","updated":"2022-06-11T02:35:07.018Z","comments":true,"path":"tag/index.html","permalink":"https://gwtt.github.io/tag/index.html","excerpt":"","text":""}],"posts":[{"title":"Bean注解","slug":"Bean注解","date":"2022-11-04T05:35:36.000Z","updated":"2022-11-04T14:47:05.055Z","comments":true,"path":"2022/11/04/Bean注解/","link":"","permalink":"https://gwtt.github.io/2022/11/04/Bean%E6%B3%A8%E8%A7%A3/","excerpt":"","text":"因为实践周，这几周天天写代码，没有时间更新博客。学会了很多go相关的东西，下次补充。 什么是Bean这个概念网上也是老生常谈了。 Spring的@Bean注解用于告诉方法，产生一个Bean对象，然后这个Bean对象交给Spring管理。产生这个Bean对象的方法Spring只会调用一次，随后这个Spring将会将这个Bean对象放在自己的IOC容器中。 SpringIOC 容器管理一个或者多个bean，这些bean都需要在@Configuration注解下进行创建，在一个方法上使用@Bean注解就表明这个方法需要交给Spring进行管理。 案例: 123456789101112131415@Configurationpublic class AppConfig &#123; // 使用@Bean 注解表明myBean需要交给Spring进行管理 // 未指定bean 的名称，默认采用的是 &quot;方法名&quot; + &quot;首字母小写&quot;的配置方式 @Bean public MyBean myBean()&#123; return new MyBean(); &#125;&#125;public class MyBean &#123; public MyBean()&#123; System.out.println(&quot;MyBean Initializing&quot;); &#125;&#125; 12345678public class SpringBeanApplicationTests &#123; public static void main(String[] args) &#123; ApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class); context.getBean(&quot;myBean&quot;); &#125;&#125;//输出 MyBean Initializing 下面是实战经历代码 1234567891011121314151617@Configurationpublic class LocalTimeConfiguration &#123; @Bean public Jackson2ObjectMapperBuilderCustomizer jackson2ObjectMapperBuilderCustomizer() &#123; return builder -&gt; &#123; DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;); //返回时间数据序列化 builder.serializerByType(LocalDateTime.class, new LocalDateTimeSerializer(formatter)); //接收时间数据反序列化 builder.deserializerByType(LocalDateTime.class, new LocalDateTimeDeserializer(formatter)); &#125;; &#125;&#125; 我们可以看到返回了一个函数式接口 Bean的基本构成123456789101112131415161718192021@Target(&#123;ElementType.METHOD, ElementType.ANNOTATION_TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Bean &#123; @AliasFor(&quot;name&quot;) String[] value() default &#123;&#125;; @AliasFor(&quot;value&quot;) String[] name() default &#123;&#125;; /** @deprecated */ @Deprecated Autowire autowire() default Autowire.NO; boolean autowireCandidate() default true; String initMethod() default &quot;&quot;; String destroyMethod() default &quot;(inferred)&quot;;&#125; @Bean不仅可以作用在方法上，也可以作用在注解类型上，在运行时提供注册。 value： name属性的别名，在不需要其他属性时使用，也就是说value 就是默认值 name： 此bean 的名称，或多个名称，主要的bean的名称加别名。如果未指定，则bean的名称是带注解方法的名称。如果指定了，方法的名称就会忽略，如果没有其他属性声明的话，bean的名称和别名可能通过value属性配置 autowire ： 此注解的方法表示自动装配的类型，返回一个Autowire类型的枚举，我们来看一下Autowire枚举类型的概念 12345678910111213141516171819202122// 枚举确定自动装配状态：即，bean是否应该使用setter注入由Spring容器自动注入其依赖项。// 这是Spring DI的核心概念public enum Autowire &#123; // 常量，表示根本没有自动装配。 NO(AutowireCapableBeanFactory.AUTOWIRE_NO), // 常量，通过名称进行自动装配 BY_NAME(AutowireCapableBeanFactory.AUTOWIRE_BY_NAME), // 常量，通过类型进行自动装配 BY_TYPE(AutowireCapableBeanFactory.AUTOWIRE_BY_TYPE); private final int value; Autowire(int value) &#123; this.value = value; &#125; public int value() &#123; return this.value; &#125; public boolean isAutowire() &#123; return (this == BY_NAME || this == BY_TYPE); &#125;&#125; autowire的默认值为No，默认表示不通过自动装配。 initMethod: 这个可选择的方法在bean实例化的时候调用，InitializationBean接口允许bean在合适的时机通过设置注解的初始化属性从而调用初始化方法，InitializationBean 接口有一个定义好的初始化方法 1void afterPropertiesSet() throws Exception; Spring不推荐使用InitializationBean 来调用其初始化方法，因为它不必要地将代码耦合到Spring。Spring推荐使用@PostConstruct注解或者为POJO类指定其初始化方法这两种方式来完成初始化。 不推荐使用： 1234public class InitBean implements InitializingBean &#123; public void afterPropertiesSet() &#123;&#125;&#125; destroyMethod: 方法的可选择名称在调用bean示例在关闭上下文的时候，例如JDBC的close()方法，或者SqlSession的close()方法。DisposableBean 接口的实现允许在bean销毁的时候进行回调调用，DisposableBean 接口之后一个单个的方法 1void destroy() throws Exception; Spring不推荐使用DisposableBean 的方式来初始化其方法，因为它会将不必要的代码耦合到Spring。作为替代性的建议，Spring 推荐使用@PreDestory注解或者为@Bean注解提供 destroyMethod 属性， 不推荐使用： 123456789101112131415161718192021222324252627282930public class DestroyBean &#123; public void cleanup() &#123;&#125;&#125;推荐使用：public class MyBean &#123; public MyBean()&#123; System.out.println(&quot;MyBean Initializing&quot;); &#125; public void init()&#123; System.out.println(&quot;Bean 初始化方法被调用&quot;); &#125; public void destroy()&#123; System.out.println(&quot;Bean 销毁方法被调用&quot;); &#125;&#125;@Configurationpublic class AppConfig &#123;// @Bean @Bean(initMethod = &quot;init&quot;, destroyMethod = &quot;destroy&quot;) public MyBean myBean()&#123; return new MyBean(); &#125;&#125; 修改一下测试类，测试其初始化方法和销毁方法在何时会被调用 1234567891011121314public class SpringBeanApplicationTests &#123; public static void main(String[] args) &#123; // ------------------------------ 测试一 ------------------------------ ApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class); //context.getBean(&quot;myBean&quot;); // 变体 context.getBean(&quot;myBean&quot;); ((AnnotationConfigApplicationContext) context).destroy(); //((AnnotationConfigApplicationContext) context).close(); &#125;&#125; 初始化方法在得到Bean的实例的时候就会被调用，销毁方法在容器销毁或者容器关闭的时候会被调用。 Bean与其他注解的配合 @Profile 注解@Profile的作用是把一些meta-data进行分类，分成Active和InActive这两种状态，然后你可以选择在active 和在Inactive这两种状态下配置bean，在Inactive状态通常的注解有一个！操作符，通常写为：@Profile(“!p”),这里的p是Profile的名字。 三种设置方式： 可以通过ConfigurableEnvironment.setActiveProfiles()以编程的方式激活 可以通过AbstractEnvironment.ACTIVE_PROFILES_PROPERTY_NAME (spring.profiles.active )属性设置为 JVM属性 作为环境变量，或作为web.xml 应用程序的Servlet 上下文参数。也可以通过@ActiveProfiles 注解在集成测试中以声明方式激活配置文件。 作用域 作为类级别的注释在任意类或者直接与@Component 进行关联，包括@Configuration 类 作为原注解，可以自定义注解 作为方法的注解作用在任何方法 注意: 如果一个配置类使用了Profile 标签或者@Profile 作用在任何类中都必须进行启用才会生效，如果@Profile({“p1”,”!p2”}) 标识两个属性，那么p1 是启用状态 而p2 是非启用状态的。 现有一个POJO类为Subject学科类，里面有两个属性，一个是like(理科)属性，一个是wenke(文科)属性，分别有两个配置类，一个是AppConfigWithActiveProfile ，一个是AppConfigWithInactiveProfile，当系统环境是 “like”的时候就注册 AppConfigWithActiveProfile ，如果是 “wenke”，就注册 AppConfigWithInactiveProfile，来看一下这个需求如何实现 Subject.java 123456789101112131415161718&gt;// 学科&gt;public class Subject &#123; // 理科 private String like; // 文科 private String wenke; get and set ... @Override public String toString() &#123; return &quot;Subject&#123;&quot; + &quot;like=&#x27;&quot; + like + &#x27;\\&#x27;&#x27; + &quot;, wenke=&#x27;&quot; + wenke + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&gt;&#125; AppConfigWithActiveProfile.java 注册Profile 为like 的时候 123456789101112&gt;@Profile(&quot;like&quot;)&gt;@Configuration&gt;public class AppConfigWithActiveProfile &#123; @Bean public Subject subject()&#123; Subject subject = new Subject(); subject.setLike(&quot;物理&quot;); return subject; &#125;&gt;&#125; AppConfigWithInactiveProfile.java 注册Profile 为wenke 的时候 1234567891011&gt;@Profile(&quot;wenke&quot;)&gt;@Configuration&gt;public class AppConfigWithInactiveProfile &#123; @Bean public Subject subject()&#123; Subject subject = new Subject(); subject.setWenke(&quot;历史&quot;); return subject; &#125;&gt;&#125; 修改一下对应的测试类，设置系统环境，当Profile 为like 和 wenke 的时候分别注册各自对应的属性 12345678&gt;// ------------------------------ 测试 profile ------------------------------&gt;AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext();&gt;// 激活 like 的profile&gt;context.getEnvironment().setActiveProfiles(&quot;like&quot;);&gt;context.register(AppConfigWithActiveProfile.class,AppConfigWithInactiveProfile.class);&gt;context.refresh();&gt;Subject subject = (Subject) context.getBean(&quot;subject&quot;);&gt;System.out.println(&quot;subject = &quot; + subject); 把context.getEnvironment().setActiveProfiles(“wenke”) 设置为wenke，观察其对应的输出内容发生了变化，这就是@Profile的作用，有一层可选择性注册的意味。 @Scope 注解在Spring中对于bean的默认处理都是单例的，我们通过上下文容器.getBean方法拿到bean容器，并对其进行实例化，这个实例化的过程其实只进行一次，即多次getBean 获取的对象都是同一个对象，也就相当于这个bean的实例在IOC容器中是public的，对于所有的bean请求来讲都可以共享此bean。 那么假如我不想把这个bean被所有的请求共享或者说每次调用我都想让它生成一个bean实例该怎么处理呢？ 多例Bean bean的非单例原型范围会使每次发出对该特定bean的请求时都创建新的bean实例，也就是说，bean被注入另一个bean，或者通过对容器的getBean()方法调用来请求它，可以用如下图来表示： 通过一个示例来说明bean的多个实例 新建一个AppConfigWithAliasAndScope配置类，用来定义多例的bean， 1234567891011121314&gt;@Configuration&gt;public class AppConfigWithAliasAndScope &#123; /** * 为myBean起两个名字，b1 和 b2 * @Scope 默认为 singleton，但是可以指定其作用域 * prototype 是多例的，即每一次调用都会生成一个新的实例。 */ @Bean(&#123;&quot;b1&quot;,&quot;b2&quot;&#125;) @Scope(&quot;prototype&quot;) public MyBean myBean()&#123; return new MyBean(); &#125;&gt;&#125; 测试一下多例的情况： 123456&gt;// ------------------------------ 测试scope ------------------------------&gt;ApplicationContext context = new AnnotationConfigApplicationContext(AppConfigWithAliasAndScope.class);&gt;MyBean myBean = (MyBean) context.getBean(&quot;b1&quot;);&gt;MyBean myBean2 = (MyBean) context.getBean(&quot;b2&quot;);&gt;System.out.println(myBean);&gt;System.out.println(myBean2); 其他情况 除了多例的情况下，Spring还为我们定义了其他情况： Scope Descriptionn singleton 默认单例的bean定义信息，对于每个IOC容器来说都是单例对象 prototype bean对象的定义为任意数量的对象实例 request bean对象的定义为一次HTTP请求的生命周期，也就是说，每个HTTP请求都有自己的bean实例，它是在单个bean定义的后面创建的。仅仅在web-aware的上下文中有效 session bean对象的定义为一次HTTP会话的生命周期。仅仅在web-aware的上下文中有效 application bean对象的定义范围在ServletContext生命周期内。仅仅在web-aware的上下文中有效 websocket bean对象的定义为WebSocket的生命周期内。仅仅在web-aware的上下文中有效 singleton和prototype 一般都用在普通的Java项目中，而request、session、application、websocket都用于web应用中。 request、session、application、websocket的作用范围 你可以体会到 request、session、application、websocket 的作用范围在当你使用web-aware的ApplicationContext应用程序上下文的时候，比如XmlWebApplicationContext的实现类。如果你使用了像是ClassPathXmlApplicationContext的上下文环境时，就会抛出IllegalStateException因为Spring不认识这个作用范围。 @Lazy 注解@Lazy : 表明一个bean 是否延迟加载，可以作用在方法上，表示这个方法被延迟加载；可以作用在@Component (或者由@Component 作为原注解) 注释的类上，表明这个类中所有的bean 都被延迟加载。如果没有@Lazy注释，或者@Lazy 被设置为false，那么该bean 就会急切渴望被加载；除了上面两种作用域，@Lazy 还可以作用在@Autowired和@Inject注释的属性上，在这种情况下，它将为该字段创建一个惰性代理，作为使用ObjectFactory或Provider的默认方法。下面来演示一下： 1234567891011121314151617&gt;@Lazy&gt;@Configuration&gt;@ComponentScan(basePackages = &quot;com.spring.configuration.pojo&quot;)&gt;public class AppConfigWithLazy &#123; @Bean public MyBean myBean()&#123; System.out.println(&quot;myBean Initialized&quot;); return new MyBean(); &#125; @Bean public MyBean IfLazyInit()&#123; System.out.println(&quot;initialized&quot;); return new MyBean(); &#125;&gt;&#125; 修改测试类 123456789101112&gt;public class SpringConfigurationApplication &#123; public static void main(String[] args) &#123; AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfigWithLazy.class); // 获取启动过程中的bean 定义的名称 for(String str : context.getBeanDefinitionNames())&#123; System.out.println(&quot;str = &quot; + str); &#125; &#125;&gt;&#125; 输出你会发现没有关于bean的定义信息，但是当把@Lazy 注释拿掉，你会发现输出了关于bean的初始化信息 @DependsOn 注解指当前bean所依赖的bean。任何指定的bean都能保证在此bean创建之前由IOC容器创建。在bean没有通过属性或构造函数参数显式依赖于另一个bean的情况下很少使用，可能直接使用在任何直接或者间接使用 Component 或者Bean 注解表明的类上。来看一下具体的用法 新建三个Bean，分别是FirstBean、SecondBean、ThirdBean三个普通的bean，新建AppConfigWithDependsOn并配置它们之间的依赖关系 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&gt;public class FirstBean &#123; @Autowired private SecondBean secondBean; @Autowired private ThirdBean thirdBean; public FirstBean() &#123; System.out.println(&quot;FirstBean Initialized via Constuctor&quot;); &#125;&gt;&#125;&gt;public class SecondBean &#123; public SecondBean() &#123; System.out.println(&quot;SecondBean Initialized via Constuctor&quot;); &#125;&gt;&#125;&gt;public class ThirdBean &#123; public ThirdBean() &#123; System.out.println(&quot;ThirdBean Initialized via Constuctor&quot;); &#125;&gt;&#125;&gt;@Configuration&gt;public class AppConfigWithDependsOn &#123; @Bean(&quot;firstBean&quot;) @DependsOn(value = &#123; &quot;secondBean&quot;, &quot;thirdBean&quot; &#125;) public FirstBean firstBean() &#123; return new FirstBean(); &#125; @Bean(&quot;secondBean&quot;) public SecondBean secondBean() &#123; return new SecondBean(); &#125; @Bean(&quot;thirdBean&quot;) public ThirdBean thirdBean() &#123; return new ThirdBean(); &#125;&gt;&#125; 使用测试类进行测试，如下 1234&gt;// ------------------------------ 测试 DependsOn ------------------------------&gt;AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfigWithDependsOn.class);&gt;context.getBean(FirstBean.class);&gt;context.close(); 输出 : 123&gt;SecondBean Initialized via Constuctor&gt;ThirdBean Initialized via Constuctor&gt;FirstBean Initialized via Constuctor 由于firstBean 的创建过程首先需要依赖secondBean 和 thirdBean的创建，所以secondBean 首先被加载其次是thirdBean 最后是firstBean。 如果把@DependsOn 注解加在AppConfigWithDependsOn 类上则它们的初始化顺序就会变为 firstBean、secondBean、thirdBean @Primary 注解指示当多个候选者有资格自动装配依赖项时，应优先考虑bean。此注解在语义上就等同于在Spring XML中定义的bean 元素的primary属性。注意： 除非使用component-scanning进行组件扫描，否则在类级别上使用@Primary不会有作用。如果@Primary 注解定义在XML中，那么@Primary 的注解元注解就会忽略，相反使用 @Primary 的两种使用方式 与@Bean 一起使用，定义在方法上，方法级别的注解 与@Component 一起使用，定义在类上，类级别的注解 通过一则示例来演示一下： 新建一个AppConfigWithPrimary类，在方法级别上定义@Primary注解 1234567891011121314&gt;@Configuration&gt;public class AppConfigWithPrimary &#123; @Bean public MyBean myBeanOne()&#123; return new MyBean(); &#125; @Bean @Primary public MyBean myBeanTwo()&#123; return new MyBean(); &#125;&gt;&#125; 上面代码定义了两个bean ，其中myBeanTwo 由@Primary 进行标注，表示它首先会进行注册，使用测试类进行测试 1234&gt;// ------------------------------ 测试 Primary ------------------------------&gt;AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfigWithPrimary.class);&gt;MyBean bean = context.getBean(MyBean.class);&gt;System.out.println(bean); 你可以尝试放开@Primary ，使用测试类测试的话会发现出现报错信息，因为你尝试获取的是MyBean.class,而我们代码中定义了两个MyBean 的类型，所以需要@Primary 注解表明哪一个bean需要优先被获取。 什么是IOC（1）控制反转：IOC—Inversion of Control，即“控制反转”，不是什么技术，而是一种设计思想 控制：IOC意味着将你设计好的对象交给IOC容器控制，而不是传统的在你的对象内部直接控制； 123IOC是有专门一个容器来创建这些对象，即由IOC容器来控制对象的创建；谁控制谁？当然是IOC容器控制了对象；控制什么？那就是主要控制了外部资源获取（不只是对象包括比如文件等）。 反转：传统应用程序是由我们自己在对象中主动控制去直接获取依赖对象，也就是正转； 123反转则是由IOC容器来帮忙创建及注入依赖对象；由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。 12345678IOC 不是一种技术，只是一种思想，一个重要的面向对象编程的法则，它能指导我们如何设计出松耦合、更优良的程序。传统应用程序都是由我们在类内部主动创建依赖对象，从而导致类与类之间高耦合，难于测试；有了IOC容器后，把创建和查找依赖对象的控制权交给了容器，由容器进行注入组合对象，所以对象与对象之间是 松散耦合，这样也方便测试，利于功能复用，更重要的是使得程序的整个体系结构变得非常灵活。其实 IOC 对编程带来的最大改变不是从代码上，而是从思想上，发生了“主从换位”的变化。应用程序原本是老大，要获取什么资源都是主动出击，但是在 IOC/DI 思想中，应用程序就变成被动的了，被动的等待 IOC 容器来创建并注入它所需要的资源了。 示例图： ​ （2）使用IOC的目的：降低耦合度用伪代码来比较：用工厂类来做例子（注意：工厂类不是IOC模式） 原始方式：一个类调用另外一个类的对象使用 升级后：工厂模式解耦，但是也不是完全解耦 （3）IOC解耦过程图解可能一时半会看不懂图，但是通过后面深入理解就会豁然开朗 IOC的思想就是我们仅仅通过修改XML配置文件的对象路径，就可以轻松把对象创建出来去被别的类进行调用 2.IOC的底层原理（1）xml解析、工厂模式、反射 3.IOC接口 （1）IOC思想 基于IOC容器完成，IOC容器底层就是对象工厂 （2）Spring提供IOC容器实现两种方式：（两个接口） 第一种: BeanFactory ，可以对IOC容器基本实现，是Spring 内部的使用接口，不提供开发人员进行使用 【加载配置文件的时候不会创建对象，在获取对象（也就是说在使用的时候）才会去创建对象】 第二种：ApplicationContext，是BeanFactory 接口的子接口，提供更多强大的功能，一般由开发人员使用 【加载配置文件的时候会把在配置文件中的对象进行创建】 一般推荐使用第二种，因为Spring框架，要结合web项目进行操作，通俗点来说就是用tomcat服务器进行启动，因此把这些耗时耗内存的都在项目启动的时候进行处理更加合适","categories":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"个人理解","slug":"个人理解","permalink":"https://gwtt.github.io/tags/%E4%B8%AA%E4%BA%BA%E7%90%86%E8%A7%A3/"},{"name":"Springboot","slug":"Springboot","permalink":"https://gwtt.github.io/tags/Springboot/"}]},{"title":"AOP初级","slug":"AOP初级","date":"2022-10-10T02:57:36.000Z","updated":"2022-10-10T15:00:58.601Z","comments":true,"path":"2022/10/10/AOP初级/","link":"","permalink":"https://gwtt.github.io/2022/10/10/AOP%E5%88%9D%E7%BA%A7/","excerpt":"","text":"今天主要来谈谈AOP面向切面编程，这东西我已经在项目里运用过好多次，但是从来没有一个系统性的整理，所以，用几篇文章来好好的梳理一下什么是AOP，并且如何在项目里充分运用它。 什么是AOP AOP面向切面编程，通过预编译方式和运行期间动态代理实现程序功能的统一维护的一种技术。AOP是OOP的延续，是软件开发中的一个热点，也是Spring框架中的一个重要内容，是函数式编程的一种衍生范型。利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。 最简单的例子就是，你实现了一个加法函数，但是你不想在改变原函数功能的基础之上，实现乘法。这时候你可以在外部添加一个类，然后通过AOP的技术给这个方法添加功能的操作，这就是AOP。 AOP的特点是什么 AOP代表的是一个横向的关系，将“对象”比作一个空心的圆柱体，其中封装的是对象的属性和行为；则面向方面编程的方法，就是将这个圆柱体以切面形式剖开，选择性的提供业务逻辑。而剖开的切面，也就是所谓的“方面”了。然后它又以巧夺天功的妙手将这些剖开的切面复原，不留痕迹，但完成了效果。 实现AOP的技术 主要分为两大类 采用动态代理技术，利用截取消息的方式，对该消息进行装饰，以取代原有对象行为的执行 采用静态织入的方式，引入特定的语法创建“方面”，从而使得编译器可以在编译期间织入有关“方面”的代码 然后AOP的底层原理就是使用了动态代理 主要分为两种情况 （1）有接口，使用了JDK的动态代理 （2）没有接口，使用了CGLIB代理 AOP使用场景 利用AOP可以对我们边缘业务进行隔离，降低无关业务逻辑耦合性。提高程序的可重用性，同时提高了开发的效率。一般用于日志记录，性能统计，安全控制，权限管理，事务处理，异常处理，资源池管理 技术要点 通知（Advice）包含了需要用于多个应用对象的横切行为，完全听不懂，没关系，通俗一点说就是定义了“什么时候”和“做什么”。 连接点（Join Point）是程序执行过程中能够应用通知的所有点。 切点（Poincut）是定义了在“什么地方”进行切入，哪些连接点会得到通知。显然，切点一定是连接点。 切面（Aspect）是通知和切点的结合。通知和切点共同定义了切面的全部内容——是什么，何时，何地完成功能。 引入（Introduction）允许我们向现有的类中添加新方法或者属性。 织入（Weaving）是把切面应用到目标对象并创建新的代理对象的过程，分为编译期织入、类加载期织入和运行期织入。 整合使用导入依赖在springboot中使用aop要导aop依赖 12345&lt;!--aop 切面--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;/dependency&gt; 注意这里版本依赖于spring-boot-start-parent父pom中的spring-boot-dependencies 编写拦截的bean这里我们定义一个controller用于拦截所有请求的记录 123456789@RestControllerpublic class AopController &#123; @RequestMapping(&quot;/hello&quot;) public String sayHello()&#123; System.out.println(&quot;hello&quot;); return &quot;hello&quot;; &#125;&#125; 定义切面SpringBoot在使用切面的时候采用@Aspect注解对POJO进行标注，该注解表明该类不仅仅是一个POJO，还是一个切面容器 定义切点切点是通过@Pointcut注解和切点表达式定义的。 @Pointcut注解可以在一个切面内定义可重用的切点。 由于Spring切面粒度最小是达到方法级别，而execution表达式可以用于明确指定方法返回类型，类名，方法名和参数名等与方法相关的部件，并且实际中，大部分需要使用AOP的业务场景也只需要达到方法级别即可，因而execution表达式的使用是最为广泛的。如图是execution表达式的语法： execution表示在方法执行的时候触发。以“”开头，表明方法返回值类型为任意类型。然后是全限定的类名和方法名，“”可以表示任意类和任意方法。对于方法参数列表，可以使用“..”表示参数为任意类型。如果需要多个表达式，可以使用“&amp;&amp;”、“||”和“!”完成与、或、非的操作。 定义通知通知有五种类型，分别是： 前置通知（@Before）：在目标方法调用之前调用通知 后置通知（@After）：在目标方法完成之后调用通知 环绕通知（@Around）：在被通知的方法调用之前和调用之后执行自定义的方法 返回通知（@AfterReturning）：在目标方法成功执行之后调用通知 异常通知（@AfterThrowing）：在目标方法抛出异常之后调用通知 代码中定义了三种类型的通知，使用@Before注解标识前置通知，打印“beforeAdvice…”，使用@After注解标识后置通知，打印“AfterAdvice…”，使用@Around注解标识环绕通知，在方法执行前和执行之后分别打印“before”和“after”。这样一个切面就定义好了，代码如下： 12345678910111213141516171819202122232425262728293031@Aspect@Componentpublic class AopAdvice &#123; @Pointcut(&quot;execution (* com.shangguan.aop.controller.*.*(..))&quot;) public void test() &#123; &#125; @Before(&quot;test()&quot;) public void beforeAdvice() &#123; System.out.println(&quot;beforeAdvice...&quot;); &#125; @After(&quot;test()&quot;) public void afterAdvice() &#123; System.out.println(&quot;afterAdvice...&quot;); &#125; @Around(&quot;test()&quot;) public void aroundAdvice(ProceedingJoinPoint proceedingJoinPoint) &#123; System.out.println(&quot;before&quot;); try &#123; proceedingJoinPoint.proceed(); &#125; catch (Throwable t) &#123; t.printStackTrace(); &#125; System.out.println(&quot;after&quot;); &#125;&#125; 运行结果 案例场景这里我们通过一个日志记录场景来完整的使用Aop切面业务层只需关心代码逻辑实现而不用关心请求参数和响应参数的日志记录 那么首先我们需要自定义一个全局日志记录的切面类GlobalLogAspect 然后在该类添加@Aspect注解，然后在定义一个公共的切入点（Pointcut），指向需要处理的包，然后在定义一个前置通知(添加@Before注解)，后置通知(添加@AfterReturning)和环绕通知（添加@Around）方法实现即可 日志信息类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package cn.soboys.core;import lombok.Data;@Datapublic class LogSubject &#123; /** * 操作描述 */ private String description; /** * 操作用户 */ private String username; /** * 操作时间 */ private String startTime; /** * 消耗时间 */ private String spendTime; /** * URL */ private String url; /** * 请求类型 */ private String method; /** * IP地址 */ private String ip; /** * 请求参数 */ private Object parameter; /** * 请求返回的结果 */ private Object result; /** * 城市 */ private String city; /** * 请求设备信息 */ private String device;&#125; 全局日志拦截123456789101112131415161718192021222324252627282930313233package cn.soboys.core;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.reflect.MethodSignature;import java.lang.reflect.Method;public class BaseAspectSupport &#123; public Method resolveMethod(ProceedingJoinPoint point) &#123; MethodSignature signature = (MethodSignature)point.getSignature();//获取方法签名 Class&lt;?&gt; targetClass = point.getTarget().getClass();//获取切入点的目标类 Method method = getDeclaredMethod(targetClass, signature.getName(), signature.getMethod().getParameterTypes()); if (method == null) &#123; throw new IllegalStateException(&quot;无法解析目标方法: &quot; + signature.getMethod().getName()); &#125; return method; &#125; private Method getDeclaredMethod(Class&lt;?&gt; clazz, String name, Class&lt;?&gt;... parameterTypes) &#123; try &#123; return clazz.getDeclaredMethod(name, parameterTypes); &#125; catch (NoSuchMethodException e) &#123; Class&lt;?&gt; superClass = clazz.getSuperclass(); if (superClass != null) &#123; return getDeclaredMethod(superClass, name, parameterTypes); &#125; &#125; return null; &#125;&#125; GlobalLogAspect类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package cn.soboys.core;import cn.hutool.core.date.DateUtil;import cn.hutool.core.date.TimeInterval;import cn.hutool.json.JSONUtil;import cn.soboys.core.utils.HttpContextUtil;import io.swagger.annotations.ApiOperation;import lombok.extern.slf4j.Slf4j;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.AfterThrowing;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Pointcut;import org.springframework.stereotype.Component;import org.springframework.web.bind.annotation.RequestBody;import org.springframework.web.bind.annotation.RequestParam;import javax.servlet.http.HttpServletRequest;import java.lang.reflect.Method;import java.lang.reflect.Parameter;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;@Slf4j@Aspect@Componentpublic class GlobalLogAspect extends BaseAspectSupport &#123; /** * 定义切面Pointcut */ @Pointcut(&quot;execution(public * cn.soboys.mallapi.controller.*.*(..))&quot;) public void log() &#123; &#125; /** * 环绕通知 * * @param joinPoint * @return */ @Around(&quot;log()&quot;) public Object doAround(ProceedingJoinPoint joinPoint) throws Throwable &#123; LogSubject logSubject = new LogSubject(); //记录时间定时器 TimeInterval timer = DateUtil.timer(true); //执行结果 Object result = joinPoint.proceed(); logSubject.setResult(result); //执行消耗时间 String endTime = timer.intervalPretty(); logSubject.setSpendTime(endTime); //执行参数 Method method = resolveMethod(joinPoint); logSubject.setParameter(getParameter(method, joinPoint.getArgs())); HttpServletRequest request = HttpContextUtil.getRequest(); // 接口请求时间 logSubject.setStartTime(DateUtil.now()); //请求链接 logSubject.setUrl(request.getRequestURL().toString()); //请求方法GET,POST等 logSubject.setMethod(request.getMethod()); //请求设备信息 logSubject.setDevice(HttpContextUtil.getDevice()); //请求地址 logSubject.setIp(HttpContextUtil.getIpAddr()); //接口描述 if (method.isAnnotationPresent(ApiOperation.class)) &#123; ApiOperation apiOperation = method.getAnnotation(ApiOperation.class); logSubject.setDescription(apiOperation.value()); &#125; String a = JSONUtil.toJsonPrettyStr(logSubject); log.info(a); return result; &#125; /** * 根据方法和传入的参数获取请求参数 */ private Object getParameter(Method method, Object[] args) &#123; List&lt;Object&gt; argList = new ArrayList&lt;&gt;(); Parameter[] parameters = method.getParameters(); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; parameters.length; i++) &#123; //将RequestBody注解修饰的参数作为请求参数 RequestBody requestBody = parameters[i].getAnnotation(RequestBody.class); //将RequestParam注解修饰的参数作为请求参数 RequestParam requestParam = parameters[i].getAnnotation(RequestParam.class); String key = parameters[i].getName(); if (requestBody != null) &#123; argList.add(args[i]); &#125; else if (requestParam != null) &#123; map.put(key, args[i]); &#125; else &#123; map.put(key, args[i]); &#125; &#125; if (map.size() &gt; 0) &#123; argList.add(map); &#125; if (argList.size() == 0) &#123; return null; &#125; else if (argList.size() == 1) &#123; return argList.get(0); &#125; else &#123; return argList; &#125; &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"AOP","slug":"AOP","permalink":"https://gwtt.github.io/tags/AOP/"},{"name":"个人理解","slug":"个人理解","permalink":"https://gwtt.github.io/tags/%E4%B8%AA%E4%BA%BA%E7%90%86%E8%A7%A3/"}]},{"title":"Java虚拟线程","slug":"Java虚拟线程","date":"2022-10-09T06:06:36.000Z","updated":"2022-10-09T11:19:49.852Z","comments":true,"path":"2022/10/09/Java虚拟线程/","link":"","permalink":"https://gwtt.github.io/2022/10/09/Java%E8%99%9A%E6%8B%9F%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"因为在Java19之前，Java只有一种线程，然后为了与19中的虚拟线程区分开来，所以之前的线程统称为平台线程。 平台线程与操作系统的内核线程是一一对应的。 然后我们主要来谈论19中新添加（模仿）的线程–虚拟线程 虚拟线程特征 是用户模式线程 由Java运行时调度 虚拟线程和内核线程是M对N的对应关系，也就是M个虚拟线程会被映射到N个内核线程上 虚拟线程可以使用独占线程处理每个请求的并发风格，也就是thread-per-requestthread-per-request特点 使用独占的线程来处理该请求 易于理解和编程实现 易于调优和性能调优 但是thread-per-request不能用平台线程来实现，因为平台线程是操作系统中的线程操作的一种封装，而操作系统的线程会占用资源，存在数量上限，对于一个海量级别的服务端来说，不可能一个请求来，然后去生成一个平台线程。 为了实现thread-per-request，目前有这几种解决思路 依赖于非阻塞I/O和异步编程（用少量线程处理大量的请求） 可以提升系统的吞吐量 开发人员必须熟悉所使用的底层框架 使用虚拟线程好处 使用最自然的方式来编写代码 把请求的处理逻辑全部在一个虚拟线程中完成 降低了编写高并发服务端应用的难度 虚拟线程不需要放入线程池 虚拟线程的调度 由JDK负责调度 JDK把虚拟线程分配个平台线程 平台线程则由操作系统负责调度 一个虚拟线程所分配的平台线程被称为该虚拟线程的载体，然后一个虚拟线程可能被调度到多个载体上，载体的标识对于虚拟线程是不可见的 JDK调度虚拟线程时，使用的是一个以FIFO模式工作的work-stealing ForkJoinPool，该ForkJoinPool的paralleism决定了调度时可以使用的平台线程的数量 虚拟线程的执行 把虚拟线程绑定到平台线程 从平台线程上接触绑定，当虚拟线程在等待I/O或是执行某些阻塞操作时，可以从平台线程上解除绑定 等待阻塞操作完成之后，可以绑定到新的平台线程上继续执行 对于应用代码来说是透明的 有些JDK中的阻塞操作并不会解除对平台线程的绑定,因此会阻塞平台线程和底层的操作系统线程，比如 文件操作、Object.wait()方法调用，这些阻塞操作的实现会在内部对此进行补偿 临时增加JDK的调度器可以使用的线程数量 在下面两种情况下，虚拟线程会被Pin在载体上而无法解除绑定 在执行Synchronized方法或块时 在执行native方法或外部方法时 创建虚拟线程 第一种1var thread = Thread.ofVirtual().name(&quot;my virtual&quot;).start(() -&gt; System.out.println(&quot;运行&quot;)); 一个新的虚拟线程被创建并启动，返回的时java.lang.Thread类的对象 第二种1Thread.startVirtualThread(Runnable task) 第三种使用线程工厂来实现 12var factory = Thread.ofVirtual().factory();var thread = factory.newThread(()-&gt; System.out.println(&quot;在工厂中创建&quot;)); 第四种使用线程池 12Executors.newVirtualThreadPerTaskExecutor();Executors.newThreadPerTaskExecutor(ThreadFactory threadFactory); 用Executors对象所能创建的线程数量理论上没有上限(受限于内存)","categories":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"个人理解","slug":"个人理解","permalink":"https://gwtt.github.io/tags/%E4%B8%AA%E4%BA%BA%E7%90%86%E8%A7%A3/"},{"name":"Juc","slug":"Juc","permalink":"https://gwtt.github.io/tags/Juc/"}]},{"title":"Sa-token","slug":"Sa-token框架理解","date":"2022-10-03T06:12:36.000Z","updated":"2022-10-03T08:30:49.552Z","comments":true,"path":"2022/10/03/Sa-token框架理解/","link":"","permalink":"https://gwtt.github.io/2022/10/03/Sa-token%E6%A1%86%E6%9E%B6%E7%90%86%E8%A7%A3/","excerpt":"","text":"这是本人打算长篇更新的博客内容，关于Sa-token这个国人开发的鉴权框架，这是我第一个尝试看源码的框架，同时也去尝试了解之前不敢涉足的一些关于Springboot底层的一些代码 本文没有先后顺序，后期会理顺整理一下 Sa-token-dao-redis 我第一次看模块就点开了这个模块，主要是最近想回顾redis，然后觉得redis应该不太复杂 我很惊讶，因为这个模块只有一个代码组件，下面就是源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203@Componentpublic class SaTokenDaoRedis implements SaTokenDao &#123; /** * String专用 */ public StringRedisTemplate stringRedisTemplate; /** * Object专用 */ public RedisTemplate&lt;String, Object&gt; objectRedisTemplate; /** * 标记：是否已初始化成功 */ public boolean isInit; @Autowired public void init(RedisConnectionFactory connectionFactory) &#123; // 不重复初始化 if(this.isInit) &#123; return; &#125; // 指定相应的序列化方案 StringRedisSerializer keySerializer = new StringRedisSerializer(); JdkSerializationRedisSerializer valueSerializer = new JdkSerializationRedisSerializer(); // 构建StringRedisTemplate StringRedisTemplate stringTemplate = new StringRedisTemplate(); stringTemplate.setConnectionFactory(connectionFactory); stringTemplate.afterPropertiesSet(); // 构建RedisTemplate RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;String, Object&gt;(); template.setConnectionFactory(connectionFactory); template.setKeySerializer(keySerializer); template.setHashKeySerializer(keySerializer); template.setValueSerializer(valueSerializer); template.setHashValueSerializer(valueSerializer); template.afterPropertiesSet(); // 开始初始化相关组件 this.stringRedisTemplate = stringTemplate; this.objectRedisTemplate = template; this.isInit = true; &#125; /** * 获取Value，如无返空 */ @Override public String get(String key) &#123; return stringRedisTemplate.opsForValue().get(key); &#125; /** * 写入Value，并设定存活时间 (单位: 秒) */ @Override public void set(String key, String value, long timeout) &#123; if(timeout == 0 || timeout &lt;= SaTokenDao.NOT_VALUE_EXPIRE) &#123; return; &#125; // 判断是否为永不过期 if(timeout == SaTokenDao.NEVER_EXPIRE) &#123; stringRedisTemplate.opsForValue().set(key, value); &#125; else &#123; stringRedisTemplate.opsForValue().set(key, value, timeout, TimeUnit.SECONDS); &#125; &#125; /** * 修改指定key-value键值对 (过期时间不变) */ @Override public void update(String key, String value) &#123; long expire = getTimeout(key); // -2 = 无此键 if(expire == SaTokenDao.NOT_VALUE_EXPIRE) &#123; return; &#125; this.set(key, value, expire); &#125; /** * 删除Value */ @Override public void delete(String key) &#123; stringRedisTemplate.delete(key); &#125; /** * 获取Value的剩余存活时间 (单位: 秒) */ @Override public long getTimeout(String key) &#123; return stringRedisTemplate.getExpire(key); &#125; /** * 修改Value的剩余存活时间 (单位: 秒) */ @Override public void updateTimeout(String key, long timeout) &#123; // 判断是否想要设置为永久 if(timeout == SaTokenDao.NEVER_EXPIRE) &#123; long expire = getTimeout(key); if(expire == SaTokenDao.NEVER_EXPIRE) &#123; // 如果其已经被设置为永久，则不作任何处理 &#125; else &#123; // 如果尚未被设置为永久，那么再次set一次 this.set(key, this.get(key), timeout); &#125; return; &#125; stringRedisTemplate.expire(key, timeout, TimeUnit.SECONDS); &#125; /** * 获取Object，如无返空 */ @Override public Object getObject(String key) &#123; return objectRedisTemplate.opsForValue().get(key); &#125; /** * 写入Object，并设定存活时间 (单位: 秒) */ @Override public void setObject(String key, Object object, long timeout) &#123; if(timeout == 0 || timeout &lt;= SaTokenDao.NOT_VALUE_EXPIRE) &#123; return; &#125; // 判断是否为永不过期 if(timeout == SaTokenDao.NEVER_EXPIRE) &#123; objectRedisTemplate.opsForValue().set(key, object); &#125; else &#123; objectRedisTemplate.opsForValue().set(key, object, timeout, TimeUnit.SECONDS); &#125; &#125; /** * 更新Object (过期时间不变) */ @Override public void updateObject(String key, Object object) &#123; long expire = getObjectTimeout(key); // -2 = 无此键 if(expire == SaTokenDao.NOT_VALUE_EXPIRE) &#123; return; &#125; this.setObject(key, object, expire); &#125; /** * 删除Object */ @Override public void deleteObject(String key) &#123; objectRedisTemplate.delete(key); &#125; /** * 获取Object的剩余存活时间 (单位: 秒) */ @Override public long getObjectTimeout(String key) &#123; return objectRedisTemplate.getExpire(key); &#125; /** * 修改Object的剩余存活时间 (单位: 秒) */ @Override public void updateObjectTimeout(String key, long timeout) &#123; // 判断是否想要设置为永久 if(timeout == SaTokenDao.NEVER_EXPIRE) &#123; long expire = getObjectTimeout(key); if(expire == SaTokenDao.NEVER_EXPIRE) &#123; // 如果其已经被设置为永久，则不作任何处理 &#125; else &#123; // 如果尚未被设置为永久，那么再次set一次 this.setObject(key, this.getObject(key), timeout); &#125; return; &#125; objectRedisTemplate.expire(key, timeout, TimeUnit.SECONDS); &#125; /** * 搜索数据 */ @Override public List&lt;String&gt; searchData(String prefix, String keyword, int start, int size, boolean sortType) &#123; Set&lt;String&gt; keys = stringRedisTemplate.keys(prefix + &quot;*&quot; + keyword + &quot;*&quot;); List&lt;String&gt; list = new ArrayList&lt;String&gt;(keys); return SaFoxUtil.searchList(list, start, size, sortType); &#125; &#125; 初看感觉很简单，就是用SpringDataRedis来进行对Sa-token的id一些存取操作 个人觉得难点在于初始化那块 1234567891011121314151617181920212223242526272829303132333435363738394041/** * String专用 */ public StringRedisTemplate stringRedisTemplate; /** * Object专用 */public RedisTemplate&lt;String, Object&gt; objectRedisTemplate;/** * 标记：是否已初始化成功 */public boolean isInit;@Autowiredpublic void init(RedisConnectionFactory connectionFactory) &#123; // 不重复初始化 if(this.isInit) &#123; return; &#125; // 指定相应的序列化方案 StringRedisSerializer keySerializer = new StringRedisSerializer(); JdkSerializationRedisSerializer valueSerializer = new JdkSerializationRedisSerializer(); // 构建StringRedisTemplate StringRedisTemplate stringTemplate = new StringRedisTemplate(); stringTemplate.setConnectionFactory(connectionFactory); stringTemplate.afterPropertiesSet(); // 构建RedisTemplate RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;String, Object&gt;(); template.setConnectionFactory(connectionFactory); template.setKeySerializer(keySerializer); template.setHashKeySerializer(keySerializer); template.setValueSerializer(valueSerializer); template.setHashValueSerializer(valueSerializer); template.afterPropertiesSet(); // 开始初始化相关组件 this.stringRedisTemplate = stringTemplate; this.objectRedisTemplate = template; this.isInit = true;&#125; 首先这个init方法至关重要，初始化RedisTemplate，同时方法是@Autowired注解修饰，这意味着方法会自动调用，参数会自动注入 那么问题来了，既然这个参数会自动注入，然后参数类型是RedisConnectionFactory，所以我们就要找了。 最终找到了唯一调用这个方法的类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104@Configurationpublic class SaAloneRedisInject implements EnvironmentAware&#123; /** * 配置信息的前缀 */ public static final String ALONE_PREFIX = &quot;sa-token.alone-redis&quot;; /** * Sa-Token 持久层接口 */ @Autowired(required = false) public SaTokenDao saTokenDao; /** * 开始注入 */ @Override public void setEnvironment(Environment environment) &#123; try &#123; // 如果为空或者默认实现，则不进行任何操作 if(saTokenDao == null || saTokenDao instanceof SaTokenDaoDefaultImpl) &#123; return; &#125; // 如果配置文件不包含相关配置，则不进行任何操作 if(environment.getProperty(ALONE_PREFIX + &quot;.host&quot;) == null) &#123; return; &#125; // ------------------- 开始注入 // 获取cfg对象 RedisProperties cfg = Binder.get(environment).bind(ALONE_PREFIX, RedisProperties.class).get(); // 1. Redis配置 RedisStandaloneConfiguration redisConfig = new RedisStandaloneConfiguration(); redisConfig.setHostName(cfg.getHost()); redisConfig.setPort(cfg.getPort()); redisConfig.setDatabase(cfg.getDatabase()); redisConfig.setPassword(RedisPassword.of(cfg.getPassword())); // 2. 连接池配置 GenericObjectPoolConfig poolConfig = new GenericObjectPoolConfig(); // pool配置 Lettuce lettuce = cfg.getLettuce(); if(lettuce.getPool() != null) &#123; RedisProperties.Pool pool = cfg.getLettuce().getPool(); // 连接池最大连接数 poolConfig.setMaxTotal(pool.getMaxActive()); // 连接池中的最大空闲连接 poolConfig.setMaxIdle(pool.getMaxIdle()); // 连接池中的最小空闲连接 poolConfig.setMinIdle(pool.getMinIdle()); // 连接池最大阻塞等待时间（使用负值表示没有限制） poolConfig.setMaxWaitMillis(pool.getMaxWait().toMillis()); &#125; LettucePoolingClientConfiguration.LettucePoolingClientConfigurationBuilder builder = LettucePoolingClientConfiguration.builder(); // timeout if(cfg.getTimeout() != null) &#123; builder.commandTimeout(cfg.getTimeout()); &#125; // shutdownTimeout if(lettuce.getShutdownTimeout() != null) &#123; builder.shutdownTimeout(lettuce.getShutdownTimeout()); &#125; // 创建Factory对象 LettuceClientConfiguration clientConfig = builder.poolConfig(poolConfig).build(); LettuceConnectionFactory factory = new LettuceConnectionFactory(redisConfig, clientConfig); factory.afterPropertiesSet(); // 3. 开始初始化 SaTokenDao // 如果是SaTokenDaoRedis try &#123; Class.forName(&quot;cn.dev33.satoken.dao.SaTokenDaoRedis&quot;); SaTokenDaoRedis dao = (SaTokenDaoRedis)saTokenDao; dao.isInit = false; dao.init(factory); return; &#125; catch (ClassNotFoundException e) &#123; &#125; // 如果是SaTokenDaoRedisJackson try &#123; Class.forName(&quot;cn.dev33.satoken.dao.SaTokenDaoRedisJackson&quot;); SaTokenDaoRedisJackson dao = (SaTokenDaoRedisJackson)saTokenDao; dao.isInit = false; dao.init(factory); return; &#125; catch (ClassNotFoundException e) &#123; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 骗过编辑器，增加配置文件代码提示 * @return 配置对象 */ @ConfigurationProperties(prefix = ALONE_PREFIX) public RedisProperties getSaAloneRedisConfig() &#123; return new RedisProperties(); &#125; &#125; 首先很明显这个类由@Configuration修饰，注入到IOC容器中，以便Springboot默认执行setEnvironment（）方法 然后，下面这段话,将配置文件中的属性对应成对象，并实例化获得 1Binder.get(environment).bind(ALONE_PREFIX, RedisProperties.class).get(); 123456RedisStandaloneConfiguration redisConfig = new RedisStandaloneConfiguration();redisConfig.setHostName(cfg.getHost());redisConfig.setPort(cfg.getPort());redisConfig.setDatabase(cfg.getDatabase());redisConfig.setPassword(RedisPassword.of(cfg.getPassword())); //这边就是配置redis 123456789101112131415161718192021222324//接下来就是连接池配置GenericObjectPoolConfig poolConfig = new GenericObjectPoolConfig();// pool配置 Lettuce lettuce = cfg.getLettuce();if(lettuce.getPool() != null) &#123; RedisProperties.Pool pool = cfg.getLettuce().getPool(); // 连接池最大连接数 poolConfig.setMaxTotal(pool.getMaxActive()); // 连接池中的最大空闲连接 poolConfig.setMaxIdle(pool.getMaxIdle()); // 连接池中的最小空闲连接 poolConfig.setMinIdle(pool.getMinIdle()); // 连接池最大阻塞等待时间（使用负值表示没有限制） poolConfig.setMaxWaitMillis(pool.getMaxWait().toMillis());&#125;LettucePoolingClientConfiguration.LettucePoolingClientConfigurationBuilder builder = LettucePoolingClientConfiguration.builder();// timeout if(cfg.getTimeout() != null) &#123; builder.commandTimeout(cfg.getTimeout());&#125;// shutdownTimeout if(lettuce.getShutdownTimeout() != null) &#123; builder.shutdownTimeout(lettuce.getShutdownTimeout());&#125; 1234// 创建Factory对象 LettuceClientConfiguration clientConfig = builder.poolConfig(poolConfig).build();LettuceConnectionFactory factory = new LettuceConnectionFactory(redisConfig, clientConfig);factory.afterPropertiesSet(); 1234567891011121314151617181920212223// 初始化SaTokenDao // 如果是SaTokenDaoRedistry &#123; Class.forName(&quot;cn.dev33.satoken.dao.SaTokenDaoRedis&quot;); SaTokenDaoRedis dao = (SaTokenDaoRedis)saTokenDao; dao.isInit = false; dao.init(factory); return;&#125; catch (ClassNotFoundException e) &#123;&#125;// 如果是SaTokenDaoRedisJacksontry &#123; Class.forName(&quot;cn.dev33.satoken.dao.SaTokenDaoRedisJackson&quot;); SaTokenDaoRedisJackson dao = (SaTokenDaoRedisJackson)saTokenDao; dao.isInit = false; dao.init(factory); return;&#125; catch (ClassNotFoundException e) &#123; &#125;&#125; catch (Exception e) &#123; e.printStackTrace();&#125;//尝试去找到对应包下的类，根据找到的情况初始化dao 对于之前那个init上面用@Autowired的问题，还是有许多疑问，比如在SaTokenDaoRedis先注入进IOC，然后再SaAloneRedisInject里面调用setEnvironment（）方法进行注入，这个时候会调用init（）方法，然后调用默认的JedisConnectionFactory容器对象。 最好的解释是，setEnvironment（）先运行，然后调用SaTokenDaoRedis的init（）方法对内部进行初始化，如果没有的话，才执行默认的连接工厂。 为此我特地去查了下Springboot中的依赖注入顺序，发现setEnvironment确实先运行","categories":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"个人理解","slug":"个人理解","permalink":"https://gwtt.github.io/tags/%E4%B8%AA%E4%BA%BA%E7%90%86%E8%A7%A3/"},{"name":"框架","slug":"框架","permalink":"https://gwtt.github.io/tags/%E6%A1%86%E6%9E%B6/"}]},{"title":"Java函数接口","slug":"Java函数接口","date":"2022-10-02T02:10:36.000Z","updated":"2022-10-10T11:46:14.384Z","comments":true,"path":"2022/10/02/Java函数接口/","link":"","permalink":"https://gwtt.github.io/2022/10/02/Java%E5%87%BD%E6%95%B0%E6%8E%A5%E5%8F%A3/","excerpt":"","text":"本人在阅读Sa-token框架的时候，在研究全局策略的时候，发现了用到了大量的函数式接口，于是写下这篇文章巩固一下自己的知识点。 总所周知啊，Java在1.8的时候推出了函数接口，分别为Function（函数式接口），Supplier（供给式接口），Consumer（消费式接口），Predicate（断言式接口），这四个后面延申出BiFunction等等相应的函数接口。下面具体介绍。 Function接口 下面是Function的源码 123456789101112131415161718@FunctionalInterfacepublic interface Function&lt;T, R&gt; &#123; R apply(T t); default &lt;V&gt; Function&lt;V, R&gt; compose(Function&lt;? super V, ? extends T&gt; before) &#123; Objects.requireNonNull(before); return (V v) -&gt; apply(before.apply(v)); &#125; default &lt;V&gt; Function&lt;T, V&gt; andThen(Function&lt;? super R, ? extends V&gt; after) &#123; Objects.requireNonNull(after); return (T t) -&gt; after.apply(apply(t)); &#125; static &lt;T&gt; Function&lt;T, T&gt; identity() &#123; return t -&gt; t; &#125;&#125; 12345678910111213141516public static void main(String[] args) &#123; Function&lt;String, Integer&gt; lengthFunction = str -&gt; str.length(); Function&lt;Integer, Integer&gt; doubleFunction = length -&gt; length * 2; Integer doubleLength = doubleFunction.compose(lengthFunction).apply(&quot;www.wdbyte.com&quot;); System.out.println(doubleLength);&#125; //另一种写法public static void main(String[] args) &#123; Function&lt;String, Integer&gt; lengthFunction = str -&gt; str.length(); Function&lt;Integer, Integer&gt; doubleFunction = length -&gt; length * 2; Integer doubleLength = lengthFunction.andThen(doubleFunction).apply(&quot;www.wdbyte.com&quot;); System.out.println(doubleLength);&#125;//输出 28 很显然，一般形式为 123456789Function&lt;T,R&gt; varname = (T) -&gt;&#123; 操作... return ans;&#125;/*apply来调用方法compose和andThen用来连接其余Function方法identity用来返回一个lambda方法*/ 然后BiFunction相对于Function而言，接受的参数从一个变成两个 不过特殊的是,BiFunction的andThen方法可以接受Function参数，所以可以进一步抽象成更加通用的方法 比如工厂模式 1234567891011121314151617181920212223242526272829//实体类public class Dog &#123; private String name; private Integer age; public Dog() &#123; &#125; public Dog(String name, Integer age) &#123; this.name = name; this.age = age; &#125; // 省略 get set toString&#125;import java.util.function.BiFunction;public class JavaBiFunctionFactory &#123; public static void main(String[] args) &#123; System.out.println(dogFactory(&quot;牧羊犬&quot;, 1, Dog::new)); System.out.println(dogFactory(&quot;哈士奇&quot;, 2, Dog::new)); &#125; public static &lt;R extends Dog&gt; Dog dogFactory(String name, Integer age, BiFunction&lt;String, Integer, R&gt; biFunction) &#123; return biFunction.apply(name, age); &#125;&#125; Supplier接口 下面是Supplier的源码，看起来非常的简单，就一个返回T 12345@FunctionalInterfacepublic interface Supplier&lt;T&gt; &#123; T get();&#125; 由于不需要提供参数，所以可以很方便的用于对象创建 123456789101112public class Java8Supplier &#123; public static void main(String[] args) &#123; Supplier&lt;Integer&gt; supplier = () -&gt; new Random().nextInt(10); System.out.println(supplier.get()); System.out.println(supplier.get()); Supplier&lt;LocalDateTime&gt; supplier2 = LocalDateTime::now; System.out.println(supplier2.get()); System.out.println(supplier2.get()); &#125;&#125; 在 Java 8 中，为了方便 Supplier 的使用，提供了指定类型的 Supplier，有 BooleanSupplier, DoubleSupplier, IntSupplier, LongSupplier,用这些来规定返回值类型。 Consumer接口 Consumer主要用来消费参数，但是不返回任何值 1234567891011@FunctionalInterfacepublic interface Consumer&lt;T&gt; &#123; void accept(T t); default Consumer&lt;T&gt; andThen(Consumer&lt;? super T&gt; after) &#123; Objects.requireNonNull(after); return (T t) -&gt; &#123; accept(t); after.accept(t); &#125;; &#125;&#125; 12345678910111213141516171819202122232425262728public class Java8Consumer &#123; public static void main(String[] args) &#123; Consumer&lt;String&gt; lengthConsumer = s -&gt; System.out.println(s.length()); lengthConsumer.accept(&quot;Hello&quot;); Consumer&lt;String&gt; printConsumer = System.out::println; printConsumer.accept(&quot;Hello&quot;); &#125;&#125;public class Java8ConsumerForEach &#123; public static void main(String[] args) &#123; Consumer&lt;String&gt; printConsumer = System.out::println; List&lt;String&gt; list = Arrays.asList(&quot;java&quot;, &quot;c#&quot;, &quot;python&quot;); forEach(list, printConsumer); forEach(list, s -&gt; System.out.println(s.length())); &#125; public static &lt;T&gt; void forEach(List&lt;T&gt; list, Consumer&lt;T&gt; consumer) &#123; for (T t : list) &#123; consumer.accept(t); &#125; &#125;&#125; 函数接口 描述 BiConsumer 传入两个任意类型参数，无返回值 DoubleConsumer 传入一个 double 参数，无返回值 IntConsumer 传入一个 int 参数，无返回值 LongConsumer 传入一个 long 参数，无返回值 ObjDoubleConsumer 传入一个任意类型参数，一个 double 参数，无返回值 ObjIntConsumer 传入一个任意类型参数，一个 int 参数，无返回值 ObjLongConsumer 传入一个任意类型参数，一个 long 参数，无返回值 Predicate接口 这个是断言式接口，什么是断言，其实就是个判断语句，它接受一个T泛型参数，返回值为布尔类型 123456789101112131415161718192021@FunctionalInterfacepublic interface Predicate&lt;T&gt; &#123; boolean test(T t); default Predicate&lt;T&gt; and(Predicate&lt;? super T&gt; other) &#123;//使用 and() 方法，可以让前后两个 Predicate 判断条件一起生效。 Objects.requireNonNull(other); return (t) -&gt; test(t) &amp;&amp; other.test(t); &#125; default Predicate&lt;T&gt; negate() &#123;//predicate.negate() 方法会返回一个与指定判断相反的 Predicate。 return (t) -&gt; !test(t); &#125; default Predicate&lt;T&gt; or(Predicate&lt;? super T&gt; other) &#123;//使用 and() 方法，可以让前后两个 Predicate 判断条件都生效。 Objects.requireNonNull(other); return (t) -&gt; test(t) || other.test(t); &#125; static &lt;T&gt; Predicate&lt;T&gt; isEqual(Object targetRef) &#123; return (null == targetRef) ? Objects::isNull : object -&gt; targetRef.equals(object); &#125;&#125; Predicate 的 or() ，and()，negate() 方法可以随意组合 Predicate，组合后的判断逻辑是从左到右，从前到后，顺次判断。 1234567891011121314151617181920212223242526272829public class Java8PredicateChain &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; numberList = Arrays.asList(3, 4, 5, 6, 7, 8, 9, 10); Predicate&lt;Integer&gt; lessThan5 = number -&gt; number &lt;= 5; Predicate&lt;Integer&gt; greaterThan9 = number -&gt; number &gt;= 9; // 小于等于 5 System.out.println(filter(numberList, lessThan5)); // 大于 5 System.out.println(filter(numberList, lessThan5.negate())); // 小于等于 5 或者大于等于 9 System.out.println(filter(numberList, lessThan5.or(greaterThan9))); // ! (小于等于 5 AND 大于等于 9) System.out.println(filter(numberList, lessThan5.and(greaterThan9).negate())); &#125; public static &lt;T&gt; List&lt;T&gt; filter(List&lt;T&gt; list, Predicate&lt;T&gt; predicate) &#123; List&lt;T&gt; resultList = new ArrayList&lt;&gt;(); for (T t : list) &#123; if (predicate.test(t)) &#123; resultList.add(t); &#125; &#125; return resultList; &#125;&#125; UnaryOperator 接口 其实是Function接口的子类，唯一特点就是默认返回值是T，只接受一个参数T 123456789101112131415161718192021222324public class Java8UnaryOperatorParams &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = Arrays.asList(&quot;java&quot;, &quot;node&quot;, &quot;c++&quot;, &quot;rust&quot;); // 转大写 UnaryOperator&lt;String&gt; upperFun = s -&gt; s.toUpperCase(); // 截取 3 位 UnaryOperator&lt;String&gt; subFun = s -&gt; s.substring(0, 3); List&lt;String&gt; resultList = map(list, upperFun, subFun); System.out.println(resultList); &#125; public static &lt;T&gt; List&lt;T&gt; map(List&lt;T&gt; list, UnaryOperator&lt;T&gt;... unaryOperator) &#123; List&lt;T&gt; resultList = new ArrayList&lt;&gt;(); for (T t : list) &#123; for (UnaryOperator&lt;T&gt; operator : unaryOperator) &#123; t = operator.apply(t); &#125; resultList.add(t); &#125; return resultList; &#125;&#125; ps: 这边有个知识点,在java中,…表示可变长度参数列表，表示接受的参数为0到多个Object类型的对象，或者是一个Object[] 可变长度参数列表必须作为最后一位参数！","categories":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"函数接口","slug":"函数接口","permalink":"https://gwtt.github.io/tags/%E5%87%BD%E6%95%B0%E6%8E%A5%E5%8F%A3/"}]},{"title":"Redis客户端","slug":"Redis客户端","date":"2022-09-24T13:23:36.000Z","updated":"2022-09-27T10:22:54.078Z","comments":true,"path":"2022/09/24/Redis客户端/","link":"","permalink":"https://gwtt.github.io/2022/09/24/Redis%E5%AE%A2%E6%88%B7%E7%AB%AF/","excerpt":"","text":"下面介绍一下在Java程序中如何使用Redis客户端 Jedis Maven依赖 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.6.1&lt;/version&gt;&lt;/dependency&gt; 简单使用字符串123456&gt;//输出结果: OKjedis.set(&quot;hello&quot;, &quot;world&quot;);//输出结果: worldjedis.get(&quot;hello&quot;);//输出结果:1jedis.incr(&quot;counter&quot;); 哈希1234&gt;jedis.hset(&quot;myhash&quot;, &quot;f1&quot;, &quot;v1&quot;);jedis.hset(&quot;myhash&quot;, &quot;f2&quot;, &quot;v2&quot;);//输出结果 : &#123;f1=v1, f2=v2&#125;jedis.hgetAll(&quot;myhash&quot;); 列表12345&gt;jedis.rpush(&quot;mylist&quot;, &quot;1&quot;);jedis.rpush(&quot;mylist&quot;, &quot;2&quot;);jedis.rpush(&quot;mylist&quot;, &quot;3&quot;);//输出结果 : [1, 2, 3]jedis.lrange(&quot;mylist&quot;, 0, -1); 集合12345&gt;jedis.sadd(&quot; myset&quot;, &quot;a&quot;);jedis.sadd(&quot; myset&quot;, &quot;b&quot;);jedis.sadd(&quot; myset&quot;, &quot;a&quot;);//输出结果 : [b, a]jedis.smembers(&quot;myset&quot;); 有序集合12345&gt;jedis.zadd(&quot;myzset&quot;, 99, &quot;tom&quot;);jedis.zadd(&quot;myzset&quot;, 66, &quot;peter&quot;);jedis.zadd(&quot;myzset&quot;, 33, &quot;james&quot;);//输出结果 : [[[&quot;james&quot;],33.0], [[&quot;peter&quot;],66.0], [[&quot;tom&quot;],99.0]]jedis.zrangeWithScores(&quot;myzset&quot;, 0, -1); Jedis连接池jedis直连每次操作创建一个jedis对象，执行完毕后关闭连接，对应的就是一次Tcp连接。 123456//生成一个jedis对象，这个对象负责和指定Redis节点进行通信 Jedis jedis = new Jedis(&quot;119.23.226.29&quot;, 6379); //带密码需要执行认证方法 //jedis.auth(&quot;123456&quot;); jedis.set(&quot;hello&quot;, &quot;world&quot;); String value = jedis.get(&quot;hello&quot;); jedis连接池预先生成一批jedis连接对象放入连接池中，当需要对redis进行操作时从连接池中借用jedis对象，操作完成后归还。这样jedis对象可以重复使用，避免了频繁创建socket连接，节省了连接开销。 1234&gt;JedisPool pool = new JedisPool(&quot;127.0.0.1&quot;, 6379);Jedis jedis = pool.getResource();jedis.set(&quot;hello&quot;, &quot;world&quot;);String value = jedis.get(&quot;hello&quot;); lettuce Lettuce 是一个可伸缩线程安全的 Redis 客户端。多个线程可以共享同一个 RedisConnection。它利用优秀 netty NIO 框架来高效地管理多个连接。 RedisURI是redis连接的一些标准信息，比如需要提供数据库名称，密码，url，超时时间等。有三种方式可以创建： 123RedisURI.create(&quot;redis://localhost/&quot;);RedisURI.Builder.redis(&quot;localhost&quot;, 6379).auth(&quot;password&quot;).database(1).build();new RedisURI(&quot;localhost&quot;, 6379, 60, TimeUnit.SECONDS); 123456事务命令multi：用于标记事务块的开始,Redis会将后续的命令逐个放入队列中，然后使用exec原子化地执行这个命令队列exec：执行命令队列discard：清除命令队列watch：监视keyunwatch：清除监视key 12345678910111213141516171819202122RedisURI redisUri = RedisURI.builder()// &lt;1&gt; 创建单机连接的连接信息 .withHost(&quot;127.0.0.1&quot;) .withPort(6379) .withTimeout(Duration.of(10, ChronoUnit.SECONDS)) .build();RedisClient redisClient = RedisClient.create(redisUri);// &lt;2&gt; 创建客户端GenericObjectPool&lt;StatefulRedisConnection&lt;String, String&gt;&gt; genericObjectPool = ConnectionPoolSupport.createGenericObjectPool(() -&gt; redisClient.connect(), new GenericObjectPoolConfig&lt;&gt;());StatefulRedisConnection&lt;String, String&gt; connection = genericObjectPool.borrowObject();//StatefulRedisConnection&lt;String, String&gt; connection = redisClient.connect();// &lt;3&gt; 创建线程安全的连接RedisCommands&lt;String, String&gt; redisCommands = connection.sync();// &lt;4&gt; 创建同步命令//redisCommands.multi();//redisCommands.set(&quot;key&quot;, &quot;value&quot;);//redisCommands.set(&quot;key2&quot;, &quot;value2&quot;);//redisCommands.exec();SetArgs setArgs = SetArgs.Builder.nx().ex(5);String result = redisCommands.set(&quot;name&quot;, &quot;chen&quot;, setArgs);System.out.println(result);result = redisCommands.get(&quot;name&quot;);System.out.println(result);// ... 其他操作connection.close();// &lt;5&gt; 关闭连接redisClient.shutdown();// &lt;6&gt; 关闭客户端 Redisson Redisson - 是一个高级的分布式协调Redis客服端，能帮助用户在分布式环境中轻松实现一些Java的对象，Redisson、Jedis、Lettuce 是三个不同的操作 Redis 的客户端，Jedis、Lettuce 的 API 更侧重对 Reids 数据库的 CRUD（增删改查），而 Redisson API 侧重于分布式开发 12345Config config = new Config();config.useSingleServer().setAddress(&quot;redis://127.0.0.1:6379&quot;);RedissonClient redissonClient = Redisson.create(config);redissonClient.getBucket(&quot;name&quot;).set(&quot;chen&quot;);System.out.println(redissonClient.getBucket(&quot;name&quot;).get()); 之后有机会好好的补充一下 lua脚本 Redis中为什么引入Lua脚本？Redis是高性能的key-value内存数据库，它帮助我们解决了大部分业务问题；提供丰富的指令集合，据官网上统计有200多个命令。这些命令显然已经满足了我们的常规的业务场景需求。但是在某些特殊的场景下，业务需要原子性操作，redis原有的命令是无法完成，所以需要额外开发实现原子操作。 因为这样的问题，Redis为开发者提供了lua脚本的支持，用户可以向服务器发送lua脚本来执行自定义动作，以此获取脚本的响应数据。Redis本身又是单线程执行lua脚本，保证了lua脚本在处理逻辑过程中不会被任意其它请求打断。 什么是LuaLua是一种轻量小巧的脚本语言，用标准C语言编写并以源代码形式开放。 其设计目的就是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。因为广泛的应用于：游戏开发、独立应用脚本、Web 应用脚本、扩展和数据库插件等。 比如：Lua脚本用在很多游戏上，主要是Lua脚本可以嵌入到其他程序中运行，游戏升级的时候，可以直接升级脚本，而不用重新安装游戏。 使用Lua脚本的好处：12345① 支持原子性操作 - Redis会将整个脚本作为一个整体执行，中间不会被其他请求插入。因此在脚本运行过程中无需担心会出现竞态条件，无需使用事务② 降低网络开销 - 将多个请求通过脚本的形式一次发送到服务器，减少了网络的时延③ 脚本复用 - 客户端发送的脚本可支持永久存在redis中，这样其他客户端可以复用这一脚本，而不需要使用代码完成相同的逻辑。 Redis中Lua的常用命令 命令不多，就下面这几个：- EVAL- EVALSHA- SCRIPT LOAD - SCRIPT EXISTS- SCRIPT FLUSH- SCRIPT KILL EVAL命令命令格式：EVAL script numkeys key [key …] arg [arg …]- script参数是一段 Lua5.1 脚本程序。脚本不必(也不应该)定义为一个 Lua 函数- numkeys指定后续参数有几个key，即：key [key …]中key的个数。如没有key，则为0- key [key …] 从 EVAL 的第三个参数开始算起，表示在脚本中所用到的那些 Redis 键(key)。在Lua脚本中通过KEYS[1], KEYS[2]获取。- arg [arg …] 附加参数。在Lua脚本中通过ARGV[1],ARGV[2]获取。 1234567891011121314151617181920212223242526272829// 例1：numkeys=1，keys数组只有1个元素key1，arg数组无元素127.0.0.1:6379&gt; EVAL &quot;return KEYS[1]&quot; 1 key1&quot;key1&quot;// 例2：numkeys=0，keys数组无元素，arg数组元素中有1个元素value1127.0.0.1:6379&gt; EVAL &quot;return ARGV[1]&quot; 0 value1&quot;value1&quot;// 例3：numkeys=2，keys数组有两个元素key1和key2，arg数组元素中有两个元素first和second // 其实&#123;KEYS[1],KEYS[2],ARGV[1],ARGV[2]&#125;表示的是Lua语法中“使用默认索引”的table表，// 相当于java中的map中存放四条数据。Key分别为：1、2、3、4，而对应的value才是：KEYS[1]、KEYS[2]、ARGV[1]、ARGV[2]// 举此例子仅为说明eval命令中参数的如何使用。项目中编写Lua脚本最好遵从key、arg的规范。127.0.0.1:6379&gt; eval &quot;return &#123;KEYS[1],KEYS[2],ARGV[1],ARGV[2]&#125;&quot; 2 key1 key2 first second 1) &quot;key1&quot;2) &quot;key2&quot;3) &quot;first&quot;4) &quot;second&quot;// 例4：使用了redis为lua内置的redis.call函数// 脚本内容为：先执行SET命令，在执行EXPIRE命令// numkeys=1，keys数组有一个元素userAge（代表redis的key）// arg数组元素中有两个元素：10（代表userAge对应的value）和60（代表redis的存活时间）127.0.0.1:6379&gt; EVAL &quot;redis.call(&#x27;SET&#x27;, KEYS[1], ARGV[1]);redis.call(&#x27;EXPIRE&#x27;, KEYS[1], ARGV[2]); return 1;&quot; 1 userAge 10 60(integer) 1127.0.0.1:6379&gt; get userAge&quot;10&quot;127.0.0.1:6379&gt; ttl userAge(integer) 44 通过上面的例4，我们可以发现，脚本中使用redis.call()去调用redis的命令。在 Lua 脚本中，可以使用两个不同函数来执行 Redis 命令，它们分别是： redis.call() 和 redis.pcall()这两个函数的唯一区别在于它们使用不同的方式处理执行命令所产生的错误，差别如下： 错误处理当 redis.call() 在执行命令的过程中发生错误时，脚本会停止执行，并返回一个脚本错误，错误的输出信息会说明错误造成的原因： 12345127.0.0.1:6379&gt; lpush foo a(integer) 1127.0.0.1:6379&gt; eval &quot;return redis.call(&#x27;get&#x27;, &#x27;foo&#x27;)&quot; 0(error) ERR Error running script (call to f_282297a0228f48cd3fc6a55de6316f31422f5d17): ERR Operation against a key holding the wrong kind of value 和 redis.call() 不同， redis.pcall() 出错时并不引发(raise)错误，而是返回一个带 err 域的 Lua 表(table)，用于表示错误： 12127.0.0.1:6379&gt; EVAL &quot;return redis.pcall(&#x27;get&#x27;, &#x27;foo&#x27;)&quot; 0(error) ERR Operation against a key holding the wrong kind of value SCRIPT LOAD命令 和 EVALSHA命令SCRIPT LOAD命令格式：SCRIPT LOAD scriptEVALSHA命令格式：EVALSHA sha1 numkeys key [key …] arg [arg …] 这两个命令放在一起讲的原因是：EVALSHA 命令中的sha1参数，就是SCRIPT LOAD 命令执行的结果。 SCRIPT LOAD 将脚本 script 添加到Redis服务器的脚本缓存中，并不立即执行这个脚本，而是会立即对输入的脚本进行求值。并返回给定脚本的 SHA1 校验和。如果给定的脚本已经在缓存里面了，那么不执行任何操作。 在脚本被加入到缓存之后，在任何客户端通过EVALSHA命令，可以使用脚本的 SHA1 校验和来调用这个脚本。脚本可以在缓存中保留无限长的时间，直到执行SCRIPT FLUSH为止。 1234567891011## SCRIPT LOAD加载脚本，并得到sha1值127.0.0.1:6379&gt; SCRIPT LOAD &quot;redis.call(&#x27;SET&#x27;, KEYS[1], ARGV[1]);redis.call(&#x27;EXPIRE&#x27;, KEYS[1], ARGV[2]); return 1;&quot;&quot;6aeea4b3e96171ef835a78178fceadf1a5dbe345&quot;## EVALSHA使用sha1值，并拼装和EVAL类似的numkeys和key数组、arg数组，调用脚本。127.0.0.1:6379&gt; EVALSHA 6aeea4b3e96171ef835a78178fceadf1a5dbe345 1 userAge 10 60(integer) 1127.0.0.1:6379&gt; get userAge&quot;10&quot;127.0.0.1:6379&gt; ttl userAge(integer) 43 SCRIPT EXISTS 命令命令格式：SCRIPT EXISTS sha1 [sha1 …]作用：给定一个或多个脚本的 SHA1 校验和，返回一个包含 0 和 1 的列表，表示校验和所指定的脚本是否已经被保存在缓存当中 1234567127.0.0.1:6379&gt; SCRIPT EXISTS 6aeea4b3e96171ef835a78178fceadf1a5dbe3451) (integer) 1127.0.0.1:6379&gt; SCRIPT EXISTS 6aeea4b3e96171ef835a78178fceadf1a5dbe3461) (integer) 0127.0.0.1:6379&gt; SCRIPT EXISTS 6aeea4b3e96171ef835a78178fceadf1a5dbe345 6aeea4b3e96171ef835a78178fceadf1a5dbe3661) (integer) 12) (integer) 0 SCRIPT FLUSH 命令命令格式：SCRIPT FLUSH作用：清除Redis服务端所有 Lua 脚本缓存 123456127.0.0.1:6379&gt; SCRIPT EXISTS 6aeea4b3e96171ef835a78178fceadf1a5dbe3451) (integer) 1127.0.0.1:6379&gt; SCRIPT FLUSHOK127.0.0.1:6379&gt; SCRIPT EXISTS 6aeea4b3e96171ef835a78178fceadf1a5dbe3451) (integer) 0 SCRIPT KILL 命令命令格式：SCRIPT FLUSH作用：杀死当前正在运行的 Lua 脚本，当且仅当这个脚本没有执行过任何写操作时，这个命令才生效。 这个命令主要用于终止运行时间过长的脚本，比如一个因为 BUG 而发生无限 loop 的脚本，诸如此类。 假如当前正在运行的脚本已经执行过写操作，那么即使执行SCRIPT KILL，也无法将它杀死，因为这是违反 Lua 脚本的原子性执行原则的。在这种情况下，唯一可行的办法是使用SHUTDOWN NOSAVE命令，通过停止整个 Redis 进程来停止脚本的运行，并防止不完整(half-written)的信息被写入数据库中。 Redis执行Lua脚本文件 在第二章中介绍的命令，是在redis客户端中使用命令进行操作。该章节介绍的是直接执行 Lua 的脚本文件。 编写Lua脚本文件12345678910local key = KEYS[1]local val = redis.call(&quot;GET&quot;, key);if val == ARGV[1]then redis.call(&#x27;SET&#x27;, KEYS[1], ARGV[2]) return 1else return 0end 执行Lua脚本文件12执行命令： redis-cli -a 密码 --eval Lua脚本路径 key [key …] , arg [arg …] 如：redis-cli -a 123456 --eval ./Redis_CompareAndSet.lua userName , zhangsan lisi 此处敲黑板，注意啦！！！“–eval”而不是命令模式中的”eval”，一定要有前端的两个-脚本路径后紧跟key [key …]，相比命令行模式，少了numkeys这个key数量值key [key …] 和 arg [arg …] 之间的“ , ”，英文逗号前后必须有空格，否则死活都报错 12345678910111213## Redis客户端执行127.0.0.1:6379&gt; set userName zhangsan OK127.0.0.1:6379&gt; get userName&quot;zhangsan&quot;## linux服务器执行## 第一次执行：compareAndSet成功，返回1## 第二次执行：compareAndSet失败，返回0[root@vm01 learn_lua]# redis-cli -a 123456 --eval Redis_CompareAndSet.lua userName , zhangsan lisi(integer) 1[root@vm01 learn_lua]# redis-cli -a 123456 --eval Redis_CompareAndSet.lua userName , zhangsan lisi(integer) 0 使用Lua控制IP访问频率 需求：实现一个访问频率控制，某个IP在短时间内频繁访问页面，需要记录并检测出来，就可以通过Lua脚本高效的实现。小声说明：本实例针对固定窗口的访问频率，而动态的非滑动窗口。即：如果规定一分钟内访问10次，记为超限。在本实例中前一分钟的最后一秒访问9次，下一分钟的第1秒又访问9次，不计为超限。脚本如下： 1234567891011local visitNum = redis.call(&#x27;incr&#x27;, KEYS[1])if visitNum == 1 then redis.call(&#x27;expire&#x27;, KEYS[1], ARGV[1])end if visitNum &gt; tonumber(ARGV[2]) then return 0endreturn 1; 演示如下： 123456789101112## LimitIP:127.0.0.1为key， 10 3表示：同一IP在10秒内最多访问三次## 前三次返回1，代表未被限制；第四、五次返回0，代表127.0.0.1这个ip已被拦截[root@vm01 learn_lua]# redis-cli -a 123456 --eval Redis_LimitIpVisit.lua LimitIP:127.0.0.1 , 10 3 (integer) 1[root@vm01 learn_lua]# redis-cli -a 123456 --eval Redis_LimitIpVisit.lua LimitIP:127.0.0.1 , 10 3 (integer) 1[root@vm01 learn_lua]# redis-cli -a 123456 --eval Redis_LimitIpVisit.lua LimitIP:127.0.0.1 , 10 3 (integer) 1[root@vm01 learn_lua]# redis-cli -a 123456 --eval Redis_LimitIpVisit.lua LimitIP:127.0.0.1 , 10 3 (integer) 0[root@vm01 learn_lua]# redis-cli -a 123456 --eval Redis_LimitIpVisit.lua LimitIP:127.0.0.1 , 10 3 (integer) 0","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Redis","slug":"Redis","permalink":"https://gwtt.github.io/tags/Redis/"}]},{"title":"Mysql行锁","slug":"Mysql行锁","date":"2022-09-22T08:38:22.000Z","updated":"2022-09-22T15:26:58.937Z","comments":true,"path":"2022/09/22/Mysql行锁/","link":"","permalink":"https://gwtt.github.io/2022/09/22/Mysql%E8%A1%8C%E9%94%81/","excerpt":"","text":"记录锁(Record Locks) 记录锁是 封锁记录，记录锁也叫行锁，例如： 1SELECT * FROM `test` WHERE `id`=1 FOR UPDATE; 它会在 id=1 的记录上加上记录锁，以阻止其他事务插入，更新，删除 id=1 这一行。 间隙锁(Gap Locks)（重点）间隙锁是封锁索引记录中的间隔，或者第一条索引记录之前的范围，又或者最后一条索引记录之后的范围。 产生间隙锁的条件（RR事务隔离级别下；）： 使用普通索引锁定； 使用多列唯一索引； 使用唯一索引锁定多行记录。 以上情况，都会产生间隙锁： 对于使用唯一索引来搜索并给某一行记录加锁的语句，不会产生间隙锁。（这不包括搜索条件仅包括多列唯一索引的一些列的情况；在这种情况下，会产生间隙锁。）例如，如果id列具有唯一索引，则下面的语句仅对具有id值100的行使用记录锁，并不会产生间隙锁： 1SELECT * FROM child WHERE id = 100 FOR UPDATE; 这条语句，就只会产生记录锁，不会产生间隙锁。 唯一索引的间隙锁测试环境： 环境：MySQL，InnoDB，默认的隔离级别（RR） 数据表： 12345CREATE TABLE `test` ( `id` int(1) NOT NULL AUTO_INCREMENT, `name` varchar(8) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 数据： 1234INSERT INTO `test` VALUES (&#x27;1&#x27;, &#x27;小罗&#x27;);INSERT INTO `test` VALUES (&#x27;5&#x27;, &#x27;小黄&#x27;);INSERT INTO `test` VALUES (&#x27;7&#x27;, &#x27;小明&#x27;);INSERT INTO `test` VALUES (&#x27;11&#x27;, &#x27;小红&#x27;); 在进行测试之前，我们先来看看test表中存在的隐藏间隙： (-infinity, 1] (1, 5] (5, 7] (7, 11] (11, +infinity] 只使用记录锁，不会产生间隙锁 我们现在进行以下几个事务的测试： 1234567891011121314151617/* 开启事务1 */BEGIN;/* 查询 id = 5 的数据并加记录锁 */SELECT * FROM `test` WHERE `id` = 5 FOR UPDATE;/* 延迟30秒执行，防止锁释放 */SELECT SLEEP(30);# 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句/* 事务2插入一条 name = &#x27;小张&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (4, &#x27;小张&#x27;); # 正常执行/* 事务3插入一条 name = &#x27;小张&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (8, &#x27;小东&#x27;); # 正常执行/* 提交事务1，释放事务1的锁 */COMMIT; 上诉的案例，由于主键是唯一索引，而且是只使用一个索引查询，并且只锁定一条记录，所以以上的例子，只会对 id = 5 的数据加上记录锁，而不会产生间隙锁。 产生间隙锁 然后回滚事务测试，我们继续在 id 唯一索引列上做以下的测试： 123456789101112131415161718192021222324252627282930/* 开启事务1 */BEGIN;/* 查询 id 在 5 - 7 范围的数据并加记录锁 */SELECT * FROM `test` WHERE `id` BETWEEN 5 AND 7 FOR UPDATE;/* 延迟30秒执行，防止锁释放 */SELECT SLEEP(30);# 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句/* 事务2插入一条 id = 3，name = &#x27;小张1&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (3, &#x27;小张1&#x27;); # 正常执行/* 事务3插入一条 id = 4，name = &#x27;小白&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (4, &#x27;小白&#x27;); # 正常执行/* 事务4插入一条 id = 6，name = &#x27;小东&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (6, &#x27;小东&#x27;); # 阻塞/* 事务5插入一条 id = 8， name = &#x27;大罗&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (8, &#x27;大罗&#x27;); # 正常执行/* 事务6插入一条 id = 9， name = &#x27;大东&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (9, &#x27;大东&#x27;); # 正常执行/* 事务7插入一条 id = 11， name = &#x27;李西&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (11, &#x27;李西&#x27;); # 正常执行/* 事务8插入一条 id = 12， name = &#x27;张三&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (12, &#x27;张三&#x27;); # 正常执行/* 提交事务1，释放事务1的锁 */COMMIT; 从上面我们可以看到[5,7]这一个区间，都不可插入数据，其它区间，都可以正常插入数据。所以我们可以得出结论：当我们给 [5,7]这个区间加锁的时候，会锁住 [5,7]这一个区间。 我们再来测试如果我们锁住不存在的数据时，会怎样： 1234567891011121314151617/* 开启事务1 */BEGIN;/* 查询 id = 3 这一条不存在的数据并加记录锁 */SELECT * FROM `test` WHERE `id` = 3 FOR UPDATE;/* 延迟30秒执行，防止锁释放 */SELECT SLEEP(30);# 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句/* 事务2插入一条 id = 3，name = &#x27;小张1&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (2, &#x27;小张1&#x27;); # 阻塞/* 事务3插入一条 id = 4，name = &#x27;小白&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (4, &#x27;小白&#x27;); # 阻塞/* 事务4插入一条 id = 6，name = &#x27;小东&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (6, &#x27;小东&#x27;); # 正常执行/* 事务5插入一条 id = 8， name = &#x27;大罗&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (8, &#x27;大罗&#x27;); # 正常执行/* 提交事务1，释放事务1的锁 */COMMIT; 我们可以看出，指定查询某一条记录时，如果这条记录不存在，会产生间隙锁。 结论 对于指定查询某一条记录的加锁语句，如果该记录不存在，会产生记录锁和间隙锁，如果记录存在，则只会产生记录锁，如：WHERE id = 5 FOR UPDATE; 对于查找某一范围内的查询语句，会产生间隙锁，如：WHERE id BETWEEN 5 AND 7 FOR UPDATE; 普通索引的间隙锁数据准备 创建 test1 表： 12345678# 注意：number 不是唯一值CREATE TABLE `test1` ( `id` int(1) NOT NULL AUTO_INCREMENT, `number` int(1) NOT NULL COMMENT &#x27;数字&#x27;, PRIMARY KEY (`id`), KEY `number` (`number`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 在这张表上，我们有 id number 这两个字段，id 是我们的主键，我们在 number 上，建立了一个普通索引，为了方便我们后面的测试。现在我们要先加一些数据： 1234INSERT INTO `test1` VALUES (1, 1);INSERT INTO `test1` VALUES (5, 3);INSERT INTO `test1` VALUES (7, 8);INSERT INTO `test1` VALUES (11, 12); 在进行测试之前，我们先来看看test1表中 number 索引存在的隐藏间隙： (-infinity, 1] (1, 3] (3, 8] (8, 12] (12, +infinity] 案例说明 我们执行以下的事务（事务1最后提交），分别执行下面的语句： 1234567891011121314151617181920212223/* 开启事务1 */BEGIN;/* 查询 number = 3 的数据并加记录锁 */SELECT * FROM `test1` WHERE `number` = 3 FOR UPDATE;/* 延迟30秒执行，防止锁释放 */SELECT SLEEP(30);# 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句/* 事务2插入一条 number = 0 的数据 */INSERT INTO `test1` (`number`) VALUES (0); # 正常执行/* 事务3插入一条 number = 1 的数据 */INSERT INTO `test1` (`number`) VALUES (1); # 被阻塞/* 事务4插入一条 number = 2 的数据 */INSERT INTO `test1` (`number`) VALUES (2); # 被阻塞/* 事务5插入一条 number = 4 的数据 */INSERT INTO `test1` (`number`) VALUES (4); # 被阻塞/* 事务6插入一条 number = 8 的数据 */INSERT INTO `test1` (`number`) VALUES (8); # 正常执行/* 事务7插入一条 number = 9 的数据 */INSERT INTO `test1` (`number`) VALUES (9); # 正常执行/* 事务8插入一条 number = 10 的数据 */INSERT INTO `test1` (`number`) VALUES (10); # 正常执行/* 提交事务1 */COMMIT; 我们会发现有些语句可以正常执行，有些语句被阻塞了。我们再来看看我们表中的数据： 执行之后的数据 这里可以看到，number (1 - 8) 的间隙中，插入语句都被阻塞了，而不在这个范围内的语句，正常执行，这就是因为有间隙锁的原因。我们再进行以下的测试，方便我们更好的理解间隙锁的区域（我们要将数据还原成原来的那样）： 12345678910111213141516171819202122/* 开启事务1 */BEGIN;/* 查询 number = 3 的数据并加记录锁 */SELECT * FROM `test1` WHERE `number` = 3 FOR UPDATE;/* 延迟30秒执行，防止锁释放 */SELECT SLEEP(30);/* 事务1插入一条 id = 2， number = 1 的数据 */INSERT INTO `test1` (`id`, `number`) VALUES (2, 1); # 阻塞/* 事务2插入一条 id = 3， number = 2 的数据 */INSERT INTO `test1` (`id`, `number`) VALUES (3, 2); # 阻塞/* 事务3插入一条 id = 6， number = 8 的数据 */INSERT INTO `test1` (`id`, `number`) VALUES (6, 8); # 阻塞/* 事务4插入一条 id = 8， number = 8 的数据 */INSERT INTO `test1` (`id`, `number`) VALUES (8, 8); # 正常执行/* 事务5插入一条 id = 9， number = 9 的数据 */INSERT INTO `test1` (`id`, `number`) VALUES (9, 9); # 正常执行/* 事务6插入一条 id = 10， number = 12 的数据 */INSERT INTO `test1` (`id`, `number`) VALUES (10, 12); # 正常执行/* 事务7修改 id = 11， number = 12 的数据 */UPDATE `test1` SET `number` = 5 WHERE `id` = 11 AND `number` = 12; # 阻塞/* 提交事务1 */COMMIT; 我们来看看结果： 执行后的数据 这里有一个奇怪的现象： 事务3添加 id = 6，number = 8 的数据，给阻塞了； 事务4添加 id = 8，number = 8 的数据，正常执行了。 事务7将 id = 11，number = 12 的数据修改为 id = 11， number = 5的操作，给阻塞了； 这是为什么呢？我们来看看下边的图，大家就明白了。 隐藏的间隙锁图 从图中可以看出，当 number 相同时，会根据主键 id 来排序，所以： 事务3添加的 id = 6，number = 8，根据id排序,这条数据是在 （3, 8） 的区间里边，所以会被阻塞； 事务4添加的 id = 8，number = 8，根据id排序,这条数据则是在（8, 12）区间里边，所以不会被阻塞； 事务7的修改语句相当于在 （3, 8） 的区间里边插入一条数据，所以也被阻塞了。 结论 在普通索引列上，不管是何种查询，只要加锁，都会产生间隙锁，这跟唯一索引不一样； 在普通索引跟唯一索引中，数据间隙的分析，数据行是优先根据普通索引排序，再根据唯一索引排序。 临键锁(Next-key Locks)临键锁，是记录锁与间隙锁的组合，它的封锁范围，既包含索引记录，又包含索引区间。 注：临键锁的主要目的，也是为了避免幻读(Phantom Read)。如果把事务的隔离级别降级为RC，临键锁则也会失效。 只使用记录锁，不会产生间隙锁 我们现在进行以下几个事务的测试： 1234567891011121314151617/* 开启事务1 */BEGIN;/* 查询 id = 5 的数据并加记录锁 */SELECT * FROM `test` WHERE `id` = 5 FOR UPDATE;/* 延迟30秒执行，防止锁释放 */SELECT SLEEP(30);# 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句/* 事务2插入一条 name = &#x27;小张&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (4, &#x27;小张&#x27;); # 正常执行/* 事务3插入一条 name = &#x27;小张&#x27; 的数据 */INSERT INTO `test` (`id`, `name`) VALUES (8, &#x27;小东&#x27;); # 正常执行/* 提交事务1，释放事务1的锁 */COMMIT; 上诉的案例，由于主键是唯一索引，而且是只使用一个索引查询，并且只锁定一条记录，所以以上的例子，只会对 id = 5 的数据加上记录锁，而不会产生间隙锁。 记录锁、间隙锁、临键锁，都属于排它锁； 记录锁就是锁住一行记录； 间隙锁只有在事务隔离级别 RR 中才会产生； 唯一索引只有锁住多条记录或者一条不存在的记录的时候，才会产生间隙锁，指定给某条存在的记录加锁的时候，只会加记录锁，不会产生间隙锁； 普通索引不管是锁住单条，还是多条记录，都会产生间隙锁； 间隙锁会封锁该条记录相邻两个键之间的空白区域，防止其它事务在这个区域内插入、修改、删除数据，这是为了防止出现 幻读 现象； 普通索引的间隙，优先以普通索引排序，然后再根据主键索引排序（多普通索引情况还未研究）； 事务级别是RC（读已提交）级别的话，间隙锁将会失效。","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Mysql","slug":"Mysql","permalink":"https://gwtt.github.io/tags/Mysql/"}]},{"title":"4/22美团后端实习面经一面","slug":"4月22日美团后端实习面经一面","date":"2022-09-19T15:15:22.000Z","updated":"2022-09-21T15:25:33.058Z","comments":true,"path":"2022/09/19/4月22日美团后端实习面经一面/","link":"","permalink":"https://gwtt.github.io/2022/09/19/4%E6%9C%8822%E6%97%A5%E7%BE%8E%E5%9B%A2%E5%90%8E%E7%AB%AF%E5%AE%9E%E4%B9%A0%E9%9D%A2%E7%BB%8F%E4%B8%80%E9%9D%A2/","excerpt":"","text":"1.自我介绍 略 2.介绍项目（基本面试官好像都喜欢挖项目） 略，后面补充 3.项目里提到的Mysql，开始展开提问 略 4.mysql的索引引擎，有什么，之间区别 有InnoDB,MyISAM,Memory InnoDB: 主键自增 支持外键 DML操作支持事务 支持行级锁 MyISAM: 支持表锁，不支持行锁 不支持外键 不支持事务 占用空间小，访问速度快 Memory: 内存存放 支持哈希索引 5.InnoDB采用什么结构存储？为什么？ B+树 节点排序（用来加快查询速度） 一个节点多个元素存取（B树高度不会很高） 叶子节点有指针（可以方便支持全表扫描，范围查找） 叶子节点冗余（提升范围查找的效率） 6.B+树存储怎么能确定高度？ InnoDB最小存储单位是页，叶子节点和非叶子节点最小单位都是页，页大小Mysql 默认设定16384字节，约为16KB。我们假设主键ID为bigint类型，长度为8字节，而指针大小在InnoDB源码中设置为6字节，这样一共14字节 我们一个页中能存放多少这样的索引元素，其实就代表有多少指针，即16384/14=1170; 1假设一行记录的数据大小为1k，实际上现在很多互联网业务数据记录大小通常就是1K左右 高度为2的B+树能存放1170×16=18720高度为3的B+树能存放1170×1170×16 = 21902400 7.最左匹配（给举个例子，说出能否用索引） 略 8.mysql中行锁具体怎么锁（当时理解错了，答得MVCC，面试官也是耐心听完了，然后问我，那行锁呢？就很尴尬） 用记录锁、间隙锁、临键锁（都是排他锁） (1)记录锁(Record): 通过主键或唯一索引加锁，锁定某行记录，锁定的是已存在的记录 (2)间隙锁(Gap): 锁定的是索引记录中的间隔，锁定的是未存在记录的区间。 (3)临键锁(Next-key): 既包含已存在的记录，又包含未存在记录的区间（记录锁+间隙锁） 锁的触发条件 1.记录锁(Record)触发条件: 查询的条件中只包含表中存在的记录 2.间隙锁(Gap)触发条件: 查询的条件中不包含表中任何记录 3.临键锁(Next-key)触发条件: 查询的条件中既包含表中存在的记录，也包含表中不存在的记录 9.redis在应用时怎么保证数据一致性的（问的是项目里用的时候） 采用延时双删策略（1）先淘汰缓存（2）再写数据库（这两步和原来一样）（3）休眠1秒，再次淘汰缓存 先更新数据库，再删除缓存（推荐） 10.有没有想过用本地缓存，不用redis（当时不了解memcache，随便说了个更新频繁） 使用ConcurrentHashMap实现本地缓存缓存的本质就是存储在内存中的KV数据结构，对应的就是jdk中线程安全的ConcurrentHashMap，但是要实现缓存，还需要考虑淘汰、最大限制、缓存过期时间淘汰等等功能； 优点是实现简单，不需要引入第三方包，比较适合一些简单的业务场景。缺点是如果需要更多的特性，需要定制化开发，成本会比较高，并且稳定性和可靠性也难以保障。对于比较复杂的场景，建议使用比较稳定的开源工具。 基于Guava Cache实现本地缓存Guava是Google团队开源的一款 Java 核心增强库，包含集合、并发原语、缓存、IO、反射等工具箱，性能和稳定性上都有保障，应用十分广泛。Guava Cache支持很多特性： 支持最大容量限制支持两种过期删除策略（插入时间和访问时间）支持简单的统计功能基于LRU算法实现 11.接10问，redis一般存哪种数据结构（存的list，所以本地缓存用不了） 存String 12.想没想过redis宕机怎么办？ 如果是一台机器，利用AOF和RDB机制进行redis数据恢复 从机宕机: 只要把从的redis重新启动，再和主的进行连接就可以 如果从redis上面做数据的持久化，可以直接连接到主的上面，只要实现增量备份。 主机宕机: 先把从的redis升级为主的redis. 执行slave of one命令原来的主的可以重新启动，作为从的redis, 连接到主的redis上面做主从复制。可以使用Redis 提供哨兵 机制来简化上面的操作。 13.了不了解垃圾回收器，说一下CMS的过程？ 初始标记 并发标记 重新标记 并发清除 14.FullGC发生的条件？（面试官说应该有三种，只说上两种） 年老代（Tenured）被写满； 持久代（Perm）被写满； System.gc()被显示调用； 上一次GC之后Heap的各域分配策略动态变化； 15.了解线程池吗？主要参数？ 了解 corePoolSize核心线程数 maximumPoolSize最大线程数 keepAliveTime线程空闲时间 unit时间单位 workQueue阻塞队列 threadFactory线程工厂 handler任务拒绝处理器 16.线程池的执行过程？ 当我们利用线程池执行任务时: 1.如果此时线程池中的线程数量小于corePoolSize，即使线程池中的线程都处于空闲状态，也要创建新的线程来处理被添加的任务。 ⒉.如果此时线程池中的线程数量等于corePoolSize，但是缓冲队列workQueue未满，那么任务被放入缓冲队列。 3.如果此时线程池中的线程数量大于等于corePoolSize，缓冲队列workQueue满，并且线程池中的数量小于maximumPoolSize，建新的线程来处理被添加的任务。 4.如果此时线程池中的线程数量大于corePoolSize，缓冲队列workQueue满，并且线程池中的数量等于maximumPoolSize，那么通过 handler所指定的策略来处理此任务。 5.当线程池中的线程数量大于corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终止。这样，线程池可以动态的调整池中的线程数 17.可重入锁是怎么实现的？ 利用AQS实现(AbstractQueueSynchronize)抽象队列同步器 18.由于17问说不知道，提醒出了18问AQS知道吗？（答完之后，面试官说这就是可重入锁实现） AQS 中有两个重要的东西，一个以Node为节点实现的链表的队列(CHL队列)，还有一个STATE标志，并且通过CAS来改变它的值。 CLH队列： 链表结构，在头尾结点中，需要特别指出的是头结点是一个空对象结点，无任何意义，即傀儡结点； 每一个Node结点都维护了一个指向前驱的指针和指向后驱的指针，结点与结点之间相互关联构成链表； 入队在尾，出队在头，出队后需要激活该出队结点的后继结点，若后继结点为空或后继结点waitStatus&gt;0，则从队尾向前遍历取waitStatus&lt;0的触发阻塞唤醒； 队列中节点状态值（waitStatus，只能为以下值） 12345678//常量：表示节点的线程是已被取消的static final int CANCELLED = 1;//常量：表示当前节点的后继节点的线程需要被唤醒static final int SIGNAL = -1;//常量：表示线程正在等待某个条件static final int CONDITION = -2;//常量：表示下一个共享模式的节点应该无条件的传播下去static final int PROPAGATE = -3; 19.CAS是什么 CAS是compare and swap的缩写，即我们所说的比较交换。cas是一种基于锁的操作，而且是乐观锁。在java中锁分为乐观锁和悲观锁。悲观锁是将资源锁住，等一个之前获得锁的线程释放锁之后，下一个线程才可以访问。而乐观锁采取了一种宽泛的态度，通过某种方式不加锁来处理资源，比如通过给记录加version来获取数据，性能较悲观锁有很大的提高。 CAS 操作包含三个操作数 —— 内存位置（V）、预期原值（A）和新值(B)。如果内存地址里面的值和A的值是一样的，那么就将内存里面的值更新成B。 CAS是通过无限循环来获取数据的，若果在第一轮循环中，a线程获取地址里面的值被b线程修改了，那么a线程需要自旋，到下次循环才有可能机会执行。 ps: 123问题:①.CAS容易造成ABA问题。一个线程a将数值改成了b，接着又改成了a，此时CAS认为是没有变化，其实是已经变化过了，而这个问题的解决方案可以使用版本号标识，每操作一次version加1。在java5中，已经提供了AtomicStampedReference来解决问题。②.CAS造成CPU利用率增加。之前说过了CAS里面是一个循环判断的过程，如果线程一直没有获取到状态，cpu资源会一直被占用。 反问20.打算实习时间 大约寒假就可以开始 转载美团Java后端实习面经（一二面），已接offer~_牛客博客 (nowcoder.net)","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"Java 线程和操作系统的线程有啥区别","slug":"Java 线程和操作系统的线程有啥区别","date":"2022-09-17T09:38:23.000Z","updated":"2022-09-19T15:04:53.671Z","comments":true,"path":"2022/09/17/Java 线程和操作系统的线程有啥区别/","link":"","permalink":"https://gwtt.github.io/2022/09/17/Java%20%E7%BA%BF%E7%A8%8B%E5%92%8C%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%BA%BF%E7%A8%8B%E6%9C%89%E5%95%A5%E5%8C%BA%E5%88%AB/","excerpt":"","text":"1. 用户空间和内核空间 至于什么是系统空间和用户空间也非常好理解：在操作系统中，内存通常会被分成用户空间（User space）与内核空间（Kernel space）这两个部分。当进程/线程运行在用户空间时就处于用户态，运行在内核空间时就处于内核态： 运行在内核态的程序可以访问用户空间和内核空间，或者说它可以访问计算机的任何资源，不受限制，为所欲为，例如协调 CPU 资源，分配内存资源，提供稳定的环境供应用程序运行等 而应用程序基本都是运行在用户态的，或者说用户态就是提供应用程序运行的空间。运行在用户态的程序只能访问用户空间 那为什么要区分用户态和内核态呢？ 其实早期操作系统是不区分用户态和内核态的，也就是说应用程序可以访问任意内存空间，如果程序不稳定常常会让系统崩溃，比如清除了操作系统的内存数据。为此大佬们设计出了一套规则：对于那些比较危险的操作需要切到内核态才能运行，比如 CPU、内存、设备等资源管理器程序就应该在内核态运行，否则安全性没有保证。 举个例子，对于文件系统和数据来说，文件系统数据和管理就必须放在内核态，但是用户的数据和管理可以放在用户态。 用户态的程序不能随意操作内核地址空间，这样有效地防止了操作系统程序受到应用程序的侵害。 那如果处于用户态的程序想要访问内核空间的话怎么办呢？就需要进行系统调用从用户态切换到内核态。 2. 操作系统线程① 在用户空间中实现线程在早期的操作系统中，所有的线程都是在用户空间下实现的，操作系统只能看到线程所属的进程，而不能看到线程。 从我们开发者的角度来理解用户级线程就是说：在这种模型下，我们需要自己定义线程的数据结构、创建、销毁、调度和维护等，这些线程运行在操作系统的某个进程内，然后操作系统直接对进程进行调度。 这种方式的好处一目了然，首先第一点，就是即使操作系统原生不支持线程，我们也可以通过库函数来支持线程；第二点，线程的调度只发生在用户态，避免了操作系统从内核态到用户态的转换开销。 当然缺点也很明显：由于操作系统看不见线程，不知道线程的存在，而 CPU 的时间片切换是以进程为维度的，所以如果进程中某个线程进行了耗时比较长的操作，那么由于用户空间中没有时钟中断机制，就会导致此进程中的其它线程因为得不到 CPU 资源而长时间的持续等待；另外，如果某个线程进行系统调用时比如缺页中断而导致了线程阻塞，此时操作系统也会阻塞住整个进程，即使这个进程中其它线程还在工作。 ② 在内核空间中实现线程 所谓内核级线程就是运行在内核空间的线程， 直接由内核负责，只能由内核来完成线程的调度。 几乎所有的现代操作系统，包括 Windows、Linux、Mac OS X 和 Solaris 等，都支持内核线程。 每个内核线程可以视为内核的一个分身，这样操作系统就有能力同时处理多件事情，支持多线程的内核就叫做多线程内核（Multi-Threads Kernel）。 从我们开发者的角度来理解内核级线程就是说：我们可以直接使用操作系统中已经内置好的线程，线程的创建、销毁、调度和维护等，都是直接由操作系统的内核来实现，我们只需要使用系统调用就好了，不需要像用户级线程那样自己设计线程调度等。 上图画的是 1：1 的线程模型，所谓线程模型，也就是用户线程和内核线程之间的关联方式，线程模型当然不止 1：1 这一种，下面我们来详细解释以下这三种多线程模型： 1）多对一线程模型： 在多对一模型中，多个用户级线程映射到某一个内核线程上 线程管理由用户空间中的线程库处理，这非常有效 但是，如果进行了阻塞系统调用，那么即使其他用户线程能够继续，整个进程也会阻塞 由于单个内核线程只能在单个 CPU 上运行，因此多对一模型不允许在多个 CPU 之间拆分单个进程 从并发性角度来总结下，虽然多对一模型允许开发人员创建任意多的用户线程，但是由于内核只能一次调度一个线程，所以并未增加并发性。现在已经几乎没有操作系统来使用这个模型了，因为它无法利用多个处理核。 2）一对一线程模型： 一对一模型克服了多对一模型的问题 一对一模型创建一个单独的内核线程来处理每个用户线程 但是，管理一对一模型的开销更大，涉及更多开销和减慢系统速度 此模型的大多数实现都限制了可以创建的线程数 从并发性角度来总结下，虽然一对一模型提供了更大的并发性，但是开发人员应注意不要在应用程序内创建太多线程（有时系统可能会限制创建线程的数量），因为管理一对一模型的开销更大。Windows (从 Win95 开始) 和 Linux 都实现了线程的一对一模型。 3）多对多线程模型： ![img](Java 线程和操作系统的线程有啥区别/1620-1663408160035-8.png) 多对多模型将任意数量的用户线程复用到相同或更少数量的内核线程上，结合了一对一和多对一模型的最佳特性 用户对创建的线程数没有限制 阻止内核系统调用不会阻止整个进程 进程可以分布在多个处理器上 可以为各个进程分配可变数量的内核线程，具体取决于存在的 CPU 数量和其他因素 3. Java 线程 在进入 Java 线程主题之前，有必要讲解一下线程库 Thread library 的概念。 在上面的模型介绍中，我们提到了通过线程库来创建、管理线程，那么什么是线程库呢？ 线程库就是为开发人员提供创建和管理线程的一套 API。 当然，线程库不仅可以在用户空间中实现，还可以在内核空间中实现。前者涉及仅在用户空间内实现的 API 函数，没有内核支持。后者涉及系统调用，也就是说调用库中的一个 API 函数将会导致对内核的系统调用，并且需要具有线程库支持的内核。 下面简单介绍下三个主要的线程库： 1）POSIX Pthreads：可以作为用户或内核库提供，作为 POSIX 标准的扩展 2）Win32 线程：用于 Window 操作系统的内核级线程库 3）Java 线程：Java 线程 API 通常采用宿主系统的线程库来实现，也就是说在 Win 系统上，Java 线程 API 通常采用 Win API 来实现，在 UNIX 类系统上，采用 Pthread 来实现。 下面我们来详细讲解 Java 线程： 事实上，在 JDK 1.2 之前，Java 线程是基于称为 “绿色线程”（Green Threads）的用户级线程实现的，也就是说程序员大佬们为 JVM 开发了自己的一套线程库或者说线程管理机制。 而在 JDK 1.2 及以后，JVM 选择了更加稳定且方便使用的操作系统原生的内核级线程，通过系统调用，将线程的调度交给了操作系统内核。而对于不同的操作系统来说，它们本身的设计思路基本上是完全不一样的，因此它们各自对于线程的设计也存在种种差异，所以 JVM 中明确声明了：虚拟机中的线程状态，不反应任何操作系统中的线程状态。 也就是说，在 JDK 1.2 及之后的版本中，Java 的线程很大程度上依赖于操作系统采用什么样的线程模型，这点在不同的平台上没有办法达成一致，JVM 规范中也并未限定 Java 线程需要使用哪种线程模型来实现，可能是一对一，也可能是多对多或多对一。 总结来说，回答下文题，现今 Java 中线程的本质，其实就是操作系统中的线程，其线程库和线程模型很大程度上依赖于操作系统（宿主系统）的具体实现，比如在 Windows 中 Java 就是基于 Wind32 线程库来管理线程，且 Windows 采用的是一对一的线程模型。 问题1.java线程是操作系统的线程吗，调度也是依据操作系统调度吗 现今 Java 中线程的本质，其实就是操作系统中的线程，其线程库和线程模型很大程度上依赖于操作系统（宿主系统）的具体实现 [转载]Java 线程和操作系统的线程有啥区别？ - 腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1818151)","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"浅谈CMS和G1收集器","slug":"浅谈CMS和G1收集器","date":"2022-09-16T12:38:23.000Z","updated":"2022-09-17T09:29:09.612Z","comments":true,"path":"2022/09/16/浅谈CMS和G1收集器/","link":"","permalink":"https://gwtt.github.io/2022/09/16/%E6%B5%85%E8%B0%88CMS%E5%92%8CG1%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"CMS收集器1.什么是CMS收集器 CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网网站或者B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用的需求。 从名字（包含 “Mark Sweep”）上就可以看出，CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几种收集器来说更复杂一些，整个过程分为4个步骤,包括: 初始标记(CMS initial mark) 并发标记(CMS concurrent mark) 重新标记(CMS remark) 并发清除(CMS concurrent sweep) 其中，初始标记、重新标记这两个步骤需要“Stop the World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。 由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。 CMS是一款优秀的收集器，它的主要优点在名字上就已经体现出来了: 并发收集、低停顿，Sum公司文档也会称为Concurrent Low Pause Collector(并发低停顿收集器) 2.优点 并发收集 低停顿 3.CMS收集器缺点 CMS收集器对CPU资源非常敏感。 在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程而导致应用程序变慢，总吞吐量会降低。 CMS收集器无法处理浮动垃圾（Floating Garbage)。 可能出现”Concurrent Mode Failure”失败而导致另一次Full GC的产生。由于CMS并发清理阶段用户线程还在运行者，伴随程序运行自然有不断的垃圾不断的产生，这些垃圾只能留在下一次GC才能清理掉。 CMS收集器是基于标记-清除算法，该算法的缺点都有。 收集结束时会有大量空间碎片。 标记和清除两个过程的效率都不高。 G1收集器1.什么是G1收集器 G1(Garbage-First)收集器是当今收集器技术发展的最前沿成果之一，早在JDK1.7刚刚确立项目目标，Sum公司给出的JDK1.7 RoadMap里面，它就被视为JDK1.7中HotSpot虚拟机的一个重要进化特征。从JDK 6u14中开始就有Early Access版本的G1收集器共开发人员实验和试用，由此开始G1收集器的“Experimental”状态持续了数年时间，直至JDK7u4，Sum公司才认为它达到足够成熟的商用程度，移除了“Experimental“的标识。 G1重新定义了堆空间，打破了原有的分代模型，将堆划分为一个个区域。这么做的目的是在进行收集时不必在全堆范围内进行，这是它最显著的特点。区域划分的好处就是带来了停顿时间可预测的收集模型：用户可以指定收集操作在多长时间内完成。即G1提供了接近实时的收集特性。 2.优点 并发与并行 1G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短Stop-the-world停顿的时间，部分其他收集器原来需要停顿Java线程执行的GC操作，G1收集器仍然可以通过并发的方式让Java程序继续运行。 分代收集 空间整合 1与CMS的标记-清除算法不同，G1从整体来看是基于标记-整理算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法实现的。但无论如何，这两种算法都意味着G1运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。 可预测的停顿 1这是G1相对于CMS的一个优势，降低停顿时间是G1和CMS共同的关注点。 在G1之前的其他收集器进行收集的范围都是整个新生代或者老年代，而G1不再是这样。在堆的结构设计时，G1打破了以往将收集范围固定在新生代或老年代的模式，G1将堆分成许多相同大小的区域单元，每个单元称为Region。Region是一块地址连续的内存空间，G1模块的组成如下图所示： G1收集器将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。Region的大小是一致的，数值是在1M到32M字节之间的一个2的幂值数，JVM会尽量划分2048个左右、同等大小的Region，这一点可以参看如下源码。其实这个数字既可以手动调整，G1也会根据堆大小自动进行调整。 G1收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个Java堆中进行全区域的垃圾收集。G1会通过一个合理的计算模型，计算出每个Region的收集成本并量化，这样一来，收集器在给定了“停顿”时间限制的情况下，总是能选择一组恰当的Regions作为收集目标，让其收集开销满足这个限制条件，以此达到实时收集的目的。 对于打算从CMS或者ParallelOld收集器迁移过来的应用，按照官方的建议，如果发现符合如下特征，可以考虑更换成G1收集器以追求更佳性能： 实时数据占用了超过半数的堆空间； 对象分配率或“晋升”的速度变化明显； 期望消除耗时较长的GC或停顿（超过0.5——1秒）。 12345Applications running today with either the CMS or the ParallelOld garbage collector would benefit switching to G1 if the application has one or more of the following traits.More than 50% of the Java heap is occupied with live data.The rate of object allocation rate or promotion varies significantly.Undesired long garbage collection or compaction pauses (longer than 0.5 to 1 second) 3.运行过程 G1收集的运作过程大致如下： 初始标记（Initial Marking）：仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建新对象，这阶段需要停顿线程，但耗时很短。 并发标记（Concurrent Marking）：是从GC Roots开始堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。 最终标记（Final Marking）：是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行。 筛选回收（Live Data Counting and Evacuation）：首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划。这个阶段也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。 全局变量和栈中引用的对象是可以列入根集合的，这样在寻找垃圾时，就可以从根集合出发扫描堆空间。在G1中，引入了一种新的能加入根集合的类型，就是记忆集（Remembered Set）。Remembered Sets（也叫RSets）用来跟踪对象引用。G1的很多开源都是源自Remembered Set，例如，它通常约占Heap大小的20%或更高。并且，我们进行对象复制的时候，因为需要扫描和更改Card Table的信息，这个速度影响了复制的速度，进而影响暂停时间。 卡表 卡表（Card Table）有个场景，老年代的对象可能引用新生代的对象，那标记存活对象的时候，需要扫描老年代中的所有对象。因为该对象拥有对新生代对象的引用，那么这个引用也会被称为GC Roots。那不是得又做全堆扫描？成本太高了吧。 HotSpot给出的解决方案是一项叫做卡表（Card Table）的技术。该技术将整个堆划分为一个个大小为512字节的卡，并且维护一个卡表，用来存储每张卡的一个标识位。这个标识位代表对应的卡是否可能存有指向新生代对象的引用。如果可能存在，那么我们就认为这张卡是脏的。 在进行Minor GC的时候，我们便可以不用扫描整个老年代，而是在卡表中寻找脏卡，并将脏卡中的对象加入到Minor GC的GC Roots里。当完成所有脏卡的扫描之后，Java虚拟机便会将所有脏卡的标识位清零。 想要保证每个可能有指向新生代对象引用的卡都被标记为脏卡，那么Java虚拟机需要截获每个引用型实例变量的写操作，并作出对应的写标识位操作。 卡表能用于减少老年代的全堆空间扫描，这能很大的提升GC效率。 记忆集 记忆集是一种用于记录从非收集区域指向收集区域的指针集合的数据结构。 如果我们不考虑效率和成本问题，我们可以用一个数组存储所有有指针指向新生代的老年代对象。但是如果这样的话我们维护成本就很好，打个比方，假如所有的老年代对象都有指针指向了新生代，那么我们需要维护整个老年代大小的记忆集，毫无疑问这种方法是不可取的。因此我们引入了卡表的数据结构 写屏障 我们每次对引用进行改变时，我们在程序中并没有手动去维护卡表的信息，那么卡表信息的维护到底是如何进行的呢，这就依赖于我们的写屏障功能。 写屏障可以理解为对于我们引用类型字段复制的AOP操作。在赋前的部分的写屏障叫作写前屏障（Pre-Write Barrier），在赋值后的部分的写屏障叫作写后屏障（PostWrite Barrier）。 伪共享问题 由于CPU集成的多级缓存中是以缓存行来读取数据的，通过MESI协议保证多个CPU之间的缓存一致性。伪共享问题是卡表元素更改时处于同一缓存行导致的，诱发的因素是不同卡页内的对象发生了跨代引用，从而使CPU并行执行变为串行执行，降低了并发性能。 举例： 若a、b位于同一缓存行，当CPU1修改a后，若CPU2想修改b，必须先提交CPU1的缓存，然后CPU2再去主存中读取数据。 伪共享问题解决方案：JAVA中的解决方案有填充法 和 Contended 注解。 填充法：就是 扩大对象的大小，这样，就可以一个缓冲行中，只存在一个对象！这样，就不会导致结果是串行执行了！(JDK8之前)Contended 注解法：Java1.8 中提供了Contended注解，使用这个注解，VM必须设置 -XX:-RestrictContended。ConcurrentHashMap的内部类CounterCell有用到这个注解","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"9/16华为技术面","slug":"9月16日华为技术面","date":"2022-09-16T02:45:22.000Z","updated":"2022-09-19T15:01:10.424Z","comments":true,"path":"2022/09/16/9月16日华为技术面/","link":"","permalink":"https://gwtt.github.io/2022/09/16/9%E6%9C%8816%E6%97%A5%E5%8D%8E%E4%B8%BA%E6%8A%80%E6%9C%AF%E9%9D%A2/","excerpt":"","text":"1. Java的跨平台的机制是什么？ java语言编写的程序,一次编译后,可以在多个系统平台上运行 Java程序是通过java虚拟机在系统平台上运行的，只要该系统可以安装相应的java虚拟机，该系统就可以运行java程序 2. JVM 内存是怎么管理的？ 堆，方法区，程序计数器，虚拟机栈，本地方法栈 3.GC 一般在什么时候触发？ Young GC 一般是在新生代的 Eden 区满了之后触发的 Full GC 年老代（Tenured）被写满； 持久代（Perm）被写满； System.gc()被显示调用； 上一次GC之后Heap的各域分配策略动态变化； 4. 调用GC（System.gc()）之后会立即触发吗？如果立马触发的话，会导致频繁回收，又会带来什么问题？ 如果不是，是什么机制来保障的（finalize） 不会立马触发。甚至可能不会垃圾回收。 造成系统卡顿 用justRanFinalization来保障System.gc()执不执行 5. Java 有了 GC 之后还会有内存泄漏问题吗？ 静态集合类泄漏静态集合类像HashMap，Vector等的使用最容易出现内存泄漏，静态变量的声明周期与应用程序一直，所有的对象Object也不能内释放，因为被其他对象引用着。 单例造成的泄漏 单例对象在初始化后将在JVM的整个生命周期中存在（以静态变量的方式），如果单例对象持有外部的引用，那么这个对象将不能被JVM正常回收，导致内存泄漏。 各种连接数据库连接，网络连接，IO连接等没有显式调用close()关闭，会导致内存泄漏。 监听器的使用在释放对象的同时，没有删除相应监听器，也会造成内存泄漏。 6. 哪些可以作为 GC Roots ？ 虚拟机栈(栈帧中的本地变量表)中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中JNI引用的对象 7. String 类型不可修改，为什么呢？字符串常量，或者new一个String对象，创建对象的时候有什么差异吗？非要改String类型的值，能改的到吗？new一个String对象，是放在哪里的？StringBuilder和StringBuffer有什么区别？ String不可变是因为字符数组被final和private修饰。并且String没有提供可以修改字符数组的API new一个对象时指向堆里面的，字符串常量时指向方法区里的字符串常量池 反射可以修改（不是声明底层字段会失败） 放在堆里（Heap） 一个线程不安全一个线程安全 8. 反射机制有了解吗？反射创建对象和new创建对象有什么区别？反射和new的性能哪个更高一点？ 有 new属于静态编译反射属于动态编译，意思就说只有到运行时才会去获得该对象的实例,Spring就是使用的反射 new性能更高 9. 多线程了解吗？通过什么类来实现？ 了解 主要通过Thread类 10. 设计模式有了解吗？用了这些模式，带来的好处是什么？为什么要用这些设计模式（方便管理类、分工开发） ​ 了解 方便重用，方便移植 方便阅读，方便他人了解 代码可靠性更高，降低错误发生 节省写项目的时间 11.Redis 和大型数据库有什么区别？为什么要用这个数据库（Redis） Redis是非关系型数据库，Mysql是关系型数据库 Mysql持久化到硬盘中，读取较慢，Redis数据存储到内存中，读取速度快 为什么要用: 反复连接数据库需要花费很多时间，从而导致运行效率过慢，反复连接也会导致数据库负载变高。由于Redis是基于内存操作，所以CPU不是性能瓶颈，机器的内存和宽带才是Redis的瓶颈，所以用Redis可以减少访问数据库的次数，提高运行效率。 12. 编码的时候，会做网络字节序转换，大小端转换，为什么要做这个转换呢？ （不会） 因为不同的计算机都有自己的主机字节序，为了保证数据传输的统一性，就是让数据在所有计算机上都以一种通用形式呈现，所以会作网络字节序转换。 TCP/IP协议规定使用“大端”字节序作为网络字节序。 13. ICMP 协议，有了解吗？ ICMP（Internet Control Message Protocol）Internet控制报文协议。它是TCP/IP协议簇的一个子协议，用于在IP主机、路由器之间传递控制消息。控制消息是指网络通不通、主机是否可达、路由是否可用等网络本身的消息。这些控制消息虽然并不传输用户数据，但是对于用户数据的传递起着重要的作用。 它的功能是报告无法传送的数据包的错误，并帮助对这些错误进行疑难解答。 典型应用:PING,traceroute（检查路由（路径）并测量跨Internet协议（IP）网络的传输延迟数据包的工具） 14. HTTP 和 HTTPS 的区别是什么？ 1、https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。 2、http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。 3、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 4、http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。 15. 数字证书使用了什么机制？数字证书为什么被创建？ 数字证书采用公钥体制，即利用一对互相匹配的密钥进行加密、解密。用于加密和验证签名。当发送一份保密文件时，发送方使用接收方的公钥对数据加密，而接收方则使用自己的私钥解密，这样信息就可以安全无误地到达目的地了。通过数字的手段保证加密过程是一个不可逆过程，即只有用私有密钥才能解密。 为了防止信息泄露，所以必须保证网络安全的四大要素: 信息传输的保密性 数据交换的完整性 发送信息的不可否认性 交易者身份的确定性 所以创建数字证书 摘自华为面经-Java-西安_笔经面经_牛客网(https://www.nowcoder.com/discuss/1053917)","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"浅谈Lambda表达式","slug":"浅谈Lambda表达式","date":"2022-09-14T11:14:23.000Z","updated":"2022-09-14T11:24:58.078Z","comments":true,"path":"2022/09/14/浅谈Lambda表达式/","link":"","permalink":"https://gwtt.github.io/2022/09/14/%E6%B5%85%E8%B0%88Lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/","excerpt":"","text":"什么是Lambda表达式？ 可以将Lambda表达式理解为一个匿名函数； Lambda表达式允许将一个函数作为另外一个函数的参数； 我们可以把 Lambda 表达式理解为是一段可以传递的代码（将代码作为实参）,也可以理解为函数式编程，将一个函数作为参数进行传递。 2. 为什么要引入Lambda表达式？ 当java程序员看到其他语言的程序员（如JS，Python）在使用闭包或者Lambda表达式的时候，于是开始吐槽世界上使用最广的语言居然不支持函数式编程。千呼万唤，Java8推出了Lambda表达式。 1234567891011121314package com.isea.java;public class TestLambda &#123; public static void main(String[] args) &#123; Thread thread = new Thread(new MyRunnable()); thread.start(); thread.close(); &#125;&#125;class MyRunnable implements Runnable&#123; @Override public void run() &#123; System.out.println(&quot;Hello&quot;); &#125;&#125; 为了使这段代码变得更加简洁，可以使用匿名内部类重构一下（注意代码中的注释） 12345678910111213package com.isea.java;public class TestLambda &#123; public static void main(String[] args) &#123; new Thread(new Runnable() &#123; //这里的new Runnable()，这里new 了接口，在这个new的接口里面，我们写了这个接口的实现类。 //这里可以看出，我们把一个重写的run()方法传入了一个构造函数中。 @Override public void run() &#123; System.out.println(&quot;Hello&quot;); &#125; &#125;).start(); &#125;&#125; 3. Lambda表达式的分类1. 无参无返回值 12345public class TestLambda &#123; public static void main(String[] args) &#123; new Thread(() -&gt; System.out.println(&quot;Hello&quot;)); &#125;&#125; 2. 有参无返回值 123456789101112131415import java.util.ArrayList;public class TestLambda &#123; public static void main(String[] args) &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;AAAAA&quot;); list.add(&quot;BBBBB&quot;); list.add(&quot;CCCCC&quot;); list.add(&quot;DDDDD&quot;);//形参的类型是确定的，可省略；只有一个形参，()可以省略； list.forEach(t -&gt; System.out.print(t + &quot;\\t&quot;));//或者更简洁的方法引用：list.forEach(System.out::println); //打印结果：AAAAA BBBBB CCCCC DDDDD &#125;&#125;public void forEach(Consumer&lt;? super E&gt; action) forEach() 功能等同与增强型for循环 这个方法来自于Iterable接口，Collection接口继承了这个接口，List又继承了Collection接口，而ArrayList是List的实现类；forEach函数，指明该函数需要传入一个函数，而且是有参数没有返回值的函数，而Consumer接口中正好有且仅有一个这样的有参无返回值的抽象方法。接下来，我们会了解到这是使用Lambda的必要条件。 3. 无参有返回值 12345678910import java.util.Random;import java.util.stream.Stream;public class TestLambda &#123; public static void main(String[] args) &#123; Random random = new Random(); Stream&lt;Integer&gt; stream = Stream.generate(() -&gt;random.nextInt(100)); stream.forEach(t -&gt; System.out.println(t)); &#125;//只有一个return，可以省略return；该方法将会不断的打印100以内的正整数。&#125;//Stream.generate()方法创建无限流，该方法要求传入一个无参有返回值的方法。public static&lt;T&gt; Stream&lt;T&gt; generate(Supplier&lt;T&gt; s) //来自源码 4. 有参有返回值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.text.Collator;import java.util.TreeSet; public class TestLambda &#123; public static void main(String[] args) &#123; Collator collator = Collator.getInstance(); TreeSet&lt;Student&gt; set = new TreeSet&lt;&gt;((s1,s2) -&gt; collator.compare(s1.getName(),s2.getName())); set.add(new Student(10,&quot;张飞&quot;)); set.add(new Student(3,&quot;周瑜&quot;)); set.add(new Student(1,&quot;宋江&quot;)); set.forEach(student -&gt; System.out.println(student)); &#125;&#125;//这里的Collator是一个抽象类，但是提供了获取该类实例的方法getInstance() class Student&#123; private int id; private String name; public Student(int id, String name) &#123; this.id = id; this.name = name; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; Home | This.ID = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;Student&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 4. 函数式接口 即SAM（Single Abstract Method ）接口，有且只有一个抽象方法的接口（可以有默认方法或者是静态方法和从Object继承来的方法，但是抽象方法有且只能有一个）。 JDK1.8之后，添加@FunctionalInterface表示这个接口是是一个函数式接口，因为有了@functionalInterface标记，也称这样的接口为Mark（标记）类型的接口。举例子： 123456789101112@FunctionalInterfacejava.lang.Runnable&#123; void run();&#125;@FunctionalInterfacejava.lang.Comparator&lt;T&gt;&#123; int compare(T o1, T o2);&#125;@FunctionalInterfacepublic interface Function&lt;T, R&gt; &#123; R apply(T t);&#125; 针对上面的例子，比方说这个Runnable接口是支持Lambda表达式，那么如果有一个方法（比如Thread类的构造函数）需要传入一个Runnable接口的实现类的话，那么就可以直接把Lambda表达式写进去。 换个角度说TreeSet，它有一个构造函数中是要求传入一个接口类型，如果这个接口类型恰好是函数式接口，那么直接传进去一个Lambda表达式即可。 函数式接口作用 函数式接口能够接受匿名内部类的实例化对象，换句话说，我们可以使用匿名内部类来实例化函数式接口的对象，而Lambda表达式能够代替内部类实现代码的进一步简化，因此，Lambda表达式和函数式接口紧密的联系到了一起，接下来的这句话非常的重要： 每一个Lambda表达式能隐式的给函数式接口赋值 1new Thread(() -&gt; System.out.println(&quot;hello&quot;)).start(); 编译器会认为Thread()中传入的是一个Runnable的对象，而我们利用IDEA的智能感知，鼠标指向“-&gt;”或“（）”的时候，会发现这是一个Runnable类型，实际上编译器会自动将Lambda表达式赋值给函数式接口，在本例中就是Runnable接口。本例中Lambda表达式将打印方法传递给了Runnable接口中的run（）方法，从而形成真正的方法体。 而且，参数与返回值是一一对应的，即如果函数式接口中的抽象方法是有返回值，有参数的，那么要求Lambda表达式也是有返回值，有参数的（余下类推） 四大函数式接口： 有时候后，如果我们调用某一个方法，发现这个方法中需要传入的参数要求是一个函数式的接口，那么我们可以直接传入Lambda表达式。这些接口位于java.util.function包下，需要注意一下，java.util包和java.util.function包这两个包没有什么关系，切不可以为function包是java.util包下面的包。 消费型接口：Consumer&lt; T&gt; void accept(T t)有参数，无返回值的抽象方法； 供给型接口：Supplier &lt; T&gt; T get() 无参有返回值的抽象方法； 断定型接口： Predicate&lt; T&gt; boolean test(T t):有参，但是返回值类型是固定的boolean 函数型接口： Function&lt; T，R&gt; R apply(T t)有参有返回值的抽象方法； 5. 新日期时间API(补充) 1. LocalDate(只有年月日)1234567public class LocalDate_Test &#123; public static void main(String[] args) &#123; LocalDate date = LocalDate.now(); System.out.println(date.getYear()+&quot; &quot;+date.getMonthValue()+&quot; &quot;+date.getDayOfMonth()); System.out.println(date.toString()); &#125;&#125; 2 . LocalTime(只有时分秒)12345678public class LocalTime_Test &#123; public static void main(String[] args) &#123; LocalTime time = LocalTime.now(); System.out.println(time.getHour()+&quot; &quot;+time.getMinute()+&quot; &quot;+time.getSecond()); System.out.println(time.toString()); System.out.println(time.toSecondOfDay()); &#125;&#125; 3. LocalDateTime(年月日和时分秒)12345678public class LocalDateTime_Test &#123; public static void main(String[] args) &#123; LocalDateTime dateTime = LocalDateTime.now(); System.out.println(dateTime.getYear()+&quot; &quot;+dateTime.getMonthValue()+&quot; &quot;+dateTime.getDayOfMonth()+ dateTime.getHour()+&quot; &quot;+dateTime.getMinute()+&quot; &quot;+dateTime.getSecond()); System.out.println(dateTime.toString()); &#125;&#125; 4. DateTimeFormatter1234567public class DateTimeFormatter_Test &#123; public static void main(String[] args) &#123; DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd:HH:mm:ss&quot;); LocalDateTime dateTime = LocalDateTime.parse(&quot;2017-12-15:19:15:01&quot;,formatter); System.out.println(dateTime.toString()); &#125;&#125; 5. ZonedDateTime(有时区)12345678public class ZonedDateTime_Test &#123; public static void main(String[] args) &#123; ZonedDateTime zonedDateTime = ZonedDateTime.now(); DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;MM/dd/yyyy:HH:mm:ss&quot;); String date = zonedDateTime.format(formatter); System.out.println(date); &#125;&#125;","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"浅谈Stream流","slug":"浅谈Stream流","date":"2022-09-14T08:52:23.000Z","updated":"2022-09-14T09:59:40.203Z","comments":true,"path":"2022/09/14/浅谈Stream流/","link":"","permalink":"https://gwtt.github.io/2022/09/14/%E6%B5%85%E8%B0%88Stream%E6%B5%81/","excerpt":"","text":"什么是Stream流 Stream被翻译为流，它的工作过程像将一瓶水导入有很多过滤阀的管道一样，水每经过一个过滤阀，便被操作一次，比如过滤，转换等，最后管道的另外一头有一个容器负责接收剩下的水。 Stream作为Java 8的一大亮点，它专门针对集合的各种操作提供各种非常便利，简单，高效的API,Stream API主要是通过Lambda表达式完成，极大的提高了程序的效率和可读性，同时Stram API中自带的并行流使得并发处理集合的门槛再次降低，使用Stream API编程无需多写一行多线程的大门就可以非常方便的写出高性能的并发程序。使用Stream API能够使你的代码更加优雅。 流的另一特点是可无限性，使用Stream，你的数据源可以是无限大的。 如何使用流 获取流 对流操作 结束对流操作 - 获取流 获取流的方式有多种，对于常见的容器(Collection)可以直接.stream()获取 例如： Collection.stream() Collection.parallelStream() Arrays.stream(T array) or Stream.of() 对于IO，我们也可以通过lines()方法获取流： java.nio.file.Files.walk() java.io.BufferedReader.lines() 最后，我们还可以从无限大的数据源中产生流： Random.ints() 值得注意的是，JDK中针对基本数据类型的昂贵的装箱和拆箱操作，提供了基本数据类型的流： IntStream LongStream DoubleStream 这三种基本数据类型和普通流差不多，不过他们流里面的数据都是指定的基本数据类型。 12Intstream.of(new int[]&#123;1,2,3&#125;);Intstream.range(1,3); 1234567# 这边有个parallelStream和stream的区别# 因为parallelstream是并行流,所以执行效率比较高# 并行流并不会按照原本的顺序轨迹执行, 而是 随机执行可以从以下三点入手考虑是否使用parallelstream是否需要并行？ 任务之间是否是独立的？是否会引起任何竞态条件？ 结果是否取决于任务的调用顺序？ 获取流的方式有多种，对于常见的容器(Collection)可以直接.stream()获取 例如： Collection.stream() Collection.parallelStream() Arrays.stream(T array) or Stream.of() 对于IO，我们也可以通过lines()方法获取流： java.nio.file.Files.walk() java.io.BufferedReader.lines() 最后，我们还可以从无限大的数据源中产生流： Random.ints() 值得注意的是，JDK中针对基本数据类型的昂贵的装箱和拆箱操作，提供了基本数据类型的流： IntStream LongStream DoubleStream 这三种基本数据类型和普通流差不多，不过他们流里面的数据都是指定的基本数据类型。 12Intstream.of(new int[]&#123;1,2,3&#125;);Intstream.rang(1,3); 1234567# 这边有个parallelStream和stream的区别# 因为parallelstream是并行流,所以执行效率比较高# 并行流并不会按照原本的顺序轨迹执行, 而是 随机执行可以从以下三点入手考虑是否使用parallelstream是否需要并行？ 任务之间是否是独立的？是否会引起任何竞态条件？ 结果是否取决于任务的调用顺序？ - 对流操作 对于中间操作，所有的API的返回值基本都是Stream&lt;T&gt;,因此以后看见一个陌生的API也能通过返回值判断它的所属类型。 map/flatMapmap顾名思义，就是映射，map操作能够将流中的每一个元素映射为另外的元素。 1&lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super T, ? extends R&gt; mapper); 可以看到map接受的是一个Function,也就是接收参数，并返回一个值。 比如： 123//提取 List&lt;Student&gt; 所有student 的名字 List&lt;String&gt; studentNames = students.stream().map(Student::getName) .collect(Collectors.toList()); 上面的代码等同于以前的： 1234List&lt;String&gt; studentNames=new ArrayList&lt;&gt;();for(Student student:students)&#123; studentNames.add(student.getName());&#125; 再比如：将List中所有字母转换为大写： 123List&lt;String&gt; words=Arrays.asList(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;);List&lt;String&gt; upperWords=words.stream().map(String::toUpperCase) .collect(Collectors.toList()); flatMap顾名思义就是扁平化映射，它具体的操作是将多个stream连接成一个stream，这个操作是针对类似多维数组的，比如容器里面包含容器等。 1234List&lt;List&lt;Integer&gt;&gt; ints=new ArrayList&lt;&gt;(Arrays.asList(Arrays.asList(1,2), Arrays.asList(3,4,5)));List&lt;Integer&gt; flatInts=ints.stream().flatMap(Collection::stream). collect(Collectors.toList()); 可以看到，相当于降维。 filter12filter`顾名思义，就是过滤，通过测试的元素会被留下来并生成一个新的`StreamStream&lt;T&gt; filter(Predicate&lt;? super T&gt; predicate); 同理，我们可以filter接收的参数是Predicate，也就是推断型函数式接口，接收参数，并返回boolean值。 比如： 123//获取所有大于18岁的学生List&lt;Student&gt; studentNames = students.stream().filter(s-&gt;s.getAge()&gt;18) .collect(Collectors.toList()); distinctdistinct是去重操作,它没有参数 1Stream&lt;T&gt; distinct(); sorted1234sorted`排序操作，默认是从小到大排列，sorted方法包含一个重载，使用sorted方法，如果没有传递参数，那么流中的元素就需要实现Comparable&lt;T&gt;方法，也可以在使sorted方法的时候传入一个`Comparator&lt;T&gt;Stream&lt;T&gt; sorted(Comparator&lt;? super T&gt; comparator);Stream&lt;T&gt; sorted(); 值得一说的是这个Comparator在Java 8之后被打上了@FunctionalInterface,其他方法都提供了default实现，因此我们可以在sort中使用Lambda表达式 例如： 123//以年龄排序students.stream().sorted((s,o)-&gt;Integer.compare(s.getAge(),o.getAge())) .forEach(System.out::println);; 然而还有更方便的，Comparator默认也提供了实现好的方法引用，使得我们更加方便的使用： 例如上面的代码可以改成如下： 123//以年龄排序 students.stream().sorted(Comparator.comparingInt(Student::getAge)) .forEach(System.out::println);; 或者： 123//以姓名排序students.stream().sorted(Comparator.comparing(Student::getName)). forEach(System.out::println); 是不是更加简洁。 peekpeek有遍历的意思，和forEach一样，但是它是一个中间操作。 peek接受一个消费型的函数式接口。 1Stream&lt;T&gt; peek(Consumer&lt;? super T&gt; action); 例如： 123//去重以后打印出来，然后再归并为ListList&lt;Student&gt; sortedStudents= students.stream().distinct().peek(System.out::println). collect(Collectors.toList()); limitlimit裁剪操作，和String::subString(0,x)有点先沟通，limit接受一个long类型参数，通过limit之后的元素只会剩下min(n,size)个元素，n表示参数，size表示流中元素个数 1Stream&lt;T&gt; limit(long maxSize); 例如： 12//只留下前6个元素并打印students.stream().limit(6).forEach(System.out::println); skipskip表示跳过多少个元素，和limit比较像，不过limit是保留前面的元素，skip是保留后面的元素 1Stream&lt;T&gt; skip(long n); 例如： 12//跳过前3个元素并打印 students.stream().skip(3).forEach(System.out::println); - 终结操作 一个流处理中，有且只能有一个终结操作，通过终结操作之后，流才真正被处理，终结操作一般都返回其他的类型而不再是一个流,一般来说，终结操作都是将其转换为一个容器。 forEachforEach是终结操作的遍历，操作和peek一样，但是forEach之后就不会再返回流 1void forEach(Consumer&lt;? super T&gt; action); 例如： 12//遍历打印students.stream().forEach(System.out::println); 上面的代码和一下代码效果相同： 123for(Student student:students)&#123; System.out.println(sudents);&#125; toArraytoArray和List##toArray()用法差不多，包含一个重载。 默认的toArray()返回一个Object[]， 也可以传入一个IntFunction&lt;A[]&gt; generator指定数据类型 一般建议第二种方式。 123Object[] toArray();&lt;A&gt; A[] toArray(IntFunction&lt;A[]&gt; generator); 例如： 1Student[] studentArray = students.stream().skip(3).toArray(Student[]::new); max/minmax/min即使找出最大或者最小的元素。max/min必须传入一个Comparator。 123Optional&lt;T&gt; min(Comparator&lt;? super T&gt; comparator);Optional&lt;T&gt; max(Comparator&lt;? super T&gt; comparator); countcount返回流中的元素数量 1long count(); 例如： 1long count = students.stream().skip(3).count(); reduce1reduce为归纳操作，主要是将流中各个元素结合起来，它需要提供一个起始值，然后按一定规则进行运算，比如相加等，它接收一个二元操作 `BinaryOperator`函数式接口。从某种意义上来说，`sum,min,max,average`都是特殊的reduce reduce包含三个重载： 1234567T reduce(T identity, BinaryOperator&lt;T&gt; accumulator);Optional&lt;T&gt; reduce(BinaryOperator&lt;T&gt; accumulator); &lt;U&gt; U reduce(U identity, BiFunction&lt;U, ? super T, U&gt; accumulator, BinaryOperator&lt;U&gt; combiner); 例如： 123List&lt;Integer&gt; integers = new ArrayList&lt;&gt;(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)); long count = integers.stream().reduce(0,(x,y)-&gt;x+y); 以上代码等同于： 1long count = integers.stream().reduce(Integer::sum).get(); reduce两个参数和一个参数的区别在于有没有提供一个起始值， 如果提供了起始值，则可以返回一个确定的值，如果没有提供起始值，则返回Opeational防止流中没有足够的元素。 anyMatch\\ allMatch\\ noneMatch测试是否有任意元素\\所有元素\\没有元素匹配表达式 他们都接收一个推断类型的函数式接口：Predicate 12345boolean anyMatch(Predicate&lt;? super T&gt; predicate);boolean allMatch(Predicate&lt;? super T&gt; predicate);boolean noneMatch(Predicate&lt;? super T&gt; predicate) 例如： 1boolean test = integers.stream().anyMatch(x-&gt;x&gt;3); findFirst、 findAny获取元素，这两个API都不接受任何参数，findFirt返回流中第一个元素，findAny返回流中任意一个元素。 123Optional&lt;T&gt; findFirst();Optional&lt;T&gt; findAny(); 也有有人会问findAny()这么奇怪的操作谁会用？这个API主要是为了在并行条件下想要获取任意元素，以最大性能获取任意元素 例如： 1int foo = integers.stream().findAny().get(); collectcollect收集操作，这个API放在后面将是因为它太重要了，基本上所有的流操作最后都会使用它。 我们先看collect的定义： 1234 &lt;R&gt; R collect(Supplier&lt;R&gt; supplier, BiConsumer&lt;R, ? super T&gt; accumulator, BiConsumer&lt;R, R&gt; combiner);&lt;R, A&gt; R collect(Collector&lt;? super T, A, R&gt; collector); 可以看到，collect包含两个重载： 一个参数和三个参数， 三个参数我们很少使用，因为JDK提供了足够我们使用的Collector供我们直接使用,我们可以简单了解下这三个参数什么意思： Supplier:用于产生最后存放元素的容器的生产者 accumulator:将元素添加到容器中的方法 combiner：将分段元素全部添加到容器中的方法 前两个元素我们都很好理解，第三个元素是干嘛的呢？因为流提供了并行操作，因此有可能一个流被多个线程分别添加，然后再将各个子列表依次添加到最终的容器中。 ↓ - - - - - - - - - ↓ — — — ↓ ——— 如上图，分而治之。 例如： 1List&lt;String&gt; result = stream.collect(ArrayList::new, List::add, List::addAll); 接下来看只有一个参数的collect 一般来说，只有一个参数的collect，我们都直接传入Collectors中的方法引用即可： 1List&lt;Integer&gt; = integers.stream().collect(Collectors.toList()); Collectors中包含很多常用的转换器。toList(),toSet()等。 1Collectors`中还包括一个`groupBy()`，他和`Sql`中的`groupBy`一样都是分组，返回一个`Map 例如： 123//按学生年龄分组Map&lt;Integer,List&lt;Student&gt;&gt; map= students.stream(). collect(Collectors.groupingBy(Student::getAge)); groupingBy可以接受3个参数，分别是 第一个参数：分组按照什么分类 第二个参数：分组最后用什么容器保存返回（当只有两个参数是，此参数默认为HashMap） 第三个参数：按照第一个参数分类后，对应的分类的结果如何收集 有时候单参数的groupingBy不满足我们需求的时候，我们可以使用多个参数的groupingBy 例如： 123//将学生以年龄分组，每组中只存学生的名字而不是对象Map&lt;Integer,List&lt;String&gt;&gt; map = students.stream(). collect(Collectors.groupingBy(Student::getAge,Collectors.mapping(Student::getName,Collectors.toList()))); toList默认生成的是ArrayList,toSet默认生成的是HashSet，如果想要指定其他容器，可以如下操作： 1234 students.stream().collect(Collectors.toCollection(TreeSet::new));Collectors`还包含一个`toMap`，利用这个`API`我们可以将`List`转换为`Map Map&lt;Integer,Student&gt; map=students.stream(). collect(Collectors.toMap(Student::getAge,s-&gt;s)); 值得注意的一点是，IntStream，LongStream,DoubleStream是没有collect()方法的，因为对于基本数据类型，要进行装箱，拆箱操作，SDK并没有将它放入流中，对于基本数据类型流，我们只能将其toArray()","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"浅谈数据库大数据量问题","slug":"数据库大数据量插入","date":"2022-09-14T03:27:23.000Z","updated":"2022-09-14T07:53:01.543Z","comments":true,"path":"2022/09/14/数据库大数据量插入/","link":"","permalink":"https://gwtt.github.io/2022/09/14/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%8F%E6%8F%92%E5%85%A5/","excerpt":"","text":"大批量插入数据优化 1.一条SQL语句插入多条数据常用的插入语句如： 1234INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES (&#x27;0&#x27;, &#x27;userid_0&#x27;, &#x27;content_0&#x27;, 0);INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES (&#x27;1&#x27;, &#x27;userid_1&#x27;, &#x27;content_1&#x27;, 1); 修改成： 12INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES (&#x27;0&#x27;, &#x27;userid_0&#x27;, &#x27;content_0&#x27;, 0), (&#x27;1&#x27;, &#x27;userid_1&#x27;, &#x27;content_1&#x27;, 1); 修改后的插入操作能够提高程序的插入效率。这里第二种SQL执行效率高的主要原因是合并后日志量（MySQL的binlog和innodb的事务让日志）减少了， 降低日志刷盘的数据量和频率，从而提高效率。通过合并SQL语句，同时也能减少SQL语句解析的次数，减少网络传输的IO 。 可以显著提高效率 2.在事务中进行插入处理。把插入修改成： 1234567START TRANSACTION;INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES (&#x27;0&#x27;, &#x27;userid_0&#x27;, &#x27;content_0&#x27;, 0);INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES (&#x27;1&#x27;, &#x27;userid_1&#x27;, &#x27;content_1&#x27;, 1);...COMMIT; 使用事务可以提高数据的插入效率，这是因为进行一个INSERT操作时，MySQL内部会建立一个事务，在事务内才进行真正插入处理操作。通过使用事务可以减少创建事务的消耗， 所有插入都在执行后才进行提交操作 。 3.数据有序插入。数据有序的插入是指插入记录在主键上是有序排列，例如datetime是记录的主键： 123456INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES (&#x27;1&#x27;, &#x27;userid_1&#x27;, &#x27;content_1&#x27;, 1);INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES (&#x27;0&#x27;, &#x27;userid_0&#x27;, &#x27;content_0&#x27;, 0);INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES (&#x27;2&#x27;, &#x27;userid_2&#x27;, &#x27;content_2&#x27;,2); 修改成： 123456INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES (&#x27;0&#x27;, &#x27;userid_0&#x27;, &#x27;content_0&#x27;, 0);INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES (&#x27;1&#x27;, &#x27;userid_1&#x27;, &#x27;content_1&#x27;, 1);INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) VALUES (&#x27;2&#x27;, &#x27;userid_2&#x27;, &#x27;content_2&#x27;,2); 由于数据库插入时，需要维护索引数据，无序的记录会增大维护索引的成本。 我们可以参照InnoDB使用的B+tree索引，如果每次插入记录都在索引的最后面，索引的定位效率很高，并且对索引调整较小；如果插入的记录在索引中间，需要B+tree进行分裂合并等处理，会消耗比较多计算资源，并且插入记录的索引定位效率会下降，数据量较大时会有频繁的磁盘操作。","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Mysql","slug":"Mysql","permalink":"https://gwtt.github.io/tags/Mysql/"}]},{"title":"Mysql索引相关","slug":"Mysql索引相关","date":"2022-09-14T02:32:47.141Z","updated":"2022-09-14T03:12:10.361Z","comments":true,"path":"2022/09/14/Mysql索引相关/","link":"","permalink":"https://gwtt.github.io/2022/09/14/Mysql%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/","excerpt":"","text":"为什么用 B+ 树做索引而不用哈希表做索引? 1、哈希表是把索引字段映射成对应的哈希码然后再存放在对应的位置，这样的话，如果我们要进行模糊查找的话，显然哈希表这种结构是不支持的，只能遍历这个表。而B+树则可以通过最左前缀原则快速找到对应的数据。 2、如果我们要进行范围查找，例如查找ID为100 ~ 400的人，哈希表同样不支持，只能遍历全表。 3、索引字段通过哈希映射成哈希码，如果很多字段都刚好映射到相同值的哈希码的话，那么形成的索引结构将会是一条很长的链表，这样的话，查找的时间就会大大增加。 主键索引和非主键索引有什么区别？ 例如对于下面这个表(其实就是上面的表中增加了一个k字段),且ID是主键。 主键索引和非主键索引的示意图如下： 其中R代表一整行的值。 从图中不难看出，主键索引和非主键索引的区别是：非主键索引的叶子节点存放的是主键的值，而主键索引的叶子节点存放的是整行数据，其中非主键索引也被称为二级索引，而主键索引也被称为聚簇索引。 根据这两种结构我们来进行下查询，看看他们在查询上有什么区别。 1、如果查询语句是 select * from table where ID = 100,即主键查询的方式，则只需要搜索 ID 这棵 B+树。 2、如果查询语句是 select * from table where k = 1，即非主键的查询方式，则先搜索k索引树，得到ID=100,再到ID索引树搜索一次，这个过程也被称为回表。 为什么建议使用主键自增的索引？ 对于这颗主键索引的树 如果我们插入 ID = 650 的一行数据，那么直接在最右边插入就可以了 但是如果插入的是 ID = 350 的一行数据，由于 B+ 树是有序的，那么需要将下面的叶子节点进行移动，腾出位置来插入 ID = 350 的数据，这样就会比较消耗时间，如果刚好 R4 所在的数据页已经满了，需要进行页分裂操作，这样会更加糟糕。 但是，如果我们的主键是自增的，每次插入的 ID 都会比前面的大，那么我们每次只需要在后面插入就行， 不需要移动位置、分裂等操作，这样可以提高性能。也就是为什么建议使用主键自增的索引。","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"MySQL","slug":"MySQL","permalink":"https://gwtt.github.io/tags/MySQL/"}]},{"title":"浅谈MySQL慢查询","slug":"浅谈Mysql慢查询","date":"2022-09-11T11:44:18.000Z","updated":"2022-09-11T15:42:07.852Z","comments":true,"path":"2022/09/11/浅谈Mysql慢查询/","link":"","permalink":"https://gwtt.github.io/2022/09/11/%E6%B5%85%E8%B0%88Mysql%E6%85%A2%E6%9F%A5%E8%AF%A2/","excerpt":"","text":"慢查询是什么 MySQL的慢查询，全名是慢查询日志，是MySQL提供的一种日志记录，用来记录在MySQL中响应时间超过阀值的语句。 具体环境中，运行时间超过long_query_time值的SQL语句，则会被记录到慢查询日志中。 long_query_time的默认值为10，意思是记录运行10秒以上的语句。 默认情况下，MySQL数据库并不启动慢查询日志，需要手动来设置这个参数。 当然，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。 慢查询日志支持将日志记录写入文件和数据库表。 慢查询配置 mysql并不启动慢查询日志，需要我们手动开启 1、输入命令开启慢查询（临时），在MySQL服务重启后会自动关闭； 2、配置my.cnf（windows是my.ini）系统文件开启，修改配置文件是持久化开启慢查询的方式 命令开启查询慢查询是否开启 1show variables like &#x27;%slow_query_log%&#x27;; 开启慢查询命令 1set global slow_query_log=&#x27;ON&#x27;; 指定记录慢查询日志SQL执行时间得阈值（long_query_time 单位：秒，默认10秒） 1set global long_query_time=1; 查询 “慢查询日志文件存放位置” 1show variables like &#x27;%slow_query_log_file%&#x27;; 配置文件开启123456# 开启慢查询功能slow_query_log=ON# 指定记录慢查询日志SQL执行时间得阈值long_query_time=1# 选填，默认数据文件路径# slow_query_log_file=/var/lib/mysql/localhost-slow.log 慢查询经验LIMIT分页 优化LIMIT分页 在系统中需要分页的操作通常会使用limit加上偏移量的方法实现，同时加上合适的order by 子句。如果有对应的索引，通常效率会不错，否则MySQL需要做大量的文件排序操作。 一个非常令人头疼问题就是当偏移量非常大的时候，例如可能是limit 1000000,10这样的查询，这是mysql需要查询1000000条然后只返回最后10条，前面的1000000条记录都将被舍弃，这样的代价很高，会造成慢查询。 优化此类查询的一个最简单的方法是尽可能的使用索引覆盖扫描，而不是查询所有的列。然后根据需要做一次关联操作再返回所需的列。对于偏移量很大的时候这样做的效率会得到很大提升。 对于下面的查询： 12-- 执行耗时：1.379sSELECT * from vio_basic_domain_info LIMIT 1000000,10; 该语句存在的最大问题在于limit M,N中偏移量M太大，导致每次查询都要先从整个表中找到满足条件 的前M条记录，之后舍弃这M条记录并从第M+1条记录开始再依次找到N条满足条件的记录。如果表非常大，且筛选字段没有合适的索引，且M特别大那么这样的代价是非常高的。 那么如果我们下一次的查询能从前一次查询结束后标记的位置开始查找，找到满足条件的10条记录，并记下下一次查询应该开始的位置，以便于下一次查询能直接从该位置 开始，这样就不必每次查询都先从整个表中先找到满足条件的前M条记录，舍弃掉，再从M+1开始再找到10条满足条件的记录了。 思路一：构造覆盖索引 通过修改SQL，使用上覆盖索引，比如我需要只查询表中的app_name、createTime等少量字段，那么我秩序在app_name、createTime字段设置联合索引，即可实现覆盖索引，无需全表扫描。适用于查询列较少的场景，查询列数过多的不推荐。 耗时：0.390s 1SELECT app_name,createTime from vio_basic_domain_info LIMIT 1000000,10; 思路二：优化offset 无法用上覆盖索引，那么重点是想办法快速过滤掉前100w条数据。我们可以利用自增主键有序的条件，先查询出第1000001条数据的id值，再往后查10行；适用于主键id自增的场景。耗时：0.471s 123SELECT * from vio_basic_domain_info where id &gt;=(SELECT id from vio_basic_domain_info ORDER BY id limit 1000000,1) limit 10; 方法三：“延迟关联”耗时：0.439s延迟关联适用于数量级较大的表，SQL如下； 12SELECT * from vio_basic_domain_info inner join (select id from vio_basic_domain_info order by id limit 1000000,10) as myNew using(id); 这里我们利用到了覆盖索引+延迟关联查询，相当于先只查询id列，利用覆盖索引快速查到该页的10条数据id，然后再把返回的10条id拿到表中通过主键索引二次查询。（表数据增速快的情况对该方法影响较小。） 索引没起作用 1.模糊查询尽量避免用通配符’%’开头，会导致数据库引擎放弃索引进行全表扫描。如下：12SELECT * FROM t WHERE username LIKE &#x27;%MIKE%&#x27; #不推荐SELECT * FROM t WHERE username LIKE &#x27;MIKE%&#x27; #推荐 如果需求是要在前面使用模糊查询， 使用MySQL内置函数INSTR(str,substr) 来匹配，作用类似于java中的indexOf()，查询字符串出现的角标位置。使用FullText全文索引，用match against 检索数据量较大的情况，建议引用ElasticSearch、solr，亿级数据量检索速度秒级当表数据量较少（几千条儿那种），别整花里胡哨的，直接用like ‘%xx%’。 但不得不说，MySQL模糊匹配大字段是硬伤，毕竟保证事务的ACID特性耗费了太多性能，因此，如果实际场景中有类似业务需求，建议果断更换大数据存储引擎如ElasticSearch、Hbase等。 2.尽量避免使用 not in，会导致引擎走全表扫描。建议用 not exists 代替，如下：12345-- 不走索引SELECT * FROM t WHERE name not IN (&#x27;提莫&#x27;,&#x27;队长&#x27;);-- 走索引select * from t as t1 where not exists (select * from t as t2 where name IN (&#x27;提莫&#x27;,&#x27;队长&#x27;) and t1.id = t2.id); 3.尽量避免使用 or，会导致数据库引擎放弃索引进行全表扫描。如下：1234567SELECT * FROM t WHERE id = 1 OR id = 3优化方式：可以用union代替or。如下：SELECT * FROM t WHERE id = 1 UNIONSELECT * FROM t WHERE id = 3 4.尽量避免进行null值的判断，会导致数据库引擎放弃索引进行全表扫描。如下：12345SELECT * FROM t WHERE score IS NULL优化方式：可以给字段添加默认值0，对0值进行判断。如下：SELECT * FROM t WHERE score = 0 5.尽量避免在where条件中等号的左侧进行表达式、函数操作，会导致数据库引擎放弃索引进行全表扫描。可以将表达式、函数操作移动到等号右侧。如下：1234-- 全表扫描SELECT * FROM T WHERE score/10 = 9-- 走索引SELECT * FROM T WHERE score = 10*9 6.当数据量大时，避免使用where 1=1的条件。通常为了方便拼装查询条件，我们会默认使用该条件，数据库引擎会放弃索引进行全表扫描。如下：12SELECT username, age, sex FROM T WHERE 1=1优化方式：用代码拼装sql时进行判断，没 where 条件就去掉 where，有where条件就加 and。 7.查询条件不要用 &lt;&gt; 或者 !=使用索引列作为条件进行查询时，需要避免使用&lt;&gt;或者!=等判断条件。如确实业务需要，使用到不等于符号，需要在重新评估索引建立，避免在此字段上建立索引，改由查询条件中其他索引字段代替。 8.where条件仅包含复合索引非前导列如：复合（联合）索引包含key_part1，key_part2，key_part3三列，但SQL语句没有包含索引前置列”key_part1”，按照MySQL联合索引的最左匹配原则，不会走联合索引。 1234-- 不走索引select col1 from table where key_part2=1 and key_part3=2-- 走索引select col1 from table where key_part1 =1 and key_part2=1 and key_part3=2 9.隐式类型转换造成不使用索引如下SQL语句由于索引对列类型为varchar，但给定的值为数值，涉及隐式类型转换，造成不能正确走索引。 1select col1 from table where col_varchar=123; 面试题你对MySQL的慢查询优化有了解吗 标准回答 ​ 慢查询优化的前提是定位到响应慢的SQL，这可以通过启用慢查询日志来实现。默认情况下，MySQL并不启用慢查询日志，我们需要手动开启这个参数。通过日志定位到慢查询的SQL之后，我们可以使用EXPLAIN语句来分析这个SQL，进而发现问题所在。导致慢查询的原因有很多，下面列举几种常见的原因，以及对应的解决方案： 向数据库请求了多余的数据： 很多时候，我们的SQL返回的结果会超出我们的需要，例如实际上它返回了更多的行，而我们只要其中的一部分。又或者我们要求返回所有的列，实际上却只有其中少数的列。对于这类问题，我们可以通过LIMIT控制返回的行数，尽量不用SELECT *避免查询到过多的列。 SQL复杂导致无法利用缓存： 处于业务的需要，我们经常会写出比较复杂的SQL，这自然包括复杂的关联查询。由于复杂SQL返回的结果涉及多张表、多个条件、甚至各种函数，这样的SQL每次返回的结果势必不同，所以很难利用到数据库的缓存。如果我们将复杂SQL进行拆分，变成若干简单的SQL，那么其中有些SQL由于条件不变，就可以利用到数据库的缓存了，从而让查询效率得以提升。 没有选择正确的索引： 我们都知道，创建索引是提高查询效率的一个常用手段，事实上我们也经常会这样做。但是，很多时候我们创建了索引，通过EXPLAIN查看会发现并没有走这个索引，最终导致SQL执行变慢。所以，不是把索引创建出来就算完成任务，还要分析索引的选择性，根据业务条件不断的优化索引，从而增加索引的命中率。 加分回答 ​ 除上述优化的方向之外，SQL中还有很多地方都有优化的空间，例如COUNT()、关联查询、子查询、GROUP BY、LIMIT、UNION等。总体来说，不同的情况要区别对待，但所有优化的背后是基于慢查询日志的定位。另外，为了能够发现问题的本质，还需要对MySQL执行查询的过程有所了解： 客户端发送一条查询SQL给服务器。 服务器先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。 服务器进行SQL解析和预处理，再由优化器生成对应的执行计划。 服务器根据优化器生成的执行计划，调用存储引擎的API来执行查询。 将结果返回给客户端。 延伸阅读​ B+树索引是基于B+树构建出来的有序结构，只有利用上它的有序性才能提高查询的效率。若不满足有序性这个前提，则在这个索引中的查询是离散的，其效率反而更低。查询优化器对索引的选择性，被称为最左前缀原则。 ​ 假设有如下一张表： 复制代码 123456CREATE TABLE t ( a VARCHAR(100), b VARCHAR(100), c VARCHAR(100), KEY idx_union(a,b,c)) ENGINE=INNODB; ​ 假设idx_union的叶子节点数据如下： 复制代码 1(1,3,2), (1,3,3), (1,3,9), (1,3,9), (1,7,4), (1,7,8), (2,1,5), (2,1,7), (2,5,1), (2,5,6) ​ 该索引的选择性示例如下： 复制代码 1234567891011121314151617-- 匹配左前缀select * from T where a=&#x27;&#x27;; -- Yselect * from T where b=&#x27;&#x27;; -- N-- 匹配列前缀select * from T where a like &#x27;x%&#x27;; -- Yselect * from T where a like &#x27;%x&#x27;; -- Nselect * from T where b like &#x27;x%&#x27;; -- N-- 全值匹配select * from T where a=&#x27;&#x27; and b=&#x27;&#x27; and c=&#x27;&#x27;; -- Yselect * from T where c=&#x27;&#x27; and b=&#x27;&#x27; and a=&#x27;&#x27;; -- Y-- 匹配范围值select * from T where a between &#x27;&#x27; and &#x27;&#x27;; -- Yselect * from T where b between &#x27;&#x27; and &#x27;&#x27;; -- N-- 全值匹配 + 范围匹配select * from T where a=&#x27;&#x27; and b between &#x27;&#x27; and &#x27;&#x27;; -- Yselect * from T where b=&#x27;&#x27; and c between &#x27;&#x27; and &#x27;&#x27;; -- Nselect * from T where a between &#x27;&#x27; and &#x27;&#x27; and b=&#x27;&#x27;; -- N","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"SQL","slug":"SQL","permalink":"https://gwtt.github.io/tags/SQL/"}]},{"title":"SQL面试","slug":"SQL语句面试","date":"2022-09-05T10:43:55.628Z","updated":"2022-09-05T10:49:52.459Z","comments":true,"path":"2022/09/05/SQL语句面试/","link":"","permalink":"https://gwtt.github.io/2022/09/05/SQL%E8%AF%AD%E5%8F%A5%E9%9D%A2%E8%AF%95/","excerpt":"","text":"1.SQL的执行顺序 from&gt;join&gt;where&gt;group by&gt;聚合函数&gt;having&gt;select&gt;order by&gt;limit 1、最先执行from table； 需要先确定从哪个表中取数据，所以最先执行from table。 2、join连接 用于把来自两个或多个表的行结合起来，简单补充一下连接的类型 自然连接（natural join） 内连接（inner join）：内连接查询能将左表和右表中能关联起来的数据连接后返回，返回的结果就是两个表中所有相匹配的数据。 外连接（outer join）：外连接分为左外连接（LEFT JOIN：即使右表中没有匹配，也从左表返回所有的行）、右外连接（RIGHT JOIN：即使左表中没有匹配，也从右表返回所有的行）、还有一个FULL JOIN(全连接)，不过MYSQL不支持全连接 交叉连接（cross join）即笛卡尔连接 3、where语句； where语句是对条件加以限定 4、分组语句【group by…… having】； group by是分组语句 having是和group by配合使用的，用来作条件限定 5、聚合函数； 常用的聚合函数有max，min， count，sum，聚合函数的执行在group by之后，having之前 举例：count函数查询分组后，每一组分别有多少条数据 1select count(*) from user group by gender 值得注意的是：聚合函数的执行在group by之后，having之前 6、select语句； 对分组聚合完的表挑选出需要查询的数据 7、Distinct distinct对数据进行去重 如果sql语句存在聚合函数，例如count、max等，会先执行聚合函数再去重 8、order by排序语句。 order by排序语句 12select * from user order by id 升序排序select * from user order by id desc 降序排序 9、limit limit用于指定返回的数据条数 12345select * from user limit 2从user表中查询前两条数据该sql等同于select * from user limit 0,2表示从第0条开始取两条数据 limit常配合order by使用 12select * from user order by id limit 3根据id排序，选出id排序前三的数据 总结 from&gt;join&gt;where&gt;group by&gt;聚合函数&gt;having&gt;select&gt;order by&gt;limit 例子123456789select distinct user.name from user join vip on user.id=vip.id where user.id&gt;10 group by user.mobile having count(*)&gt;2 order by user.idlimit 3; 执行顺序 from user join vip on user.id=vip.id ，join是表示要关联的表，on是连接的条件 where user.id&gt;10 group by user.mobile 根据user.mobile分组 然后先执行count(*)在执行having，查询分组之后数量大于2的分组数据 select 对分组聚合完的表挑选出需要查询的数据 distinct查询出来的数据去重 order by user.id 对去重后的数据排序 limit 3对排序后的数据选出前面3条","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"SQL","slug":"SQL","permalink":"https://gwtt.github.io/tags/SQL/"}]},{"title":"Linux防火墙","slug":"Linux防火墙开启","date":"2022-09-05T10:32:23.000Z","updated":"2022-09-05T10:35:55.817Z","comments":true,"path":"2022/09/05/Linux防火墙开启/","link":"","permalink":"https://gwtt.github.io/2022/09/05/Linux%E9%98%B2%E7%81%AB%E5%A2%99%E5%BC%80%E5%90%AF/","excerpt":"","text":"Linux 防火墙开启指定端口通常情况下，CentOS 系统部署完成后，关闭并禁用防火墙。但有些特殊情况需要保持防火墙的启用。 12345678910111213141516171819202122232425#打开防火墙systemctl start firewalld#启用防火墙systemctl enable firewalld#查看已经开放的端口firewall-cmd --list-ports#开启指定端口firewall-cmd --zone=public --add-port=[端口号]/[协议] --permanentfirewall-cmd --zone=public --add-port=80/tcp --permanent--zone=&lt;zone&gt; # 指定 zone--add-port=&lt;portid&gt;]/&lt;protocol&gt; # 端口id / 协议--permanent # 永久开启，不添加则重启失效#关闭指定端口firewall-cmd --zone=public –remove-port=[端口号]/[协议] --permanentfirewall-cmd --zone=public –remove-port=80/tcp --permanent#重新加载防火墙systemctl reload firewalld","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"运维","slug":"运维","permalink":"https://gwtt.github.io/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"Mysql代码例子","slug":"Mysql代码例子","date":"2022-09-05T10:21:18.000Z","updated":"2022-09-05T10:29:11.380Z","comments":true,"path":"2022/09/05/Mysql代码例子/","link":"","permalink":"https://gwtt.github.io/2022/09/05/Mysql%E4%BB%A3%E7%A0%81%E4%BE%8B%E5%AD%90/","excerpt":"","text":"建立一个用户允许远程连接，并赋予对应库的权限 123456789mysql -u root -p;#用密码登录mysql数据库use mysql;#使用对应的数据库select host,user,password from user;#查看数据库所有用户和密码修改host：update user set host = &#x27;%&#x27; where user = &#x27;用户名&#x27;;刷新数据：flush privileges;#或者添加用户：grant all privileges on *.* to 用户名@&#x27;%&#x27; identified by &quot;用户密码&quot;;刷新数据：flush privileges;","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"SQL","slug":"SQL","permalink":"https://gwtt.github.io/tags/SQL/"}]},{"title":"gitlab自动部署","slug":"gitlab自动部署","date":"2022-09-03T02:29:18.000Z","updated":"2022-09-05T05:08:22.295Z","comments":true,"path":"2022/09/03/gitlab自动部署/","link":"","permalink":"https://gwtt.github.io/2022/09/03/gitlab%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2/","excerpt":"","text":"本文是关于GItlab CI/CD的自动部署方案 GitLabCI/CD工作原理 将代码托管到Git存储库 在项目根目录创建ci文件.gitlab-ci.yml,在文件中指定构建、测试和部署脚本 GitLab将检测到它并使用名为GitLab Runner的工具运行脚本 脚本被分组为作业，它们共同组成一个管道 运行效果: 首先什么我们要知道Gitlab是什么 是一种类似github的服务，组织可以使用它来提供git存储库的内部管理。 它是一个自我托管的Git-repository管理系统，可以保持用户代码的私密性，并且可以轻松地部署代码的更改。 GitLab安装 检查配置 1234567891011The following is the recommended minimum CPU hardware guidance for a handful of example GitLab user base sizes.4 cores is the recommended minimum number of cores and supports up to 500 users8 cores supports up to 1000 usersThe following is the recommended minimum Memory hardware guidance for a handful of example GitLab user base sizes.4GB RAM is the required minimum memory size and supports up to 500 users8GB RAM supports up to 1000 users一般来说，两核八GB就够用了 1234567查询cpu指令cat /proc/cpuinfo查询内存指令cat /proc/meminfo查询Linux内核版本cat /proc/version友情提示：如果是一核两G不用试了 使用rpm包安装 1234567wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/e17/gitlab-ce-15.3.2-ce.0.el7.x86_64.rpmrpm -ivh gitlab-ce-15.3.2-ce.0.el7.x86_64.rpmvim /etc/gitlab.rb #编辑站点地址gitlab-ctl reconfigure #配置 12345678&gt;启动gitlab-ctl start状态gitlab-ctl status&gt;停止gitlab-ctl stop&gt;重启gitlab-ctl restart 另外docker安装和kubernetes安装就不赘述了 GitLab Runner介绍相关 GitLab Runner简介: GitLab Runner是一个开源项目，用于运行作业并将结果发送GitLab 与GitLabCI结合使用，GitLabCI是GitLab随附的用于协调作业的开源持续集成服务 GitLab Runner是用Go编写的，可以在Linux,macOS和Windos操作系统上运行 容器部署需使用最新Docker版本。 可以根据配置需要配置任意数量的Runner Runner特点: 作业运行控制：同时执行多个作业 作业运行环境: 在本地、使用Docker容器、使用Docker容器并通过SSH执行作业 使用Docker容器在不同的云和虚拟化管理程序上自动缩放 连接到远程SSH服务器 自动重新加载配置，无需重启 易于安装，可作为Linux,macOS和Windos的服务 GitLab Runner类型与状态 类型: shared 共享类型,运行整个平台项目的作业 group项目组类型，运行特定group下所有项目的作业 specific项目类型，运行指定的项目作业 状态 locked：锁定状态，无法运行项目作业 paused：暂停状态，暂时不会接受新的作业 GitLab Runner安装相关 包管理工具 12345678910111213Add the official GitLab repository 添加官方仓库curl -L &quot;https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.rpm.sh&quot; | sudo bashInstall the latest version of GitLab Runner, or skip to the next step to install a specific version 安装最新版本或者下一步sudo yum install gitlab-runnerTo install a specific version of GitLab Runner:安装指定版本yum list gitlab-runner --showduplicates | sort -rsudo yum install gitlab-runner-10.0.0-1更新runnersudo yum updatesudo yum install gitlab-runner GitLab Runner注册 获取shared类型runnertoken 进入系统设置-&gt;Runners 同理也可以找到对应得group CI/CD Runner和单项目得CI/CD Runner 1234/usr/local/bin/gitlab-runner register --locked=&quot;false&quot;#向GitLab-CI注册一个Runner需要两样东西：GitLab-CI的url和注册token。 其中，token是为了确定你这个Runner是所有工程都能够使用的Shared Runner还是具体某一个工程才能使用的Specific Runner。 如果要注册Shared Runner，你需要到管理界面的Runners页面里面去找注册token。如下图所示 GitLab CI/CD支持的执行器有很多种，最常用的是Docker， shell，Kubernets三种。 Shell 是最易于配置的执行器。构建中所需的依赖得你手工装在 Runner 所在机器上。 更好的方式是使用 Docker，它让你拥有干净的构建环境，以及简易的依赖管理——所有的编译项目所需的依赖都可以放进 Docker 镜像中。Docker 执行器很容易就能创建带有依赖服务的编译环境，比如 MySQL。 1234567891011docker run --rm -v v/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner register \\ --non-interactive \\ --executor &quot;docker&quot; \\ --docker-image alpine:latest \\ --url &quot;url&quot; \\ # 网页端域名 --registration-token &quot;token&quot; \\ # gitlab提供的token --description &quot;first-register-runner&quot; \\ --tag-list &quot;test-cicd1,dockercicd1&quot; \\ --run-untagged=&quot;true&quot; \\ --locked=&quot;false&quot; \\ --access-level=&quot;not_protected&quot; .gitlab-ci.yml文件 如何检查你的yml文件是否符合,可以用CI Lint Pipeline语法1.Job 在.gitlab-ci.yml的文件中，可以定义一个或多个作业（job）。每个作业必须具有唯一的名称（不能使用关键字），每个作业是独立执行。作业定义了在约束条件下进行相关操作，每一个作业至少要包含至少一个script 12job1: script: &quot;execute-script-for-job1&quot; 2.script 1234job: script: - uname -a - bundle exec rspec 有时，script命令将需要用单引号或双引号引起来。例如，包含冒号命令(:)需要加引号，以便被包裹的YAML解析器知道来解释整个事情作为一个字符串，而不是一个”键:值”对.使用特殊字符时要小心: ,&#123;,&#125;,[,], , 等等 3.before_script 跟script差不多，只不过在作业之前运行，如果失败则整个任务失败，作业失败不会影响after_sciprt 4.stages 用于定义作业可以使用的阶段，并且是全局定义的。同一阶段的作业并行运行，不同阶段按顺序执行 12345stages: - build - test - codescan - deploy 5. .pre&amp;.post .pre始终是整个管道的第一个运行阶段, .post始终是整个管道的最后一个运行阶段。用户定义的阶段都在两者之间运行。.pre和.post的顺序无法更改。如果管道仅包含.pre或.post阶段的作业，则不会创建管道 12345678codescan: stage: .pre tags: - build only: - master script: - echo &quot;codescan&quot; 6.tags(指定runner) 用于从允许运行该项目的所有Runner列表中选择特定的Runner，在Runner注册期间，您可以指定Runner标签 7.allow_failure允许失败 allow_failure允许作业失败，默认值为false。启用后，如果作业失败，将会在用户界面中显示橙色警告。但是，管道的逻辑流程将认为作业成功/通过，并且不会阻塞。假设所有其他作业均成功，则该作业的阶段及其管道将显示相同的橙色警告。但是，关联的提交将被标记”通过”,而不会发出警告。 12345job: stage: test script: - execute allow_failure: true 12345678910111213141516171819202122232425262728293031323334353637383940414243444546variables: MAVEN_CLI_OPTS: &quot;--batch-mode --errors --fail-at-end --show-version -s .m2/settings.xml&quot; MAVEN_OPTS: &quot;-Dmaven.repo.local=.m2/repository&quot; PACKAGE_NAME: &quot;certificate.jar&quot; PACKAGE_BACK_NAME: &quot;certificateBack.jar&quot; PROD_ENV_1: &quot;10.166.41.101&quot;image: maven:3.8.1-openjdk-11stages: - build - deploycache: key: $&#123;CI_COMMIT_REF_SLUG&#125; paths: - .m2/repository - sacc/target/build: stage: build script: - &#x27;cd sacc&#x27; - &#x27;mvn $MAVEN_CLI_OPTS package -Dmaven.test.skip=true -Pprod&#x27; - &#x27;ls -al&#x27; - &#x27;ls -al target&#x27; only: - masterdeploy-prod: stage: deploy script: - &#x27;mkdir -p ~/.ssh &amp;&amp; chmod 700 ~/.ssh/&#x27; - &#x27;echo &quot;$SSH_PRIVATE_KEY&quot; &gt;&gt; ./id_rsa &amp;&amp; chmod 600 ./id_rsa&#x27; - &#x27;echo -e &quot;Host *\\n\\tStrictHostKeyChecking no\\n\\n&quot; &gt; ~/.ssh/config&#x27; - &#x27;ls -al sacc/target&#x27; - &#x27;ssh -i ./id_rsa root@$PROD_ENV_1 &quot;yum install net-tools&quot;&#x27; - &#x27;ssh -i ./id_rsa root@$PROD_ENV_1 &quot;/bin/cp -rf /opt/webapps/certificate/$PACKAGE_NAME \\&quot;/opt/webapps/certificate/$PACKAGE_NAME.$(date +\\&quot;%y%m%d\\&quot;)\\&quot;&quot;&#x27; - &#x27;scp -i ./id_rsa -r sacc/target/$PACKAGE_NAME root@$PROD_ENV_1:/opt/webapps/certificate/$PACKAGE_NAME&#x27; - &#x27;ssh -i ./id_rsa root@$PROD_ENV_1 &quot;cp /dev/null /opt/webapps/certificate/nohup.out&quot;&#x27; - &quot;ssh -i ./id_rsa root@$PROD_ENV_1 \\&quot;pkill -f certificate.jar\\&quot;&quot; - &#x27;ssh -i ./id_rsa root@$PROD_ENV_1 &quot;nohup java -jar /opt/webapps/certificate/certificate.jar &amp;&gt; /opt/webapps/certificate/nohup.out &amp;&quot;&#x27; only: - master","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"运维","slug":"运维","permalink":"https://gwtt.github.io/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"五种IO模型","slug":"五种IO模型","date":"2022-09-01T10:54:21.938Z","updated":"2022-09-01T11:45:02.177Z","comments":true,"path":"2022/09/01/五种IO模型/","link":"","permalink":"https://gwtt.github.io/2022/09/01/%E4%BA%94%E7%A7%8DIO%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"Java中的三大IO模型在JDK1.4之前，基于Java所有的socket通信都采用了同步阻塞模型（BIO），这种模型性能低下，当时大型的服务均采用C或C++开发，因为它们可以直接使用操作系统提供的异步IO或者AIO，使得性能得到大幅提升。 2002年，JDK1.4发布，新增了java.nio包，提供了许多异步IO开发的API和类库。新增的NIO，极大的促进了基于Java的异步非阻塞的发展和应用。 2011年，JDK7发布，将原有的NIO进行了升级，称为NIO2.0，其中也对AIO进行了支持。 BIO模型 java中的BIO是blocking I/O的简称，它是同步阻塞型IO，其相关的类和接口在java.io下。 BIO模型简单来讲，就是服务端为每一个请求都分配一个线程进行处理，如下： 示例代码： public class BIOServer { 12345678910111213141516171819202122232425public static void main(String[] args) throws Exception &#123; ServerSocket serverSocket = new ServerSocket(6666); ExecutorService executorService = Executors.newCachedThreadPool(); while (true) &#123; System.out.println(&quot;等待客户端连接。。。。&quot;); Socket socket = serverSocket.accept(); //阻塞 executorService.execute(() -&gt; &#123; try &#123; InputStream inputStream = socket.getInputStream(); //阻塞 byte[] bytes = new byte[1024]; while (true)&#123; int length = inputStream.read(bytes); if(length == -1)&#123; break; &#125; System.out.println(new String(bytes, 0, length, &quot;UTF-8&quot;)); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;); &#125;&#125; }这种模式存在的问题： 客户端的并发数与后端的线程数成1:1的比例，线程的创建、销毁是非常消耗系统资源的，随着并发量增大，服务端性能将显著下降，甚至会发生线程堆栈溢出等错误。 当连接创建后，如果该线程没有操作时，会进行阻塞操作，这样极大的浪费了服务器资源。 NIO模型 NIO，称之为New IO 或是 non-block IO （非阻塞IO），这两种说法都可以，其实称之为非阻塞IO更恰当一些。 NIO相关的代码都放在了java.nio包下，其三大核心组件：Buffer（缓冲区）、Channel（通道）、Selector（选择器/多路复用器） Buffer 在NIO中，所有的读写操作都是基于缓冲区完成的，底层是通过数组实现的，常用的缓冲区是ByteBuffer，每一种java基本类型都有对应的缓冲区对象（除了Boolean类型），如：CharBuffer、IntBuffer、LongBuffer等。 Channel 在BIO中是基于Stream实现，而在NIO中是基于通道实现，与流不同的是，通道是双向的，既可以读也可以写。 Selector Selector是多路复用器，它会不断的轮询注册在其上的Channel，如果某个Channel上发生读或写事件，这个Channel就处于就绪状态，会被Selector轮询出来，然后通过SelectionKey获取就绪Channel的集合，进行IO的读写操作。 基本示意图如下： 可以看出，NIO模型要优于BIO模型，主要是： 通过多路复用器就可以实现一个线程处理多个通道，避免了多线程之间的上下文切换导致系统开销过大。 NIO无需为每一个连接开一个线程处理，并且只有通道真正有有事件时，才进行读写操作，这样大大的减少了系统开销。 示例代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class SelectorDemo &#123; /** * 注册事件 * * @return */ private Selector getSelector() throws Exception &#123; //获取selector对象 Selector selector = Selector.open(); ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); //非阻塞 //获取通道并且绑定端口 ServerSocket socket = serverSocketChannel.socket(); socket.bind(new InetSocketAddress(6677)); //注册感兴趣的事件 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); return selector; &#125; public void listen() throws Exception &#123; Selector selector = this.getSelector(); while (true) &#123; selector.select(); //该方法会阻塞，直到至少有一个事件的发生 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator(); while (iterator.hasNext()) &#123; SelectionKey selectionKey = iterator.next(); process(selectionKey, selector); iterator.remove(); &#125; &#125; &#125; private void process(SelectionKey key, Selector selector) throws Exception &#123; if(key.isAcceptable())&#123; //新连接请求 ServerSocketChannel server = (ServerSocketChannel)key.channel(); SocketChannel channel = server.accept(); channel.configureBlocking(false); //非阻塞 channel.register(selector, SelectionKey.OP_READ); &#125;else if(key.isReadable())&#123; //读数据 SocketChannel channel = (SocketChannel)key.channel(); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); channel.read(byteBuffer); System.out.println(&quot;form 客户端 &quot; + new String(byteBuffer.array(), 0, byteBuffer.position())); &#125; &#125; public static void main(String[] args) throws Exception &#123; new SelectorDemo().listen(); &#125;&#125; AIO模型 在NIO中，Selector多路复用器在做轮询时，如果没有事件发生，也会进行阻塞，如何能把这个阻塞也优化掉呢？那么AIO就在这样的背景下诞生了。 AIO是asynchronous I/O的简称，是异步IO，该异步IO是需要依赖于操作系统底层的异步IO实现。 AIO的基本流程是：用户线程通过系统调用，告知kernel内核启动某个IO操作，用户线程返回。kernel内核在整个IO操作（包括数据准备、数据复制）完成后，通知用户程序，用户执行后续的业务操作。 kernel的数据准备 将数据从网络物理设备（网卡）读取到内核缓冲区。 kernel的数据复制 将数据从内核缓冲区拷贝到用户程序空间的缓冲区。 目前AIO模型存在的不足： 需要完成事件的注册与传递，这里边需要底层操作系统提供大量的支持，去做大量的工作。 Windows 系统下通过 IOCP 实现了真正的异步 I/O。但是，就目前的业界形式来说，Windows 系统，很少作为百万级以上或者说高并发应用的服务器操作系统来使用。 而在 Linux 系统下，异步IO模型在2.6版本才引入，目前并不完善。所以，这也是在 Linux 下，实现高并发网络编程时都是以 NIO 多路复用模型模式为主。 Reactor模型 Reactor线程模型不是Java专属，也不是Netty专属，它其实是一种并发编程模型，是一种思想，具有指导意义。比如，Netty就是结合了NIO的特点，应用了Reactor线程模型所实现的。 Reactor模型中定义的三种角色： Reactor：负责监听和分配事件，将I/O事件分派给对应的Handler。新的事件包含连接建立就绪、读就绪、写就绪等。 Acceptor：处理客户端新连接，并分派请求到处理器链中。 Handler：将自身与事件绑定，执行非阻塞读/写任务，完成channel的读入，完成处理业务逻辑后，负责将结果写出channel。 常见的Reactor线程模型有三种，如下： Reactor单线程模型 Reactor多线程模型 主从Reactor多线程模型 单Reactor单线程模型 说明: Reactor充当多路复用器角色，监听多路连接的请求，由单线程完成 Reactor收到客户端发来的请求时，如果是新建连接通过Acceptor完成，其他的请求Handler完成。 Handler完成业务逻辑的处理，基本的流程是：Read –&gt; 业务处理 –&gt; Send 。 这种模型优点: 结构简单，由单线程完成，没有多线程、进程通信等问题 适合在一些业务逻辑比较简单、对于性能要求不高的应用场景 缺点： 由于是单线程操作、不能充分发挥多核CPU的性能 当Reactor线程负载过重之后、处理速度将变慢，这会导致大量客户端连接超时，超时之后往往会进行重发，这更加重Reactor线程的负载，最终会导致大量消息积压和处理超时，成为系统的性能瓶颈。 可靠性差，如果该线程进入死循环或意外终止，就会导致整个通信系统不可用，容易造成单点故障。 单Reactor多线程模型 说明: 在Reactor多线程模型相比较单线程模型而言，不同点在于，Handler不会处理业务逻辑，只是负责响应用户请求，真正的业务逻辑，在另外的线程中完成。 这样可以降低Reactor的性能开销，充分利用CPU资源，从而更专注的做事件分发工作了，提升整个应用的吞吐。 但是这个模型存在的问题： 多线程数据共享和访问比较复杂。如果子线程完成业务处理后，把结果传递给主线程Reactor进行发送，就会涉及共享数据的互斥和保护机制。 Reactor承担所有事件的监听和响应，只在主线程中运行，可能会存在性能问题。例如并发百万客户端连接，或者服务端需要对客户端握手进行安全认证，但是认证本身非常损耗性能。 为了解决性能问题，产生了第三种主从Reactor多线程模型。 主从Reactor多线程模型 在主从模型中，将Reactor分成2部分： MainReactor负责监听server socket，用来处理网络IO连接建立操作，将建立的socketChannel指定注册给SubReactor。 SubReactor主要完成和建立起来的socket的数据交互和事件业务处理操作。 该模型的优点： 响应快，不必为单个同步事件所阻塞，虽然Reactor本身依然是同步的。 可扩展性强，可以方便地通过增加SubReactor实例个数来充分利用CPU资源。 可复用性高，Reactor模型本身与具体事件处理逻辑无关，具有很高的复用性。 Netty模型 Netty模型是基于Reactor模型实现的，对于以上三种模型都有非常好的支持，也非常的灵活，一般情况，在服务端会采用主从架构模型，基本示意图如下： 说明： 在Netty模型中，负责处理新连接事件的是BossGroup，负责处理其他事件的是WorkGroup。Group就是线程池的概念。 NioEventLoop表示一个不断循环的执行处理任务的线程，用于监听绑定在其上的读/写事件。 通过Pipeline（管道）执行业务逻辑的处理，Pipeline中会有多个ChannelHandler，真正的业务逻辑是在ChannelHandler中完成的。","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Linux","slug":"Linux","permalink":"https://gwtt.github.io/tags/Linux/"},{"name":"IO","slug":"IO","permalink":"https://gwtt.github.io/tags/IO/"}]},{"title":"浅谈AQS","slug":"浅谈AQS","date":"2022-09-01T09:53:23.000Z","updated":"2022-09-01T10:43:11.081Z","comments":true,"path":"2022/09/01/浅谈AQS/","link":"","permalink":"https://gwtt.github.io/2022/09/01/%E6%B5%85%E8%B0%88AQS/","excerpt":"","text":"AQS的原理 AQS:全称是AbstractQuenedSynchronizer（抽象队列同步器）。是除了java自带的synchronized关键字之外的锁机制。 AQS的核心思想是: 如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并将共享资源设置为锁定状态，如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH（Craig，Landin，and Hagersten locks）队列是一个虚拟的双向队列，虚拟的双向队列即不存在队列实例，仅存在节点之间的关联关系。AQS是将每一条请求共享资源的线程封装成一个CLH锁队列的一个结点（Node），来实现锁的分配。 用大白话来说，AQS就是基于CLH队列，用volatile修饰共享变量state，线程通过CAS去改变状态符，成功则获取锁成功，失败则进入等待队列，等待被唤醒。 注意：AQS是自旋锁：在等待唤醒的时候，经常会使用自旋（while(!cas())）的方式，不停地尝试获取锁，直到被其他线程获取成功 实现了AQS的锁有：自旋锁、互斥锁、读锁写锁、条件产量、信号量、栅栏都是AQS的衍生物 如图示，AQS维护了一个volatile int state和一个FIFO线程等待队列，多线程争用资源被阻塞的时候就会进入这个队列。state就是共享资源，其访问方式有如下三种： 12345getState()setState()compareAndSetState() AQS 定义了两种资源共享方式：1.Exclusive：独占，只有一个线程能执行，如ReentrantLock2.Share：共享，多个线程可以同时执行，如Semaphore、CountDownLatch、ReadWriteLock，CyclicBarrier 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了。自定义同步器实现时主要实现以下几种方法： 12345isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 state状态AbstractQueuedSynchronizer维护了一个volatile int类型的变量，用户表示当前同步状态。volatile虽然不能保证操作的原子性，但是保证了当前变量state的可见性。 1234567891011121314151617181920212223242526272829303132333435363738/** * The synchronization state. */private volatile int state; /** * Returns the current value of synchronization state. * This operation has memory semantics of a &#123;@code volatile&#125; read. * @return current state value */protected final int getState() &#123; return state;&#125;/** * Sets the value of synchronization state. * This operation has memory semantics of a &#123;@code volatile&#125; write. * @param newState the new state value */protected final void setState(int newState) &#123; state = newState;&#125;/** * Atomically sets synchronization state to the given updated * value if the current state value equals the expected value. * This operation has memory semantics of a &#123;@code volatile&#125; read * and write. * * @param expect the expected value * @param update the new value * @return &#123;@code true&#125; if successful. False return indicates that the actual * value was not equal to the expected value. */protected final boolean compareAndSetState(int expect, int update) &#123; // See below for intrinsics setup to support this return unsafe.compareAndSwapInt(this, stateOffset, expect, update);&#125; 应用实现 Lock 12345678910111213141516171819202122232425static final class NonfairSync extends Sync &#123; private static final long serialVersionUID = 7316153563782823691L; /** * Performs lock. Try immediate barge, backing up to normal * acquire on failure. */ final void lock() &#123; if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires); &#125;&#125; public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 通过注释我们知道，acquire方法是一种互斥模式，且忽略中断。该方法至少执行一次tryAcquire(int)方法，如果tryAcquire(int)方法返回true，则acquire直接返回，否则当前线程需要进入队列进行排队。函数流程如下： tryAcquire()尝试直接去获取资源，如果成功则直接返回； addWaiter()将该线程加入等待队列的尾部，并标记为独占模式； acquireQueued()使线程在等待队列中获取资源，一直获取到资源后才返回。如果在整个等待过程中被中断过，则返回true，否则返回false。 如果线程在等待过程中被中断过，它是不响应的。只是获取资源后才再进行自我中断selfInterrupt()，将中断补上。 tryAcquire12345678910111213141516171819202122protected boolean tryAcquire(int arg) &#123; throw new UnsupportedOperationException();&#125;final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false; &#125; 1.判断状态位是否为0,0是可以占用,如果是0的话占用,不是0的话返回false 2.判断当前线程是否为得到位置的线程,比如如果前一个线程走了,然后又回来有点事情的话,那么返回false","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Juc","slug":"Juc","permalink":"https://gwtt.github.io/tags/Juc/"}]},{"title":"手动实现栈和队列","slug":"手动实现栈和队列","date":"2022-09-01T04:55:57.000Z","updated":"2022-09-01T09:41:55.345Z","comments":true,"path":"2022/09/01/手动实现栈和队列/","link":"","permalink":"https://gwtt.github.io/2022/09/01/%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/","excerpt":"","text":"1. 手动实现栈 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import java.util.Arrays;public class MyStack&lt;T&gt; &#123; private T[]stack;//数组 private int top;//当前可以存放数据元素的下标——&gt;栈顶指针 //用构造函数给定一个初始容量10的数组 public MyStack( )&#123; this.stack = (T[])new Object[10];//泛型不能实例化对象，但是可以类型转换 &#125; //判断栈是否满了 public boolean isFull()&#123; return (stack.length == this.top); &#125; //判断栈是否为空 public boolean empty()&#123; return this.top == 0; &#125; //入栈操作 public void push(T value) &#123; //判断栈是否已经满了 if (isFull())&#123; this.stack = Arrays.copyOf(stack,2*stack.length);//满了就扩容成原来容量的两倍 &#125; this.stack[this.top] = value;//给top位置添加元素 this.top++;//top指针指向下一可用空间 &#125; //出栈操作，并返回弹出（删除）栈顶元素 public T pop() &#123; //先判断栈是否为空 if (empty()) &#123; throw new IllegalStateException(&quot;栈为空！&quot;); &#125; //弹出元素 T ret = this.stack[this.top-1]; this.top--; return ret;//返回删除的元素 &#125; //得到栈顶元素，但是不删除 public T peek() &#123; //判断是否为空 if (empty())&#123; throw new IllegalStateException(&quot;栈为空！&quot;); &#125; //返回栈顶元素，不删除 return this.stack[top-1]; &#125; //展示栈元素 public void show() &#123; for (int i = top-1; i&gt;=0 ; i--)&#123; System.out.print(stack[i]+&quot; &quot;); &#125; System.out.println(); &#125;&#125; 2.手动实现队列 123456789101112131415161718192021222324public class Node &#123; private int val; private Node next; public Node(int val)&#123; this.val = val; &#125; public int getVal() &#123; return val; &#125; public void setVal(int val) &#123; this.val = val; &#125; public Node getNext() &#123; return next; &#125; public void setNext(Node next) &#123; this.next = next; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class MyQueue &#123; private Node first; private Node last; //队列是否为空 public boolean isEmpty() &#123; return this.first == null; &#125; //入队 public void offer(int value)&#123; Node node = new Node(value); //尾插法，要判断是否第一次插入 if (this.first == null)&#123; this.first = node; this.last = node; &#125;else&#123; this.last.setNext(node); this.last = node; &#125; &#125; //出队 public int poll()&#123; //判断是否为空 if (isEmpty())&#123; throw new IllegalStateException(&quot;队列为空&quot;); &#125; int ret = this.first.getVal(); this.first = this.first.getNext(); return ret;//返回出队元素 &#125; //得到队头元素但是不删除 public int peek() &#123; //不要移动first if(isEmpty()) &#123; throw new UnsupportedOperationException(&quot;队列为空！&quot;); &#125; return this.first.getVal(); &#125; //展示队列 public void show() &#123; Node cur = this.first; while(cur != null) &#123; System.out.print(cur.getVal()+&quot; &quot;); cur = cur.getNext(); &#125; System.out.println(); &#125;&#125;","categories":[{"name":"java知识","slug":"java知识","permalink":"https://gwtt.github.io/categories/java%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"java基础","slug":"java基础","permalink":"https://gwtt.github.io/tags/java%E5%9F%BA%E7%A1%80/"}]},{"title":"Mysql的MVCC","slug":"Mysql的MVCC","date":"2022-08-30T07:06:22.000Z","updated":"2022-08-30T09:32:54.245Z","comments":true,"path":"2022/08/30/Mysql的MVCC/","link":"","permalink":"https://gwtt.github.io/2022/08/30/Mysql%E7%9A%84MVCC/","excerpt":"","text":"什么是MVCC MVCC，全称 Multi-Version Concurrency Control ，即多版本并发控制。mvcc，它是一种并发控制方法，一般在数据库管理系统中，实现数据库的并发访问，在编程语言中实现事务内存。 总结：主要为了提升并发性能 为什么需要MVCC 数据库原生的锁 最原生的锁，锁住一个资源后会禁止其他任何线程访问同一个资源。但是很多应用的一个特点都是读多写少的场景，很多数据的读取次数远大于修改的次数，而读取数据间互相排斥显得不是很必要。 读写锁的出现 读锁和读锁之间不互斥，而写锁和写锁、读锁都互斥。这样就很大提升了系统的并发能力。之后人们发现并发读还是不够 mvcc概念出现 能不能让读写之间也不冲突的方法，就是读取数据时通过一种类似快照的方式将数据保存下来，这样读锁就和写锁不冲突了，不同的事务session会看到自己特定版本的数据。当然快照是一种概念模型，不同的数据库可能用不同的方式来实现这种功能 MVCC适用于的事务隔离级别 MVCC只在 READ COMMITTED (读取已提交) 和 REPEATABLE READ (可重复读) 两个隔离级别下工作。其他两个隔离级别够和MVCC不兼容, 因为 READ UNCOMMITTED (读取未提交) 总是读取最新的数据行, 而不是符合当前事务版本的数据行。而 SERIALIZABLE (可串行化) 则会对所有读取的行都加锁。 MVCC实现原理✔ MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决读写冲突，它的实现原理主要是依赖记录中的 3个隐式字段，undo日志 ，**Read View** 来实现的。 3个隐式字段1`DB_TRX_ID`, `DB_ROLL_PTR`, `DB_ROW_ID 列名 长度（字节） 作用 DB_TRX_ID 6 插入或更新行的最后一个事务的事务标识符。（删除视为更新，将其标记为已删除） DB_ROLL_PTR 7 写入回滚段的撤销日志记录（若行已更新，则撤销日志记录包含在更新之前重建行内容所需的信息） DB_ROW_ID 6 行标识（隐藏单调自增id） 比如: id name age DB_ROW_ID DB_TRX_ID DB_ROLL_PTR 1 张三 18 1 空 空 DB_ROW_ID 是数据库默认为该行记录生成的唯一隐式主键，DB_TRX_ID 是当前操作该记录的事务 ID ,而 DB_ROLL_PTR 是一个回滚指针，用于配合 undo日志，指向上一个旧版本 事务A：对数据进行了修改（将name中的张三改为李四） 第一步：用排他锁锁定这一条记录 id name age DB_ROW_ID DB_TRX_ID DB_ROLL_PTR 1 张三 18 1 空 空 第二步：UNDOLOG会记录日志，作为旧记录，既在 undo log 中有当前行的拷贝副本 UNDO_LOG id name age DB_ROW_ID DB_TRX_ID DB_ROLL_PTR 1 张三 18 1 空 空 第三步：将回滚指针的值copy到UNDOLOG中 UNDO_LOG id name age DB_ROW_ID DB_TRX_ID DB_ROLL_PTR(这就是存储回滚指针的值) 1 张三 18 1 空 ox29349384 第四步：修改当前的name值并且修改隐藏字段的事务 ID 为当前事务 1的 ID, 我们默认从 1 开始，之后递增，回滚指针指向拷贝到 undo log 的副本记录，既表示我的上一个版本就是它 id name age DB_ROW_ID DB_TRX_ID DB_ROLL_PTR(这就是存储回滚指针的值) 1 李四 18 1 1 ox29349384 事务B：事务A修改但未提交，同时对事务B也对该行数据做了修改 下表就是事务B做出的改变（改变的是年龄） id name age DB_ROW_ID DB_TRX_ID DB_ROLL_PTR 1 张三 30 1 2 ox23874982 上表的ox23874982指的地址是下表的地址 UNDO_LOG id name age DB_ROW_ID DB_TRX_ID DB_ROLL_PTR 1 张三 18 1 1 ox29349384 上表的ox29349384指的地址是下表的地址 id name age DB_ROW_ID DB_TRX_ID DB_ROLL_PTR 1 张三 18 1 空 空 所以总结: 如果有当前事务，最早事务，最晚事务 最早事务ID&lt;当前事务ID&lt;最晚事务ID 事务的排他锁形式修改数据 修改之前先把数据放到undolog，通过回滚指针关联 失败了从undolog回滚 undo日志 insert undo log代表事务在 insert 新记录时产生的 undo log, 只在事务回滚时需要，并且在事务提交后可以被立即丢弃 update undo log事务在进行 update 或 delete 时产生的 undo log ; 不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被 purge 线程统一清除在不考虑redo log 的情况下利用undo log工作的简化过程为： 序号 动作 1 开始事务 2 记录数据行数据快照到undolog 3 更新数据 4 将undolog写到磁盘 5 将数据写到磁盘 6 提交事务 1）为了保证数据的持久性数据要在事务提交之前持久化 2）undo log的持久化必须在在数据持久化之前，这样才能保证系统崩溃时，可以用undo log来回滚事务 执行流程如下： *一、比如一个有个事务插入 persion 表插入了一条新记录，记录如下，name 为 小明 , age 为 10 岁，*隐式主键*是 1，*事务 ID*和*回滚指针*，我们假设为 NULL* 二、 现在来了一个事务 1对该记录的 name 做出了修改，改为 小红 在事务 1修改该行(记录)数据时，数据库会先对该行加排他锁 然后把该行数据拷贝到 undo log 中，作为旧记录，既在 undo log 中有当前行的拷贝副本 Undo日志 拷贝完毕后，修改该行name为小红，并且修改隐藏字段的事务 ID 为当前事务 1的 ID, 我们默认从 1 开始，之后递增，回滚指针指向拷贝到 undo log 的副本记录，既表示我的上一个版本就是它 上个表的回滚指针地址指的是下个表 事务提交后，释放锁","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Mysql","slug":"Mysql","permalink":"https://gwtt.github.io/tags/Mysql/"}]},{"title":"聚集索引和非聚集索引","slug":"聚簇索引和非聚簇索引","date":"2022-08-29T08:51:49.429Z","updated":"2022-08-29T09:10:55.643Z","comments":true,"path":"2022/08/29/聚簇索引和非聚簇索引/","link":"","permalink":"https://gwtt.github.io/2022/08/29/%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95%E5%92%8C%E9%9D%9E%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95/","excerpt":"","text":"聚集索引和非聚集索引什么是聚集索引 聚集索引是将索引列字段和行记录数据维护在了一起,它的叶子节点存储的是 索引列字段 + 完整的行记录数据,通过聚集索引能直接获取到整行数据 Innodb 的主键索引就是基于聚集索引实现的 通俗点讲:利用聚集索引可以直接获取对应的元素数据 什么是非聚集索引 非聚集索引是相比较于聚集索引来说,它是把索引和行数据分开维护,叶子节点并没有包含完整的数据记录(叶子节点的数据区存储的是聚集索引的 id 或 数据的磁盘地址)Mysql 非聚集索引底层的数据结构也是 b+ 树,例如 Myisam 的索引、Innodb 的辅助索引 比如在搜索年龄为41的数据时，会找到13，然后再去找利用聚集索引找主键为13的数据 回表（尽量去减少回表的产生） 当通过非聚集索引来查询数据时,存储引擎会根据索引字段定位到最底层的叶子节点,并通过叶子节点获得指向主键索引的主键 id,然后通过主键 id 去主键索引(聚集索引)上找到一个完整的行记录.这个过程被称为 回表","categories":[{"name":"数据库","slug":"数据库","permalink":"https://gwtt.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Mysql","slug":"Mysql","permalink":"https://gwtt.github.io/tags/Mysql/"},{"name":"索引","slug":"索引","permalink":"https://gwtt.github.io/tags/%E7%B4%A2%E5%BC%95/"}]},{"title":"jdk动态代理和cglib动态代理","slug":"jdk动态代理和cgib动态代理","date":"2022-08-28T08:32:02.000Z","updated":"2022-08-28T11:54:16.423Z","comments":true,"path":"2022/08/28/jdk动态代理和cgib动态代理/","link":"","permalink":"https://gwtt.github.io/2022/08/28/jdk%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E5%92%8Ccgib%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/","excerpt":"","text":"两者有何区别1、Jdk动态代理：利用拦截器（必须实现InvocationHandler接口）加上反射机制生成一个代理接口的匿名类，在调用具体方法前调用InvokeHandler来处理 JDK动态代理主要是通过，反射包中的Porxy类和InvokationHandler接口。它们结合在一起后可以创建动态代理类。Porxy类基于传递的参数创建动态代理类。InvokationHandler则用于激发动态代理类的方法。这个过程是在程序执行过程中动态生成与处理的，所以叫动态代理。 2、 Cglib动态代理：利用ASM框架，对代理对象类生成的class文件加载进来，通过修改其字节码生成子类来进行代理 所以： 如果想要实现JDK动态代理那么代理类必须实现接口，否则不能使用; 如果想要使用CGlib动态代理，那么代理类不能使用final修饰类和方法； 还有： 在jdk6、jdk7、jdk8逐步对JDK动态代理优化之后，在调用次数较少的情况下，JDK代理效率高于CGLIB代理效率，只有当进行大量调用的时候，jdk6和jdk7比CGLIB代理效率低一点，但是到jdk8的时候，jdk代理效率高于CGLIB代理。 JDk动态代理例子 UserService接口 1234567public interface UserService &#123; void addUser(); void updateUser(String str);&#125; UserServiceImpl实现类 1234567891011public class UserServiceImpl implements UserService &#123; @Override public void addUser() &#123; System.out.println(&quot;添加用户&quot;); &#125; @Override public void updateUser(String str) &#123; System.out.println(&quot;更新用户信息&quot; + str); &#125;&#125; UserProxy代理类，实现InvocationHandler接口重写invoke方法 1234567891011121314151617public class UserProxy implements InvocationHandler &#123; private Object target; public UserProxy(Object target) &#123; this.target = target; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object res = method.invoke(target, args); System.out.println(&quot;记录日志&quot;); return res; &#125;&#125; test测试类 1234567891011public class test &#123; public static void main(String[] args) &#123; UserServiceImpl impl = new UserServiceImpl(); UserProxy userProxy = new UserProxy(impl); UserService userService = (UserService) Proxy.newProxyInstance(impl.getClass().getClassLoader(),impl.getClass().getInterfaces(),userProxy); userService.addUser(); userService.updateUser(&quot;：我是滚韬&quot;); &#125;&#125; CGLIB动态代理 首先要加入依赖 12345&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt;&lt;/dependency&gt; 具体类12345678public class Hello &#123; public String sayHello(boolean throwException) throws Exception &#123; System.out.println(&quot;hello everyone!&quot;); if(throwException) throw new Exception(&quot;test exception&quot;); return &quot;123&quot;; &#125;&#125; 实现MethodInterceptor接口12345678910111213141516171819202122232425262728293031323334353637383940414243public class ProxyFactory implements MethodInterceptor &#123; //要代理的原始对象 private Object obj; public Object createProxy(Object target) &#123; this.obj = target; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(this.obj.getClass());// 设置代理目标 enhancer.setCallback(this);// 设置回调 enhancer.setClassLoader(target.getClass().getClassLoader()); return enhancer.create(); &#125; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; Object result = null; try &#123; // 前置通知 before(); result = proxy.invokeSuper(obj, args); // 后置通知 after(); &#125; catch (Exception e) &#123; exception(); &#125;finally&#123; beforeReturning(); &#125; return result; &#125; private void before() &#123; System.out.println(&quot;before method invoke&quot;); &#125; private void after() &#123; System.out.println(&quot;after method invoke&quot;); &#125; private void exception() &#123; System.out.println(&quot;method invoke exception&quot;); &#125; private void beforeReturning() &#123; System.out.println(&quot;before returning&quot;); &#125;&#125; 测试类12345678910public class EnhancerTest &#123; public static void main(String[] args) throws Exception &#123; Hello hello = new Hello(); ProxyFactory cglibProxy = new ProxyFactory(); Hello proxy = (Hello) cglibProxy.createProxy(hello); String result=proxy.sayHello(true); System.out.println(result); &#125;&#125; 两个动态代理的使用场景是哪里 我们主要是在Spring Aop项目中去使用它们 12345678910111213141516171819202122&gt;@Override&gt;public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException &#123; if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) &#123; Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException(&quot;TargetSource cannot determine target class: &quot; + &quot;Either an interface or a target is required for proxy creation.&quot;); &#125; //如果 if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) &#123; return new JdkDynamicAopProxy(config); &#125; return new ObjenesisCglibAopProxy(config); &#125; else &#123; return new JdkDynamicAopProxy(config); &#125;&gt;&#125;&gt;1、如果目标对象实现了接口，默认情况下会采用JDK的动态代理&gt;2、如果目标对象实现了接口，也可以强制使用CGLIB&gt;3、如果目标对象没有实现了接口，必须采用CGLIB库，spring会自动在JDK动态代理和CGLIB之间转换 如果需要强制使用CGLIB来实现AOP，需要配置spring.aop.proxy-target-class=true或**@EnableAspectJAutoProxy(proxyTargetClass = true** (补充)Porxy类Porxy类提供了一个静态方法创建动态代理类。 1234public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h)throws IllegalArgumentException 1、ClassLoader： ClassLoader会定义动态代理类，ClassLoader可以通过类或者接口获得，如果我们想通过接口获得，调用方法如下。 1Task.class.getClassLoader() 如果通过类来获得，加入我们有一个类TaskImpl实现了Task接口，我们有个TaskImpl的对象ob，然后ClassLoader获取方法如下 1ob.getClassLoader() 2、 Class&lt;?&gt;[] interfaces：动态代理类需要实现的接口 3、InvocationHandler：传递一个实现了InvokationHandler接口的类的实例 InvokationHandler是Java 反射包里面的一个接口。InvokationHandler通过用户类来实现，来激发一个动态代理类的方法。它只有一个方法： 1&gt;public Object invoke(Object proxy, Method method, Object[] args) throws Throwable; 1、Object：实现方法的代理对象 2、Method：代理实例激发的方法，Porxy参数中的接口方法 3、Object[]：传递给方法的一系列参数","categories":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"动态代理","slug":"动态代理","permalink":"https://gwtt.github.io/tags/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"}]},{"title":"手写生产者消费者模式","slug":"手写生产者消费者模式","date":"2022-08-27T11:47:36.000Z","updated":"2022-09-23T01:33:34.448Z","comments":true,"path":"2022/08/27/手写生产者消费者模式/","link":"","permalink":"https://gwtt.github.io/2022/08/27/%E6%89%8B%E5%86%99%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"生产者-消费者模式设计要求这种模式满足三点要求：（1）生产者生产数据到缓冲区中，消费者从缓冲区中取数据。（2）缓冲区满时，生产者线程阻塞，进入等待状态。这期间消费者一旦取走数据，队列未满，就会唤醒阻塞的生产者。（3）缓冲区空时，消费者线程阻塞，进入等待状态。这期间生产者一旦往队列中放入数据，就会唤醒阻塞的消费者。 模式组成：公共的缓存队列（给予缓存上限）+ 生产者线程 + 消费者线程。特点：1.实现了生产者、消费者的解耦：通过共享的数据缓冲区域，生产者生产数据之后直接放置在共享数据区中，并不需要关心消费者的行为；而消费者只需要从共享数据区中去获取数据，就不再需要关心生产者的行为。2.实现了线程间的并发协作：如果共享数据区已满的话，阻塞生产者继续生产数据放置入内；如果共享数据区为空的话，阻塞消费者继续消费数据。应用场景：模式解耦、消息队列、分布式场景中很常见。 通常情况下，有5种方式来实现 synchronized + wait() + notify() 方式 可重入锁ReentrantLock （配合Condition） BlockingQueue 阻塞队列方式 信号量Semaphore 方式 管道输入输出流PipedInputStream和PipedOutputStream 方式 第一种方式（synchronized ）12345678910111213141516171819202122232425262728293031323334public class MyBlockingQueue &#123; private int maxSize; private LinkedList&lt;Integer&gt; queue; public MyBlockingQueue(int size) &#123; this.maxSize = size; queue = new LinkedList&lt;Integer&gt;(); &#125; public synchronized void put() throws InterruptedException&#123; while(queue.size() == maxSize) &#123; System.out.println(&quot;队列已满，生产者：&quot; + Thread.currentThread().getName() + &quot;进入等待&quot;); wait(); &#125; Random random = new Random(); int i = random.nextInt(100); System.out.println(&quot;队列未满，生产者：&quot;+Thread.currentThread().getName() + &quot;放入数据&quot; + i); if(queue.size() == 0) &#123; notifyAll(); &#125; queue.add(i); &#125; public synchronized void take() throws InterruptedException&#123; while(queue.size() == 0) &#123; System.out.println(&quot;队列为空，消费者：&quot; + Thread.currentThread().getName() + &quot;进入等待&quot;); wait(); &#125; if(queue.size() == maxSize) &#123; notifyAll(); &#125; System.out.println(&quot;队列有数据，消费者：&quot;+Thread.currentThread().getName() + &quot;取出数据&quot; + queue.remove());//删除第一个数据，最早放入的数据 &#125; &#125; 12345678910111213141516public class Producer implements Runnable&#123; private MyBlockingQueue myBlockingQueue; public Producer(MyBlockingQueue myBlockingQueue) &#123; this.myBlockingQueue = myBlockingQueue; &#125; public void run() &#123; for(int i = 0 ; i &lt; 5 ; i++) &#123; try &#123; myBlockingQueue.put(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 12345678910111213141516public class Consumer implements Runnable&#123; private MyBlockingQueue myBlockingQueue; public Consumer(MyBlockingQueue myBlockingQueue) &#123; this.myBlockingQueue = myBlockingQueue; &#125; public void run() &#123; for(int i = 0 ; i &lt; 5 ; i++) &#123; try &#123; myBlockingQueue.take(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 1234567891011121314public class producer_consumer_1 &#123; public static void main(String[] args) &#123; MyBlockingQueue myBlockingQueue = new MyBlockingQueue(8); Producer producer = new Producer(myBlockingQueue); Producer producer2 = new Producer(myBlockingQueue); Consumer consumer = new Consumer(myBlockingQueue); Consumer consumer2 = new Consumer(myBlockingQueue); new Thread(producer).start();//生产者线程1，Thread-0 new Thread(producer2).start();//生产者线程2，Thread-1 new Thread(consumer).start();//消费者线程1，Thread-3 new Thread(consumer2).start();//消费者线程2，Thread-4 &#125;&#125; 补充说明：1.使用Linkedlist+等待唤醒机制（wait、notify/notifyAll）+Synchronized实现线程安全。 2.为什么使用while不是if？ 判断线程是否进入等待状态时，判断需要while，不能用if。在生产者、消费者线程只有一个时，if可以使用。但是多个线程的情况时就会出现问题。 例如：假设有两个消费者线程，一个生产者线程。队列为空时，消费者1进入等待状态，释放锁。消费者2抢到锁，进去后判断也进入等待，释放锁。这时生产者抢到锁生产数据，队列中有数据了，反过来唤醒两个消费者。消费者1抢到锁执行wait()的逻辑，取出数据并释放锁。这时消费者2拿到锁，执行wait()后的逻辑取数据，但是此时队列中的数据已经被消费者1取出了，没有数据，这时就会出现线程不安全的情况。利用while实现多次判断，不管消费者1还是2抢到锁，执行循环体的逻辑之前，会再一次判断条件是否成立，而if不会，所以使用while。 第二种方式（ReentrantLock ）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class MyBlockingQueueForCondition &#123; private Queue&lt;Integer&gt; queue; private int max; private ReentrantLock lock = new ReentrantLock(); private Condition notEmpty = lock.newCondition(); private Condition notFull = lock.newCondition(); public MyBlockingQueueForCondition(int max) &#123; this.max = max; queue = new LinkedList&lt;Integer&gt;(); &#125; public void put(int i) throws InterruptedException&#123; lock.lock(); try &#123; while(queue.size() == max) &#123; System.out.println(&quot;队列已满，生产者：&quot; + Thread.currentThread().getName() + &quot;进入等待&quot;); notFull.await(); &#125; if(queue.size() == 0) &#123; notEmpty.signalAll(); &#125; System.out.println(&quot;队列未满，生产者：&quot;+Thread.currentThread().getName() + &quot;放入数据&quot; + i); queue.add(i); &#125; catch (Exception e) &#123; // TODO: handle exception &#125;finally &#123; lock.unlock(); &#125; &#125; public int take() throws InterruptedException&#123; lock.lock(); try &#123; while(queue.size() == 0) &#123; System.out.println(&quot;队列为空，消费者：&quot; + Thread.currentThread().getName() + &quot;进入等待&quot;); notEmpty.await(); &#125; if(queue.size() == max) &#123; notFull.signalAll(); &#125; &#125; catch (Exception e) &#123; // TODO: handle exception &#125;finally &#123; lock.unlock(); &#125; int i = queue.remove(); System.out.println(&quot;队列有数据，消费者：&quot;+Thread.currentThread().getName() + &quot;取出数据&quot; + i); return i; &#125;&#125; 1234567891011121314151617public class ProducerForCondition implements Runnable&#123; private MyBlockingQueueForCondition myBlockingQueueForCondition; public ProducerForCondition(MyBlockingQueueForCondition myBlockingQueueForCondition) &#123; this.myBlockingQueueForCondition = myBlockingQueueForCondition; &#125; public void run() &#123; for(int i = 0 ; i &lt; 5 ; i++) &#123; try &#123; myBlockingQueueForCondition.put(i); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 1234567891011121314151617public class ConsumerForCondition implements Runnable&#123; private MyBlockingQueueForCondition myBlockingQueueForCondition; public ConsumerForCondition(MyBlockingQueueForCondition myBlockingQueueForCondition) &#123; this.myBlockingQueueForCondition = myBlockingQueueForCondition; &#125; public void run() &#123; for(int i = 0 ; i &lt; 5 ; i++) &#123; try &#123; myBlockingQueueForCondition.take(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 123456789101112131415public class producer_consumer_2 &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub MyBlockingQueueForCondition myBlockingQueueForCondition = new MyBlockingQueueForCondition(8); ProducerForCondition producerForCondition1 = new ProducerForCondition(myBlockingQueueForCondition); ProducerForCondition producerForCondition2 = new ProducerForCondition(myBlockingQueueForCondition); ConsumerForCondition consumerForCondition1 = new ConsumerForCondition(myBlockingQueueForCondition); ConsumerForCondition consumerForCondition2 = new ConsumerForCondition(myBlockingQueueForCondition); new Thread(producerForCondition1).start(); new Thread(producerForCondition2).start(); new Thread(consumerForCondition1).start(); new Thread(consumerForCondition2).start(); &#125;&#125; 第三种方式（BlockingQueue ）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class ArrayBlockingQueueTest &#123; private static BlockingQueue&lt;Integer&gt; queue = new ArrayBlockingQueue(10); public static void main(String[] args) &#123; Producer producer1 = new Producer(queue); Consumer consumer1 = new Consumer(queue); new Thread(producer1).start(); new Thread(consumer1).start(); &#125; static class Producer implements Runnable&#123; private BlockingQueue queue; public Producer(BlockingQueue queue) &#123; this.queue = queue; &#125; public void run() &#123; // TODO Auto-generated method stub try &#123; for(int i = 0 ; i &lt; 20;i++) &#123; Random random = new Random(); int element = random.nextInt(100); System.out.println(&quot;生产者&quot; + Thread.currentThread().getName() + &quot;生产数据：&quot; + element); queue.put(element); &#125; &#125; catch (InterruptedException e) &#123; // TODO: handle exception e.printStackTrace(); &#125; &#125; &#125; static class Consumer implements Runnable&#123; private BlockingQueue queue; public Consumer(BlockingQueue queue) &#123; this.queue = queue; &#125; public void run() &#123; // TODO Auto-generated method stub try &#123; for(int i = 0 ; i &lt; 20;i++) &#123; Integer element = (Integer) queue.take(); System.out.println(&quot;消费者&quot; + Thread.currentThread().getName() + &quot;消费数据：&quot; + element); Thread.sleep(1000); &#125; &#125; catch (InterruptedException e) &#123; // TODO: handle exception e.printStackTrace(); &#125; &#125; &#125;&#125;","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"浅谈股票dp问题","slug":"浅谈股票dp问题","date":"2022-08-23T09:40:23.000Z","updated":"2022-08-23T16:03:13.801Z","comments":true,"path":"2022/08/23/浅谈股票dp问题/","link":"","permalink":"https://gwtt.github.io/2022/08/23/%E6%B5%85%E8%B0%88%E8%82%A1%E7%A5%A8dp%E9%97%AE%E9%A2%98/","excerpt":"","text":"笔者在力扣算题时，遇到股票题，觉得很有意思，于是写下自己的总结 1.第一个股票问题（一次买卖） 首先是最简单的题目，只有一次购买，一次卖出 思路还是挺清晰的，还是DP思想： 记录【今天之前买入的最小值】 计算【今天之前最小值买入，今天卖出的获利】，也即【今天卖出的最大获利】 比较【每天的最大获利】，取最大值即可 1234567891011121314151617181920212223242526272829303132333435class Solution &#123; public int maxProfit(int[] prices) &#123; if(prices.length &lt;= 1) return 0; //初始化 int[][] dp = new int[prices.length][2]; //两个状态：手里没股票，手里有股票 dp[0][1] = -prices[0]; dp[0][0] = 0; for(int i = 1;i &lt; prices.length; i ++)&#123; //手里没股票 dp[i][0] = Math.max(dp[i - 1][0],dp[i - 1][1] + prices[i]); //手里有股票 dp[i][1] = Math.max(dp[i - 1][1],-prices[i]); &#125; return dp[prices.length - 1][0]; &#125;&#125;当然因为我们只是用二维数组保存了两个状态，有股票和没股票，所以可以简化一下class Solution &#123; public int maxProfit(int[] prices) &#123; int length = prices.length; //两个状态：手里没股票，手里有股票 int dp0 = 0,dp1 = Integer.MIN_VALUE; for(int i = 0;i &lt; length; i ++)&#123; //手里没股票 dp0 = Math.max(dp0,dp1 + prices[i]); //手里有股票 dp1 = Math.max(dp1,-prices[i]); &#125; //返回没股票的时候 return dp0; &#125;&#125; 总之，没股票可以从昨天的有股票卖出，或者昨天的没股票得出（当然我们要尽量去获取最大值，毕竟利润最大），有股票可以是买今天的，或者昨天的有股票得到。 2.第二个股票问题（多次买卖） 1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123; public int maxProfit(int[] prices) &#123; if(prices.length &lt;= 1) return 0; //初始化 int[][] dp = new int[prices.length][2]; //两个状态：手里没股票，手里有股票 dp[0][1] = -prices[0]; dp[0][0] = 0; for(int i = 1;i &lt; prices.length; i ++)&#123; //手里没股票 dp[i][0] = Math.max(dp[i - 1][0],dp[i - 1][1] + prices[i]); //手里有股票 dp[i][1] = Math.max(dp[i - 1][1],dp[i - 1][0] - prices[i]); &#125; return dp[prices.length - 1][0]; &#125;&#125;注意到上面的状态转移方程中，每一天的状态只与前一天的状态有关，而与更早的状态都无关，因此我们不必存储这些无关的状态，只需要将dp[i-1][0] 和dp[i-1][1] 存放在两个变量中，通过它们计算出 dp[i][0] 和 dp[i][1] 并存回对应的变量，以便于第 i+1 天的状态转移即可。 class Solution &#123; public int maxProfit(int[] prices) &#123; if(prices.length &lt;= 1) return 0; //两个状态：手里没股票，手里有股票 int dp0 = 0,dp1 = -prices[0]; for(int i = 1;i &lt; prices.length; i ++)&#123; //手里没股票 dp0 = Math.max(dp0,dp1 + prices[i]); //手里有股票 dp1 = Math.max(dp1,dp0 - prices[i]); &#125; //返回没股票的时候 return dp0; &#125;&#125; 相比第一题，这题多了一个条件，可以多次买卖。 总之，没股票可以从昨天的有股票卖出，或者昨天的没股票得出（当然我们要尽量去获取最大值，毕竟利润最大），有股票可以是从之前没股票的状态买今天的（之前的是不能的，只能是买一次），或者昨天的有股票得到。 3.第三个股票问题（两次买卖） 相比第二问，加了条只能买卖两次的设定，这使得我们必须要记录下数次交易中的两次最大值，所以我们必须新加状态进行购买次数的限制 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465一天结束时，可能有持股、可能未持股、可能卖出过1次、可能卖出过2次、也可能未卖出过所以定义状态转移数组dp[天数][当前是否持股][卖出的次数]具体一天结束时的6种状态：未持股，未卖出过股票：说明从未进行过买卖，利润为0dp[i][0][0]=0未持股，卖出过1次股票：可能是今天卖出，也可能是之前卖的（昨天也未持股且卖出过）dp[i][0][1]=max(dp[i-1][1][0]+prices[i],dp[i-1][0][1])未持股，卖出过2次股票:可能是今天卖出，也可能是之前卖的（昨天也未持股且卖出过）dp[i][0][2]=max(dp[i-1][1][1]+prices[i],dp[i-1][0][2])持股，未卖出过股票：可能是今天买的，也可能是之前买的（昨天也持股）dp[i][1][0]=max(dp[i-1][0][0]-prices[i],dp[i-1][1][0])持股，卖出过1次股票：可能是今天买的，也可能是之前买的（昨天也持股）dp[i][1][1]=max(dp[i-1][0][1]-prices[i],dp[i-1][1][1])持股，卖出过2次股票：最多交易2次，这种情况不存在dp[i][1][2]=float(&#x27;-inf&#x27;)class Solution &#123; public int maxProfitDP(int[] prices) &#123; if (prices == null || prices.length &lt;= 1) return 0; int[][][] dp = new int[prices.length][2][3]; int MIN_VALUE = Integer.MIN_VALUE / 2;//因为最小值再减去1就是最大值Integer.MIN_VALUE-1=Integer.MAX_VALUE //初始化 dp[0][0][0] = 0;//第一天休息 dp[0][0][1] = dp[0][1][1] = MIN_VALUE;//不可能 dp[0][0][2] = dp[0][1][2] = MIN_VALUE;//不可能 dp[0][1][0] = -prices[0];//买股票 for (int i = 1; i &lt; prices.length; i++) &#123; dp[i][0][0] = 0; dp[i][0][1] = Math.max(dp[i - 1][1][0] + prices[i], dp[i - 1][0][1]); dp[i][0][2] = Math.max(dp[i - 1][1][1] + prices[i], dp[i - 1][0][2]); dp[i][1][0] = Math.max(dp[i - 1][0][0] - prices[i], dp[i - 1][1][0]);//持有昨天的股票，或者昨天没买，今天买了 dp[i][1][1] = Math.max(dp[i - 1][0][1] - prices[i], dp[i - 1][1][1]); dp[i][1][2] = MIN_VALUE; &#125; return Math.max(0, Math.max(dp[prices.length - 1][0][1], dp[prices.length - 1][0][2])); &#125;&#125;大神版class Solution &#123; public int maxProfit(int[] prices) &#123; /** 对于任意一天考虑四个变量: fstBuy: 在该天第一次买入股票可获得的最大收益 fstSell: 在该天第一次卖出股票可获得的最大收益 secBuy: 在该天第二次买入股票可获得的最大收益 secSell: 在该天第二次卖出股票可获得的最大收益 分别对四个变量进行相应的更新, 最后secSell就是最大 收益值(secSell &gt;= fstSell) **/ int fstBuy = Integer.MIN_VALUE, fstSell = 0; int secBuy = Integer.MIN_VALUE, secSell = 0; for(int p : prices) &#123; //状态转移 fstBuy = Math.max(fstBuy, -p); fstSell = Math.max(fstSell, fstBuy + p); secBuy = Math.max(secBuy, fstSell - p); secSell = Math.max(secSell, secBuy + p); &#125; return secSell; &#125;&#125; 本质上也是考察的是对于状态的分析变化，需要我们理清昨天和今天的状态关系 4.第四个股票问题（k次买卖） 其实套路跟第三题一模一样，只是要求我们发现创建dp数组的规律 123456789101112131415161718192021222324252627282930313233class Solution &#123; public int maxProfit(int k, int[] prices) &#123; if (prices == null || prices.length &lt;= 1) return 0; int[][][] dp = new int[prices.length][2][k+1]; int MIN_VALUE = Integer.MIN_VALUE / 2;//因为最小值再减去1就是最大值,防止数据溢出 dp[0][0][0] = 0;//第一天休息 for(int i = 1;i&lt;=k;i++)&#123; dp[0][0][i] = dp[0][1][i] = MIN_VALUE;//不可能 dp[0][0][i] = dp[0][1][i] = MIN_VALUE;//不可能 &#125; dp[0][1][0] = -prices[0];//买股票 for (int i = 1; i &lt; prices.length; i++) &#123; for(int j = 0;j&lt;2;j++)&#123; for(int l =0;l&lt;=k;l++)&#123; if(j==0&amp;&amp;l==0)&#123; dp[i][j][l]=0; &#125;else if(j==1&amp;&amp;l==k)&#123; dp[i][j][l]=MIN_VALUE; &#125;else if(j==0)&#123; dp[i][j][l]=Math.max(dp[i-1][1][l-1]+prices[i],dp[i-1][0][l]); &#125;else if(j==1)&#123; dp[i][j][l]=Math.max(dp[i-1][0][l]-prices[i],dp[i-1][1][l]); &#125; &#125; &#125; &#125; int max = 0; for(int i = 0;i&lt;=k;i++)&#123; max=Math.max(max,dp[prices.length-1][0][i]); &#125; return max; &#125;&#125; 5.第二个股票问题加上冷冻期 题目中定义的“冷冻期”=卖出的那一天的后一天，题目设置冷冻期的意思是，如果昨天卖出了，今天不可买入，那么关键在于哪一天卖出，只要在今天想买入的时候判断一下前一天是不是刚卖出，即可，所以关键的一天其实是卖出的那一天，而不是卖出的后一天 12345678910111213141516171819class Solution &#123; public int maxProfit(int[] prices) &#123; int n=prices.length; if(n&lt;=1) return 0; //0代表不持股且当天没卖出 //1代表持股 //2代表不持股且当天卖出 int [][] dp=new int[n][3]; dp[0][0]=0; dp[0][1]=-prices[0]; dp[0][2]=0; for(int i=1;i&lt;n;i++)&#123;//从[1]...[n-1] dp[i][0]=Math.max(dp[i-1][0],dp[i-1][2]); dp[i][1]=Math.max(dp[i-1][1],dp[i-1][0]-prices[i]); dp[i][2]=dp[i-1][1]+prices[i]; &#125; return Math.max(dp[n-1][0],dp[n-1][2]); &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gwtt.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试， 动态规划","slug":"面试，-动态规划","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95%EF%BC%8C-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}]},{"title":"浅谈垃圾回收器","slug":"浅谈垃圾回收器","date":"2022-08-23T02:57:34.000Z","updated":"2022-08-23T10:37:44.782Z","comments":true,"path":"2022/08/23/浅谈垃圾回收器/","link":"","permalink":"https://gwtt.github.io/2022/08/23/%E6%B5%85%E8%B0%88%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8/","excerpt":"","text":"CMS垃圾回收器如果用Seria和Parallel系列的垃圾收集器：在垃圾回收的时，用户线程都会完全停止，直至垃圾回收结束！ CMS的全称：Concurrent Mark Sweep，翻译过来是并发标记清除 用CMS对比上面的垃圾收集器(Seria和Parallel和parNew)：它最大的不同点就是并发：在GC线程工作的时候，用户线程不会完全停止，用户线程在部分场景下与GC线程一起并发执行。 但是，要理解的是，无论是什么垃圾收集器，Stop The World是一定无法避免的！ CMS只是在部分的GC场景下可以让GC线程与用户线程并发执行 CMS的设计目标是为了避免老年代 GC出现长时间的卡顿（Stop The World） CMS的工作流程CMS可以简单分为5个步骤：初始标记、并发标记、（并发预清理）、重新标记以及并发清除 从步骤就不难看出，CMS主要是实现了标记清除垃圾回收算法 初始标记的过程初始标记会标记GCRoots直接关联的对象以及年轻代指向老年代的对象 初始标记这个过程是会发生Stop The World的。但这个阶段的速度算是很快的，因为没有向下追溯（只标记一层） 并发标记的过程在初始标记完了之后，就进入了并发标记阶段啦 并发标记这个过程是不会停止用户线程的（不会发生 Stop The World）。这一阶段主要是从GC Roots向下追溯，标记所有可达的对象。 并发标记在GC的角度而言，是比较耗费时间的（需要追溯） 并发标记这个阶段完成之后，就到了并发预处理阶段啦 并发预处理这个阶段主要想干的事情：希望能减少下一个阶段重新标记所消耗的时间 因为下一个阶段重新标记是需要Stop The World的 并发标记这个阶段由于用户线程是没有被挂起的，所以对象是有可能发生变化的 可能有些对象，从新生代晋升到了老年代。可能有些对象，直接分配到了老年代（大对象）。可能老年代或者新生代的对象引用发生了变化… 跨代引用的问题针对老年代的对象，其实还是可以借助类card table的存储（将老年代对象发生变化所对应的卡页标记为dirty） 所以并发预处理这个阶段会扫描可能由于并发标记时导致老年代发生变化的对象，会再扫描一遍标记为dirty的卡页 对于新生代的对象，我们还是得遍历新生代来看看在并发标记过程中有没有对象引用了老年代.. 不过JVM里给我们提供了很多参数，有可能在这个过程中会触发一次 minor GC（触发了minor GC 是意味着就可以更少地遍历新生代的对象） 重新标记的过程并发预处理这个阶段阶段结束后，就到了重新标记阶段 重新标记阶段会Stop The World，这个过程的停顿时间其实很大程度上取决于上面并发预处理阶段（可以发现，这是一个追赶的过程：一边在标记存活对象，一边用户线程在执行产生垃圾） 并发清除的过程最后就是并发清除阶段，不会Stop The World 一边用户线程在执行，一边GC线程在回收不可达的对象 这个过程，还是有可能用户线程在不断产生垃圾，但只能留到下一次GC 进行处理了，产生的这些垃圾被叫做“浮动垃圾” 完了以后会重置 CMS 算法相关的内部数据，为下一次 GC 循环做准备 CMS的缺点 空间需要预留：CMS垃圾收集器可以一边回收垃圾，一边处理用户线程，那需要在这个过程中保证有充足的内存空间供用户使用。如果CMS运行过程中预留的空间不够用了，会报错（Concurrent Mode Failure），这时会启动 Serial Old垃圾收集器进行老年代的垃圾回收，会导致停顿的时间很长。显然啦，空间预留多少，肯定是有参数配置的。 浮动垃圾：由于垃圾回收和用户线程是同时进行的，在进行标记或者清除的同时，用户的线程还会去改变对象的引用，使得原来某些对象不是垃圾，但是当 CMS 进行清理的时候变成了垃圾，CMS 收集器无法收集，只能等到下一次 GC。CMS 收集器无法处理浮动垃圾（Floating Garbage），可能出现 “Concurrent Mode Failure” 失败而导致另一次 Full GC 的产生。如果在应用中老年代增长不是太快，可以适当调高参数 - XX:CMSInitiatingOccupancyFraction 的值来提高触发百分比，以便降低内存回收次数从而获取更好的性能。 内存碎片问题：CMS本质上是实现了标记清除算法的收集器（从过程就可以看得出），这会意味着会产生内存碎片。由于碎片太多，又可能会导致内存空间不足所触发full GC，CMS一般会在触发full GC这个过程对碎片进行整理。整理涉及到移动/标记，那这个过程肯定会Stop The World的，如果内存足够大（意味着可能装载的对象足够多），那这个过程卡顿也是需要一定的时间的。 补充面试题:1.CMS的过程？ 初始标记、并发标记、（并发预清理）、重新标记以及并发清除 2,怎么标记垃圾的？ 使用三色标记法 3.什么是三色标记法 三色标记法，是把内存中的对象，标记为3种颜色，分布是：黑、灰、白。(上文图中的红色仅供参考) 黑：表示该对象已经扫描到，并且它可触达的对象也已经扫描到； 灰：表示该对象已经扫描到，但是它能触发的对象至少还有一个没有扫描到； 白：表示该节点没有被扫描到； 4.CMS和G1的区别 G1和CMS都分为4个阶段,前三个阶段基本相同都为初始标记,并发标记,再次标记,区别在于最后清除阶段CMS是并发的,G1不是并发的,因此CMS最终会产生浮动垃圾,只能等待下次gc才能清除 G1可以管理整个堆,而CMS只能作用于老年代,并且CMS在老年代使用的是标记清除算法,会产生内存碎片,而G1使用标记整理算法,不会产生内存碎片 G1相比于CMS最大的区别是G1将内存划分为大小相等的Region,可以选择垃圾对象多的Region而不是整个堆从而减少STW,同时使用Region可以更精确控制收集,我们可以手动明确一个垃圾回收的最大时间 5.CMS什么时候会STW？为什么要STW（咋瓦鲁多）? 初始标记和重新标记的时候 因为初始标记标记的是GC Root，而GC Root容易变动，比如栈帧中的本地变量表。所以需要STW。 因为在重新标记之前是并发标记，在并发标记的期间会出现漏标和多标的对象，所以为了修正这部分对象，需要在重新标记期间STW。","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Jvm","slug":"Jvm","permalink":"https://gwtt.github.io/tags/Jvm/"}]},{"title":"十大排序算法","slug":"十大排序算法","date":"2022-08-20T06:28:34.000Z","updated":"2022-09-23T01:33:20.739Z","comments":true,"path":"2022/08/20/十大排序算法/","link":"","permalink":"https://gwtt.github.io/2022/08/20/%E5%8D%81%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","excerpt":"","text":"下图展示了十大排序的名字和大致用法 说到排序，首先要用到就是交换数字，接下来谈谈三次交换方法 1234567891011121314151617181920212223// 方法一: 利用临时数tmpprivate void swap(int[] arr, int i, int j) &#123; int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp;&#125;// 方法二: 利用加减运算private void swapCal(int[] arr, int i, int j) &#123; if(i == j) return; // 若无法保证swapCal被调用时满足 i != j，则需有此句，否则i == j时此数将变为0 arr[i] = arr[i] + arr[j]; // a = a + b arr[j] = arr[i] - arr[j]; // b = a - b arr[i] = arr[i] - arr[j]; // a = a - b&#125;// 方法三: 利用异或运算private void swapXOR(int[] arr, int i, int j) &#123; if(i == j) return; // 若无法保证swapXOR被调用时满足 i != j，则需有此句，否则i == j时此数将变为0 arr[i] = arr[i] ^ arr[j]; // a = a ^ b，也可写成 arr[i] ^= arr[j]; arr[j] = arr[i] ^ arr[j]; // b = (a ^ b) ^ b = a ^ (b ^ b) = a ^ 0 = a， 也可写成 arr[j] ^= arr[i]; arr[i] = arr[i] ^ arr[j]; // a = (a ^ b) ^ a = (a ^ a) ^ b = 0 ^ b = b， 也可写成 arr[i] ^= arr[j];&#125;方法一: 利用一个临时数 tmp 来交换 arr[i] ，arr[j] 。方法二: 利用 arr[i] 和和 arr[j] 的加减运算避免临时数 tmp 的开销，但由于涉及到加减法可能导致数字 「提前溢出」 。方法三: 利用位运算中的 异或 运算，能够避免 tmp 的开销且不会导致数字溢出。 冒泡排序 从第一位开始向后依次比较，如果前者大则交换（实际根据大小方向），循环arr.length-1次 最笨的形式1234567891011public int[] bubbleSort(int[] arr) &#123; for (int i = 0; i &lt; arr.length - 1; i++) &#123; for (int j = 1; j &lt; arr.length - i; j++) &#123; if (arr[j - 1] &gt; arr[j]) &#123; swap(arr, j - 1, j); &#125; &#125; &#125; return arr;&#125; 优化提前结束优化当某一轮比较均未发生交换，说明排序已完成，可设置一个布尔值记录一轮排序是否有发生交换，若无则提前退出循环结束程序。 12345678910111213141516171819public int[] bubbleSort(int[] arr) &#123; if (arr.length &lt; 2) return arr; // n - 1轮次执行，当前 n - 1 个元素排好后，最后一个元素无需执行，故i &lt; arr.length - 1 for (int i = 0; i &lt; arr.length - 1; i++) &#123; // 本轮执行是否有交换的标志，若无则false，若有则true boolean swapped = false; // 每轮循环，通过依次向右比较两个数，将本轮循环中最大的数放到最右 for (int j = 1; j &lt; arr.length - i; j++) &#123; // 若左大于右则交换，并将swapped置为true if (arr[j - 1] &gt; arr[j]) &#123; swap(arr, j - 1, j); swapped = true; &#125; &#125; // 若无交换，表示当前数组已完全排序，退出大循环 if (!swapped) break; &#125; return arr;&#125; 冒泡界优化记录前一轮交换的最终位置，说明该位置之后的元素为已排序状态，下一轮的交换只需执行到该处。 1234567891011121314151617181920212223public int[] bubbleSort(int[] arr) &#123; if (arr.length &lt; 2) return arr; boolean swapped = true; int lastSwappedIdx = arr.length - 1 ; int swappedIdx = -1; // lastSwappedIdx表示前一轮交换的最终位置，即下标为lastSwappedIdx是未排序部分中的最后一个数的下标， // 因此for中的界是i &lt; lastSwappedIdx而不需要写成i &lt;= lastSwappedIdx while (swapped) &#123; // 当swapped = false时，排序完成 // 本轮执行是否有交换的标志，若无则true，若有则false swapped = false; // 每轮循环，通过依次向右比较两个数，将本轮循环中最大的数放到最右 for (int i = 0; i &lt; lastSwappedIdx; i++) &#123; // 若左大于右则交换，并将swapped置为true if (arr[i] &gt; arr[i + 1]) &#123; swap(arr, i, i + 1); swapped = true; swappedIdx = i; &#125; &#125; lastSwappedIdx = swappedIdx; &#125; return arr;&#125; 平均时间复杂度:O(n^2) 空间复杂度:O(n) 稳定 选择排序 每一轮循环选一个最小（或者最大）的数放到第i位，循环arr.length-1次 单元选择排序123456789101112131415public int[] selectSort(int[] arr) &#123; if (arr.length &lt; 2) return arr; // n - 1 轮次执行,当前 n - 1 个元素排好后，最后一个元素无需执行，故 i &lt; arr.length - 1 for (int i = 0; i &lt; arr.length - 1; i++) &#123; int minIdx = i; // 找到本轮执行中最小的元素，将最小值下标赋值给min for (int j = i + 1; j &lt; arr.length; j++) &#123; if (arr[j] &lt; arr[minIdx]) minIdx = j; &#125; // 若本轮第一个数字不是最小值，则交换位置 if (minIdx != i) swap(arr, i, minIdx); &#125; return arr;&#125; 双元选择排序1234567891011121314151617181920212223public int[] selectSortDouble(int[] arr) &#123; if (arr.length &lt; 2) return arr; int n = arr.length; // 每轮确定两个数字，因此界也会动态变化 for (int i = 0; i &lt; n - 1 - i; i++) &#123; int minIdx = i, maxIdx = i; // 找到本轮执行中最小和最大的元素 for (int j = i + 1; j &lt; n - i; j++) &#123; if (arr[j] &lt; arr[minIdx]) minIdx = j; if(arr[j] &gt; arr[maxIdx]) maxIdx = j; &#125; // 若本轮最大值等于最小值，说明未排序部分所有元素相等，无需再排序 if(minIdx == maxIdx) break; // 若本轮第一个数字不是最小值，则交换位置（将最小值与本轮第一个数字交换位置） if (minIdx != i) swap(arr, i, minIdx); // 在交换i和minIdx时，有可能出现i即maxIdx的情况，此时需要修改maxIdx为minIdx if(maxIdx == i) maxIdx = minIdx; // 若本轮最后一个数字不是最大值，则交换位置（将最大值与本轮最后一个数字交换位置） if (maxIdx != n - 1 - i) swap(arr, n - 1 - i, maxIdx); &#125; return arr;&#125; 平均时间:O(n^2) 空间:O(1) 不稳定 插入排序 对于待排序数组，从第2个元素开始(称作插入对象元素)，比较它与之前的元素(称作比较对象元素)，当插入对象元素小于比较对象元素时，继续往前比较，直到不小于(≥)比较对象，此时将插入对象元素插入到该次比较对象元素之后。重复这个插入过程直到最后一个元素作为插入对象元素完成插入操作。 简单插入排序1234567891011121314public int[] insertSort(int[] arr) &#123; if (arr.length &lt; 2) return arr; for (int i = 1; i &lt; arr.length; i++) &#123; // N-1轮次执行 int target = arr[i], j = i - 1; for (; j &gt;= 0; j--) &#123; if(target &lt; arr[j]) arr[j + 1] = arr[j]; else break; &#125; arr[j + 1] = target; // 若发生移动，此时的插入对象数字≥j位置的数字，故插入位置为j + 1，若未移动也成立，无需判断 // if(j != i - 1) arr[j + 1] = target; // 也可以用这种写法，表示发生移动才插入，否则不必插入(赋值)，但不判断效率更高 &#125; return arr;&#125; 折半插入优化(利用二分来减少中间比较次数)注意到插入排序的每一轮向前插入都使得该元素在完成插入后，从第一个元素到该元素是排序状态（指这部分的相对排序状态，在它们中间后续可能还会插入其他数字），利用这一点，对一个新的插入对象向前执行折半插入，能够显著减少比较的次数。另一种优化是增量递减插入排序，也叫希尔排序，将在希尔排序章节中介绍。 折半插入的关键在于找到插入位置，折半过程代码如下。这实际上是二分查找「模版一」中的「小于等于」情形。 1234567891011121314151617181920212223public int[] insertSortBinary(int[] arr) &#123; if (arr.length &lt; 2) return arr; // n - 1 轮次执行 for (int i = 1; i &lt; arr.length; i++) &#123; // 若当前插入对象大于等于前一个对象，无需插入 if (arr[i - 1] &lt;= arr[i]) continue; int target = arr[i]; // 折半查找 (二分查找「模版一」) int low = 0, high = i - 1; // while结束后，target要插入的位置为low或high + 1 (low = high + 1) while (low &lt;= high) &#123; int center = low + (high - low) / 2; if (arr[center] &lt;= target) low = center + 1; else high = center - 1; &#125; for (int j = i; j &gt; low; j--) &#123; // 移动 arr[j] = arr[j - 1]; &#125; arr[low] = target; // 插入 &#125; return arr;&#125; 平均时间:O(n^2) 空间:O(1) 不稳定 希尔排序 希尔排序是基于插入排序的以下两点性质而提出改进方法的： 插入排序在对几乎已经排好序的数据操作时，效率高，即可以达到线性排序的效率； 但插入排序一般来说是低效的，因为插入排序每次只能将数据移动一位； 希尔排序的基本思想是：先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录”基本有序”时，再对全体记录进行依次直接插入排序。 Shell增量1234567891011121314151617181920// 希尔排序：采用Shell增量 N / 2^kpublic int[] shellSortShell(int[] arr) &#123; if (arr.length &lt; 2) return arr; int n = arr.length; for (int gap = n / 2; gap &gt; 0; gap /= 2) &#123; // gap 初始为 n/2，缩小gap直到1 for(int start = 0; start &lt; gap; start++) &#123; // 步长增量是gap，当前增量下需要对gap组序列进行简单插入排序 for (int i = start + gap; i &lt; n; i += gap) &#123; // 此for及下一个for对当前增量序列执行简单插入排序 int target = arr[i], j = i - gap; for (; j &gt;= 0; j -= gap) &#123; if (target &lt; arr[j]) &#123; arr[j + gap] = arr[j]; &#125; else break; &#125; if (j != i - gap) arr[j + gap] = target; &#125; &#125; &#125; return arr;&#125; Hibbard增量123456789101112131415161718192021// 希尔排序： 采用Hibbard增量 &#123;1, 3, 7, 15,...&#125;public int[] shellSortHibbard(int[] arr) &#123; if (arr.length &lt; 2) return arr; int n = arr.length, gap = 1; while (gap &lt; n / 2) gap = gap * 2 + 1; // 初始化gap (Hibbard增量序列) for (; gap &gt; 0; gap /= 2) &#123; // 缩小gap直到1 for(int start = 0; start &lt; gap; start++) &#123; // 步长增量是gap，当前增量下需要对gap组序列进行简单插入排序 for (int i = start + gap; i &lt; arr.length; i += gap) &#123; // 此for及下一个for对当前增量序列执行简单插入排序 int target = arr[i], j = i - gap; for (; j &gt;= 0; j -= gap) &#123; if (target &lt; arr[j]) &#123; arr[j + gap] = arr[j]; &#125; else break; &#125; if (j != i - gap) arr[j + gap] = target; &#125; &#125; &#125; return arr;&#125; Knuth增量1234567891011121314151617181920// 希尔排序： 采用Knuth增量 &#123;1, 4, 13, 40,...&#125;public int[] shellSortKnuth(int[] arr) &#123; if (arr.length &lt; 2) return arr; int n = arr.length, gap = 1; while (gap &lt; n / 3) gap = gap * 3 + 1; // 初始化gap (Knuth增量序列) for (; gap &gt; 0; gap /= 3) &#123; // 缩小gap直到1 for(int start = 0; start &lt; gap; start++) &#123; // 步长增量是gap，当前增量下需要对gap组序列进行简单插入排序 for (int i = start + gap; i &lt; arr.length; i += gap) &#123; // 此for及下一个for对当前增量序列执行简单插入排序 int target = arr[i], j = i - gap; for (; j &gt;= 0; j -= gap) &#123; if (target &lt; arr[j]) &#123; arr[j + gap] = arr[j]; &#125; else break; &#125; if (j != i - gap) arr[j + gap] = target; &#125; &#125; &#125; return arr;&#125; 简单插入排序和希尔排序比较 归并排序 归并排序是 分治思想 的应用，即将原待排数组 递归或迭代地 分为左右两半，直到数组长度为1，然后对左右数组进行合并(merge)，在合并中完成排序。 自顶向下(top-down)：从输入数组出发，不断二分该数组，直到数组长度为1，再执行合并。适合用 递归 实现。 自底向上(bottom-up)：从输入数组的单个元素出发，一一合并，二二合并，四四合并直到数组有序。适合用 迭代 实现。 自顶向下非原地归并123456789101112131415161718192021222324252627282930313233343536373839404142434445public int[] mergeSort(int[] arr) &#123; if (arr.length &lt; 2) return arr; int[] tmpArr = new int[arr.length]; mergeSort(arr, tmpArr, 0, arr.length - 1); return arr;&#125;private void mergeSort(int[] arr, int[] tmpArr, int left, int right) &#123; if(left &lt; right) &#123; int center = left + (right - left) / 2; mergeSort(arr, tmpArr, left, center); mergeSort(arr, tmpArr, center + 1, right); merge(arr, tmpArr, left, center, right); &#125;&#125;// 非原地合并方法private void merge(int[] arr, int[] tmpArr, int leftPos, int leftEnd, int rightEnd) &#123; int rightPos = leftEnd + 1; int startIdx = leftPos; int tmpPos = leftPos; while (leftPos &lt;= leftEnd &amp;&amp; rightPos &lt;= rightEnd) &#123; if (arr[leftPos] &lt;= arr[rightPos]) &#123; tmpArr[tmpPos++] = arr[leftPos++]; &#125; else &#123; tmpArr[tmpPos++] = arr[rightPos++]; &#125; &#125; // 比较完成后若左数组还有剩余，则将其添加到tmpArr剩余空间 while (leftPos &lt;= leftEnd) &#123; tmpArr[tmpPos++] = arr[leftPos++]; &#125; // 比较完成后若右数组还有剩余，则将其添加到tmpArr剩余空间 while (rightPos &lt;= rightEnd) &#123; tmpArr[tmpPos++] = arr[rightPos++]; &#125; // 容易遗漏的步骤，将tmpArr拷回arr中 // 从小区间排序到大区间排序，大区间包含原来的小区间，需要从arr再对应比较排序到tmpArr中， // 所以arr也需要动态更新为排序状态，即随时将tmpArr拷回到arr中 for(int i = startIdx; i &lt;= rightEnd; i++) &#123; arr[i] = tmpArr[i]; &#125;&#125; 自顶向下原地归并12345678910111213141516171819202122232425262728293031323334353637383940public int[] mergeSort(int[] arr) &#123; if (arr.length &lt; 2) return arr; mergeSort(arr, 0, arr.length - 1); return arr;&#125;private void mergeSort(int[] arr, int left, int right) &#123; if(left &lt; right) &#123; int center = left + (right - left) / 2; mergeSort(arr, left, center); mergeSort(arr, center + 1, right); merge(arr, left, center, right); &#125;&#125;// 原地归并（手摇算法）private void merge(int[] arr, int leftPos, int leftEnd, int rightEnd) &#123; int i = leftPos, j = leftEnd + 1; // #1 while(i &lt; j &amp;&amp; j &lt;= rightEnd) &#123; while(i &lt; j &amp;&amp; arr[i] &lt;= arr[j]) i++; // #2 int index = j; // #3 while(j &lt;= rightEnd &amp;&amp; arr[j] &lt; arr[i]) j++; // #4 注意是 arr[j] &lt; arr[i]，即找到j使得arr[j] 为第一个大于等于 arr[i]值 exchange(arr, i, index - 1, j - 1); // #5 &#125;&#125;// 三次翻转实现交换private void exchange(int[] arr, int left, int leftEnd, int rightEnd) &#123; reverse(arr, left, leftEnd); reverse(arr, leftEnd + 1, rightEnd); reverse(arr, left, rightEnd);&#125;private void reverse(int[] arr, int start, int end) &#123; while(start &lt; end) &#123; swap(arr, start, end); start++; end--; &#125;&#125; 自底向上非原地归并1234567891011121314public int[] mergeSortBU(int[] arr) &#123; if (arr.length &lt; 2) return arr; int[] tmpArr = new int[arr.length]; // 间隔，注意不能写成gap &lt; arr.length / 2 + 1，此种写法只适用于元素个数为2的n次幂时 for(int gap = 1; gap &lt; arr.length; gap *= 2) &#123; // 基本分区合并(随着间隔的成倍增长，一一合并，二二合并，四四合并...) for(int left = 0; left &lt; arr.length - gap; left += 2 * gap) &#123; // 调用非原地合并方法。leftEnd = left+gap-1; rightEnd = left+2*gap-1; merge(arr, tmpArr, left, left + gap - 1, Math.min(left + 2 * gap - 1, arr.length - 1)); &#125; &#125; return arr;&#125; 自底向上原地归并123456789101112public int[] mergeSortBUInPlace(int[] arr) &#123; if (arr.length &lt; 2) return arr; // 间隔，注意不能写成gap &lt; arr.length / 2 + 1，此种写法只适用于元素个数为2的n次幂时 for(int gap = 1; gap &lt; arr.length; gap *= 2) &#123; // 基本分区合并(随着间隔的成倍增长，一一合并，二二合并，四四合并...) for(int left = 0; left &lt; arr.length - gap; left += 2 * gap) &#123; // 调用原地合并方法。leftEnd = left+gap-1; rightEnd = left+2*gap-1; merge(arr, left, left + gap - 1, Math.min(left + 2 * gap - 1, arr.length - 1)); &#125; &#125; return arr;&#125; 平均时间复杂度:O(nlogn) 空间复杂度:O(n) 稳定 快速排序 与归并排序一样，快速排序也是一种利用 分治思想 的排序方法，确定 主轴及分区 是快速排序的核心操作。首先在数组中确定一个主轴元素(下标记为 pivot)，然后将数组分为两部分，小于主轴的放在（确定最终位置的）主轴左侧，大于等于主轴的放在主轴右侧。递归地对主轴左右两侧数组执行这个过程，每次递归都传入待排序数组 arr 和本次要处理的部分的左右界，只处理这个范围内的序列。当所有递归都到达基准情形时，排序完成。因为是原地交换，递归过程中 arr总是在动态排序，递归过程无需返回，为尾递归形式。 递归快排12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879// 三数取中快排public int[] quickSortMedian3(int[] arr) &#123; if (arr.length &lt; 2) return arr; quickSortMedian3(arr, 0, arr.length - 1); // 后两个参数是下标值 return arr;&#125;private void quickSortMedian3(int[] arr, int left, int right) &#123; if (left &lt; right) &#123; // 执行median3将左，中，右三数中值放到left位置上 median3(arr, left, right); int pivot = partition(arr, left, right); quickSortMedian3(arr, left, pivot - 1); quickSortMedian3(arr, pivot + 1, right); &#125;&#125;// 将left, center, right下标三个数中，大小居中者放到left下标处private void median3(int[]arr, int l, int r) &#123; int c = l + (r - l) / 2; if (arr[l] &gt; arr[c]) swap(arr, l, c); // 左中，大者居中 if (arr[c] &gt; arr[r]) swap(arr, c, r); // 中右，大者居右，此时最大者居右 if (arr[c] &gt; arr[l]) swap(arr, l, c); // 左中，大者居左，此时中者居左&#125;// 随机主轴快排public int[] quickSortRandom(int[] arr) &#123; if (arr.length &lt; 2) &#123; return arr; &#125; quickSortRandom(arr, 0, arr.length - 1); return arr;&#125;private void quickSortRandom(int[] arr, int left, int right) &#123; if (left &lt; right) &#123; // 取区间内随机下标，注意Random().nextInt(int x)方法的使用（含0不含x） int randomIndex = new Random().nextInt(right - left) + left + 1; // 在[left + 1, right]范围内的随机值 // 交换随机取得的下标元素与当前起始元素 swap(arr, left, randomIndex); // arr[left]与它之后的某个数交换 int pivot = partition(arr, left, right); quickSortRandom(arr, left, pivot - 1); quickSortRandom(arr, pivot + 1, right); &#125;&#125;// 朴素快排(首位为主轴)public int[] quickSortSimple(int[] arr) &#123; if (arr.length &lt; 2) return arr; quickSortSimple(arr, 0, arr.length - 1); // 后两个参数是下标值 return arr;&#125;private void quickSortSimple(int[] arr, int left, int right) &#123; // 若left == right，表示此时arr只有一个元素，即为基准情形，完成递归(准确说是完成递进) // (尾递归，“回归”过程中不做任何事情） if (left &lt; right) &#123; int pivot = partition(arr, left, right); quickSortSimple(arr, left, pivot - 1); quickSortSimple(arr, pivot + 1, right); &#125;&#125;// partition方法private int partition(int[] arr, int left, int right) &#123; int pivot = left, index = pivot + 1; // 注意此时right是坐标，要执行到最后一个元素，所以是&lt;= for (int i = index; i &lt;= right; i++) &#123; if (arr[i] &lt; arr[pivot]) &#123; swap(arr, index, i); index++; &#125; &#125; // 最后一个小于主轴元素的元素下标是index - 1 swap(arr, pivot, index - 1); return index - 1;&#125; 非递归快排 (迭代快排)12345678910111213141516171819202122232425262728293031public int[] quickSortStack(int[] arr) &#123; // 用于保存区间左右边界的栈，按right到left的顺序将初始区间界入栈 Deque&lt;Integer&gt; stack = new ArrayDeque&lt;&gt;(); stack.push(arr.length - 1); stack.push(0); // 判断栈是否空，不空则弹出一对left，right界 while(!stack.isEmpty()) &#123; int left = stack.pop(), right = stack.pop(); if(left &lt; right) &#123; // 执行partition的前提是left小于right // 对[left, right]区间执行partition方法，得到pivot // 加入后续两行实现随机轴快排 // int randomIndex = new Random().nextInt(right - left) + left + 1; // 在[left + 1, right]范围内的随机值 // swap(arr, left, randomIndex); // arr[left]与它之后的某个数交换 // 加入下行实现三数取中快排 median3(arr, left, right); int pivot = partition(arr, left, right); // 当前pivot的左区间存在则将该区间right，left界入栈 if(pivot &gt; left) &#123; stack.push(pivot - 1); stack.push(left); &#125; // 当前pivot的右区间存在则将该区间right，left界入栈 if(right &gt; pivot) &#123; stack.push(right); stack.push(pivot + 1); &#125; &#125; &#125; return arr;&#125; 平均时间复杂度:O(nlogn) 空间复杂度:O(logn) 不稳定 堆排序 将输入数组建立为一个 大顶堆，之后反复取出堆顶并对剩余元素重建大顶堆，将依次取出的堆顶逆序排列，即可将原数组从小到大排列完成排序。 1234567891011121314151617181920212223242526272829303132333435363738public int[] heapSort(int[] arr) &#123; if (arr.length &lt; 2) return arr; heapify(arr, arr.length - 1); // 构建大顶堆 for (int i = arr.length - 1; i &gt; 0; i--) &#123; // i &gt; 0即可，无需写成i &gt;= 0，当n - 1个元素排序时，最后一个元素也已排序 swap(arr, 0, i); // 交换堆顶和当前未排序部分最后一个元素 // 此时除当前堆顶元素外都是保持堆序的，只需要对该堆顶调用一次下滤操作 siftDown(arr, 0, i - 1); // i - 1是未排序部分最后一个元素下标，确保下滤不会超过此范围 &#125; return arr;&#125;private void heapify(int[] arr, int endIdx) &#123; for (int hole = (endIdx - 1) / 2; hole &gt;= 0; hole--) &#123; // (endIdx - 1) / 2伪最后一个非叶子节点下标 siftDown(arr, hole, endIdx); &#125;&#125;private void siftDown(int[] arr, int hole, int endIdx) &#123; int target = arr[hole]; // target是要下滤的节点 int child = hole * 2 + 1; while(child &lt;= endIdx) &#123; // 满足第一个条件child &lt; endIdx表示hole有右孩子，不满足则hole无右孩子，跳过 // 第二个条件arr[child + 1] &gt; arr[child]只在第一个条件成立前提下进行判断（因此不必担心arr[child + 1]越界）， // 若满足，表示hole有右孩子且右孩子更大，令child为右孩子下标。 // 因此此if过后使得child是hole的孩子中较大的那个 if (child &lt; endIdx &amp;&amp; arr[child + 1] &gt; arr[child]) &#123; child++; &#125; // 若child大于target，则child上移到当前hole，hole下滤到child位置 if (arr[child] &gt; target) &#123; arr[hole] = arr[child]; hole = child; child = hole * 2 + 1; // 当然也可以写成child = child * 2 + 1 &#125; else break; // 若无需交换hole与child，说明hole已经满足堆序(无需/无法再下滤)，退出while &#125; arr[hole] = target; // 将target填入hole中&#125; 平均时间复杂度:O(nlogn) 空间复杂度:O(1) 不稳定 计数排序 计数排序的特征当输入的元素是 n 个 0 到 k 之间的整数时，它的运行时间是 Θ(n + k)。计数排序不是比较排序，排序的速度快于任何比较排序算法。 由于用来计数的数组C的长度取决于待排序数组中数据的范围（等于待排序数组的最大值与最小值的差加上1），这使得计数排序对于数据范围很大的数组，需要大量时间和内存。例如：计数排序是用来排序0到100之间的数字的最好的算法，但是它不适合按字母顺序排序人名。但是，计数排序可以用在基数排序中的算法来排序数据范围很大的数组。 通俗地理解，例如有 10 个年龄不同的人，统计出有 8 个人的年龄比 A 小，那 A 的年龄就排在第 9 位,用这个方法可以得到其他每个人的位置,也就排好了序。当然，年龄有重复时需要特殊处理（保证稳定性），这就是为什么最后要反向填充目标数组，以及将每个数字的统计减去 1 的原因。 算法的步骤如下： （1）找出待排序的数组中最大和最小的元素 （2）统计数组中每个值为i的元素出现的次数，存入数组C的第i项 （3）对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加） （4）反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1 不稳定计数排序123456789101112131415161718192021public int[] countSortUnstable(int[] arr) &#123; if (arr.length &lt; 2) return arr; int min = arr[0], max = arr[0]; for (int i = 1; i &lt; arr.length; i++) &#123; min = Math.min(min, arr[i]); max = Math.max(max, arr[i]); &#125; int[] countArr = new int[max - min + 1]; for (int i = 0; i &lt; arr.length; i++) &#123; countArr[arr[i] - min]++; &#125; int index = 0; for (int i = 0; i &lt; countArr.length; i++) &#123; // 遍历countArr for (int j = 0; j &lt; countArr[i]; j++) &#123; // countArr[i]可能有多个相同数字 arr[index] = i + min; // 复用了原输入数组arr index++; &#125; &#125; return arr; &#125; 稳定计数排序12345678910111213141516171819202122public int[] countSort(int[] arr) &#123; if (arr.length &lt; 2) return arr; int n = arr.length, min = arr[0], max = arr[0]; for (int i = 1; i &lt; n; i++) &#123; min = Math.min(min, arr[i]); max = Math.max(max, arr[i]); &#125; int[] countArr = new int[max - min + 1]; // arr最多有max-min+1种数字 for (int i = 0; i &lt; n; i++) &#123; countArr[arr[i] - min]++; // arr[i]的值出现一次，则countArr[arr[i]-min]加1 &#125; for (int i = 1; i &lt; countArr.length; i++) &#123; // 变形 countArr[i] += countArr[i - 1]; &#125; int[] sortedArr = new int[n]; // 根据sortedArr, nums, countArr三者关系完成sortedArr的输出 for (int i = n - 1; i &gt;= 0; i--) &#123; sortedArr[countArr[arr[i] - min] - 1] = arr[i]; countArr[arr[i] - min]--; &#125; return sortedArr;&#125; 平均时间复杂度:O(n+k) 空间复杂度:O(n+k) 稳定 基数排序 非比较排序，「基」指的是数的位，例如十进制数 123，共有百十个位，共 3 个位。基数排序 按数字的位进行循环，每一轮操作都是对当前位（基数）的计数排序，使得输出到 arr 后所有数字在截止到当前位上（即去掉未考察的位后）是排序状态，考察完最大位后完成排序。具体过程如下： 遍历待排序数组 arr ，找到最大值，计算其位数，例如 arr 中最大数为 123 ，则 maxDigitLen = 3 。 数组的数字为 n 进制，就创建大小为 n 的计数数组 countArr ，也可以称为 n 个桶。 开始「位」的 for 循环，循环次数等于 maxDigitLen ，每一轮对 当前所有数字的当前位 执行一次 计数排序。 每次计数排序结束后将结果写回 arr 。 for循环结束后返回排序结果 arr。 以计数排序为基础1234567891011121314151617181920212223242526272829303132333435363738394041public int[] radixSort(int[] arr) &#123; if (arr.length &lt; 2) return arr; int max = Math.abs(arr[0]); // 找到arr中绝对值最大者 for (int i = 1; i &lt; arr.length; i++) &#123; max = Math.max(max, Math.abs(arr[i])); &#125; int maxDigitLen = 0, base = 10; // 最大位数 &amp; 基（几进制就是几） while (max != 0) &#123; maxDigitLen++; max /= base; &#125; // 在接下来的for中，每一轮都对当前位(基数)执行一次计数排序 int[] sortedArr = new int[arr.length]; for (int i = 0; i &lt; maxDigitLen; i++) &#123; int[] countArr = new int[19]; // 处理负数优化 // 根据每一个数字当前位的数字，累计相应位置的计数 for (int j = 0; j &lt; arr.length; j++) &#123; // 此步处理要注意，当base大于10时，例如base=100时，1234%100=34 // 还需要再除以(base/10)，得到的3，然后再+9（考虑负数）才是本次的bucketIdx int bucketIdx = (arr[j] % base) / (base / 10) + 9; countArr[bucketIdx]++; &#125; // countArr变形，得到每个下标所代表的arr中的数的当前位在arr中的最大位置（从1开始） for (int j = 1; j &lt; countArr.length; j++) &#123; countArr[j] += countArr[j - 1]; &#125; // 逆序输出保持稳定性 for (int j = arr.length - 1; j &gt;= 0; j--) &#123; int thisBase = (arr[j] % base) / (base / 10) + 9; // countArr[thisBase]得到的从1开始计算的位置，转成下标要-1 sortedArr[countArr[thisBase] - 1] = arr[j]; countArr[thisBase]--; &#125; // 完成当前位的计数排序后将排序结果拷贝回原数组 arr = Arrays.copyOf(sortedArr, sortedArr.length); // base进一位，准备下一轮对下一位的计数排序 base *= 10; &#125; return arr;&#125; 不以计数排序为基础12345678910111213141516171819202122232425262728293031323334353637public int[] radixSort(int[] arr) &#123; if (arr.length &lt; 2) return arr; // 找到arr中绝对值最大者 int max = Math.abs(arr[0]); for (int i = 1; i &lt; arr.length; i++) &#123; max = Math.max(max, Math.abs(arr[i])); &#125; int maxDigitLen = 0, base = 10; // 最大位数 &amp; 基 while (max != 0) &#123; maxDigitLen++; max /= base; &#125; // arr.length + 1的作用是令每个桶的第0位保存该桶的元素个数。 int[][] buckets = new int[19][arr.length + 1]; // 处理负数优化 // 在每一位上将数组中所有具有该位的数字装入对应桶中 for (int i = 0; i &lt; maxDigitLen; i++) &#123; for (int j = 0; j &lt; arr.length; j++) &#123; // 此步处理要注意，当base大于10时，例如base=100时，1234%100=34 // 还需要再除以(base/10)，得到的3才是本次的bucketIndex int bucketIdx = (arr[j] % base) / (base / 10) + 9; // +9使其可以处理负数 int currentBucketQuantity = buckets[bucketIdx][0]; buckets[bucketIdx][currentBucketQuantity + 1] = arr[j]; buckets[bucketIdx][0]++; &#125; // 将当前所有桶的数按桶序，桶内按低到高输出为本轮排序结果 int arrIdx = 0; for (int j = 0; j &lt; buckets.length; j++) &#123; for (int k = 1; k &lt;= buckets[j][0]; k++) &#123; arr[arrIdx++] = buckets[j][k]; &#125; &#125; // 每一轮过后将桶计数归零 for (int[] bucket : buckets) bucket[0] = 0; base *= 10; // 调整base &#125; return arr;&#125; 平均时间复杂度:O(d(n+k)) 空间复杂度:O(n+k) 稳定 桶排序 桶排序将原数组划分到称为 「桶」 的多个区间中，然后对每个桶单独进行排序，之后再按桶序和桶内序输出结果。适合于分布较均匀的数据，具体做法如下。 根据数据规模按照 一定的方法 将待排序数组arr划分为多个区间，每个区间称作一个桶。 每个桶可以是数组，也可以是泛型容器，用于保存arr中落在该桶范围内的数。 对每一个桶都单独排序，需要 以适当的排序 方法支持，例如插入排序，快速排序等。 所有桶完成排序后，按桶序，桶内序依次输出所有元素，得到arr的排序结果。 稳定性：取决于桶内排序方法的稳定性。 1234567891011121314151617181920212223242526272829303132333435public int[] bucketSort(int[] arr) &#123; int min = arr[0], max = arr[0]; for (int i = 0; i &lt; arr.length; i++) &#123; min = Math.min(min, arr[i]); max = Math.max(max, arr[i]); &#125; // 用泛型List存储所有桶，每个桶是一个ArrayList&lt;Integer&gt;，并初始化所有桶。 // arr.length/3表示设置数组大小三分之一数量的桶 List&lt;ArrayList&lt;Integer&gt;&gt; buckets = new ArrayList&lt;&gt;(arr.length / 3); for (int i = 0; i &lt; arr.length; i++) &#123; buckets.add(new ArrayList&lt;&gt;()); &#125; // 遍历arr，根据元素值将所有元素装入对应值区间的桶中 for (int i = 0; i &lt; arr.length; i++) &#123; // (arr[i] - min)/D为arr[i]元素应该装入的桶的下标，间隔D = (max-min)/(arr.length-1) // 虽可写成(arr[i] - min)*(arr.length-1)/(max-min)的形式，但当输入数组取值范围较大且元素较多时 // (arr[i] - min)*(arr.length-1)可能会超过int上限，因此先做除法求出double类型的D // 再做一次除法求出bucketIndex，可以避免计算精度不够高带来的问题 double interval = (double)(max - min) / (double)(arr.length - 1); int bucketIdx = (int) ((arr[i] - min) / interval); buckets.get(bucketIdx).add(arr[i]); &#125; // 桶内排序(调用库函数，从小到大) for (int i = 0; i &lt; buckets.size(); i++) &#123; Collections.sort(buckets.get(i)); &#125; int index = 0; for (ArrayList&lt;Integer&gt; bucket : buckets) &#123; for (int sortedItem : bucket) &#123; arr[index] = sortedItem; // 复用输入数组arr index++; &#125; &#125; return arr;&#125; 平均时间复杂度:O(n) 空间复杂度:O(n) 稳定","categories":[{"name":"算法","slug":"算法","permalink":"https://gwtt.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试， 排序","slug":"面试，-排序","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95%EF%BC%8C-%E6%8E%92%E5%BA%8F/"}]},{"title":"Linux面试(1)","slug":"Linux面试(1)","date":"2022-08-18T14:23:13.000Z","updated":"2022-09-01T04:53:20.002Z","comments":true,"path":"2022/08/18/Linux面试(1)/","link":"","permalink":"https://gwtt.github.io/2022/08/18/Linux%E9%9D%A2%E8%AF%95(1)/","excerpt":"","text":"1.在Linux系统中如何统计某个字符串出现的次数 12345grep -o targetStr filename | wc -l（单个字符串）&quot;targetStr&quot;可以不加引号grep -o targetStr_1\\|targetStr_2\\|targetStr_3…… filename | wc -l&quot;targetStr_1&quot;这些必须加引号但是上面的方法是不准确的，因为wc -l只是显示行数，如果一行字符串多次出现，那么结果不准确 2.Linux查看日志的几种命令 1. tail 查看实时变化的日志(比较吃内存)退出tail命令：Ctrl+c -f 循坏读取 -q 不显示处理信息 -v 显示详细的处理信息 -c &lt;数目&gt; 显示的字节数 -n 显示行数 （相当于nl命令） -pid=PID 与-f合用,表示在进程ID,PID死掉之后结束. -q,-quiet,-slient 从不输出给出文件名的首部 -s, –sleep-interval=S 与-f合用,表示在每次反复的间隔休眠S秒 用法 作用 1. tail -f filename 默认最后10行,相当于增加参数 -n 10 2. tail -n 20 filename 显示filename最后20行 3. tail -n +5 filename 从第5行开始显示文件 2. cat命令cat命令搜索关键字附近的日志 常用用法： 查看log.log前200行 cat log.log | head -n 200 查看test.log倒数200行 cat test.log | tail -n 200 查看test.log中包含http的所有行 cat test.log | grep &quot;http&quot; 查看test.log中包含http的所有行，并显示前后行 cat -n test.log | grep -C 5 “http” (匹配字串那行以及前后5行) cat -n test.log | grep -B 5 “http” (匹配字串那行以及前5行) cat -n test.log | grep -A 5 “http” (匹配字串那行以及后5行) 3. grep命令grep命令，文本搜索命令，可以使用正则表达式搜索文本用法示例： 查看test.log中包含http的所有行(-i忽略大小写） 1grep -i &quot;http&quot; ./test.log 4. sed命令SED 查看某时间段日志 1sed -n &#x27;/2022-06-02 13:10:30/,/2022-06-02 13:10:40/p&#x27; test.log 某时间端日志输出到指定文件中 1sed -n &#x27;/2022-06-02 13:10:30/,/2022-06-02 13:10:40/p&#x27; test.log &gt; test20220602.log 当然还可以使用vi文本编辑命令，less或者more命令进行查看 正向查找： 1/关键字 按n键把光标移动到下一个符合条件的地方 反向查找： 1?关键字 按shift+n 键,把光标移动到下一个符合条件的 日志文件本身还是一个文件，检索命令，编辑命令，查找命令都是可以进行操作的。","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Linux","slug":"Linux","permalink":"https://gwtt.github.io/tags/Linux/"},{"name":"指令","slug":"指令","permalink":"https://gwtt.github.io/tags/%E6%8C%87%E4%BB%A4/"}]},{"title":"限流算法","slug":"限流算法","date":"2022-08-18T13:46:07.000Z","updated":"2022-08-23T10:54:44.583Z","comments":true,"path":"2022/08/18/限流算法/","link":"","permalink":"https://gwtt.github.io/2022/08/18/%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/","excerpt":"","text":"限流限流顾名思义就是限制流量，在软件系统中就是限制流量进入软件系统。 为什么要限流？在实际的生活场景中，当一个 web 服务部署到生产环境，也就是我们所说的公网。这个时候就会受到互联网上所有人的访问请求，比如像百度。每天都会有很多人访问 www.baidu.com ，如果有些人不怀好意的拼命的访问这个网站，那么整个系统就会因为这个人的恶作剧，从而浪费了很多不必要的带宽和系统资源。 限流实现因为我们现在的软件系统都是微服务形式的，一个 HTTP 请求可能要经过后端十几个软件服务，最后才能得到结果返回给用户。如果我们对一些请求进行限制，比如只允许某一个 IP 在 10 秒钟内访问 20 次，如果超出了这 20 次，直接最前端就返回 429 状态码。这样就保护了后端十几个服务，避免为这些恶意请求消耗系统资源。 常见的几种限流算法。有想法就会有实现，当前最常见的几种限流算法有固定窗口计数器算法 、滑动窗口计数器算法、漏桶算法、令牌桶算法。其中固定窗口计数器算法和滑动窗口计数器算法比较相似，漏桶算法和令牌桶算法比较类似。以下我们用只允许一个 ip 在 10 秒钟内只能访问 20 次这个限流需求来解释这些限流算法 固定窗口算法固定窗口算法就是设置一个固定的时间期限，当第一条请求到来的时候就开始计时同时计数，当接下来的 10 秒中内每来一条请求计数器就＋1。当计数器值到 20 后，接下来的所有请求都拒绝。十秒钟过后重置计数器。但是此算法存在一个缺陷：假设以下一种场景，攻击者在知道限流窗口是 10s 的情况下，先发送一条消息，让限流算法开启计数，此时计数器为 1，然后等到第 9.5 秒的时候持续发送请求攻击，这样 9.5-10 这个时间窗口里面会被允许经过 19 条消息，过了 10 秒后计数器归零，又马上接收了 20 条请求，这样，在 9.5-10.5 这个时间窗口总共接收了 39 条消息，限流值直接放大了一倍(原本是希望最大一秒钟只有 10 条的并发量) 滑动窗口算法滑动窗口算法就解决了上面固定窗口算法的缺陷。所谓的滑动窗口就是在原有的固定窗口上新增了一个和固定窗口大小一样的窗口，此窗口可以滑动如下图。 当第1条消息来到的时候，10秒的窗口期就生成，此时滑动窗口和第1个窗口重叠。接下来在第9.5秒的时候开始发动请求攻击，在第10秒的那一刻，滑动窗口。所含钙的窗口里面的计数器的值已经达到20，接下来我们假设又过了0.5秒，此时滑动窗口来到10.5秒。这时候滑动窗口涵盖了两个窗口。此时如何计算滑动窗口中计数器的值呢？我们可以假设前面的窗口所过来的流量是按照时间均匀分布的（虽然实际上并不是）。那么这个时候我们就可以计算出一个权重。就是滑动窗口涵盖第1个窗口时间的百分比：9.5/10=0.95。那么我们就假设当前这个窗口中所占有的数据为20*0.95=19。因此接下来我们只能允许通过一条数据。同时我们也可以计算出，10-10.5秒这个区间内算出来的值肯定小于1，因此这个区间内过来的请求全部会拒绝。 漏桶算法漏桶算法的思想类似于小时候的那道数学题，一个水缸一个水龙头放水，一个出水口出水，进水口就是攻击者的请求，放水口就是限流算法允许通过的请求的速率，当水缸满出来了，则将请求拒绝，水缸里的水就是攻击者的请求被缓存起来。 接下来还是拿上面的例子：假设水缸容量是20，放水速率是每秒2个，当攻击者突然一秒钟打过来30个请求，如果是窗口计数器算法(不管是固定还是滑动窗口)会直接一下子允许20个请求通过剩下10个拒绝，但是接下来剩余窗口时间内不允许有新的请求进来。而漏桶算法则是会缓存这20个请求剩下10个拒绝，然后以每秒2个请求的速率往下游传递。此算法不允许突发流量，永远保证下游的速率一致。 令牌桶算法令牌桶算法则是在漏桶算法上进行了修改。它的思想是假设桶内有很多令牌，同时以固定速率生成令牌放到桶内，如果桶内令牌满则丢弃。当请求过来的时候只要能拿到令牌就能通行。以上面例子为例：桶内存在20个令牌，当同时以每秒2个的速度生成令牌。当一次来30条请求，则由于桶内存在20个令牌，因此前20个请求都会被放行，剩下的10个请求都会被拒绝，接下来如果继续有请求过来的话，就会以每秒2个请求的速率放行，当一段时间没有请求后，桶内令牌又会存满。 总结总共介绍了四种常见的算到， 固定窗口算法实现简单，但是有缺点就是会超出限流阈值两倍的请求 滑动窗口可以解决固定窗口超出限流阈值的问题，到时他的计算权重并不是准确的，而是按照时间线将请求平均在时间线上 漏桶算法不允许一定的突发流量，这有时候可能在特定场景造成请求超时。 令牌桶允许突发流量 代码演示实现可以查看:(39条消息) 5种限流算法，7种限流方式，挡住突发流量_文晓武的博客-CSDN博客_限流算法 转自:几种常见的限流算法 - zhqqqy - 博客园 (cnblogs.com)","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://gwtt.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"泰拉瑞亚服务器搭建","slug":"泰拉瑞亚服务器搭建","date":"2022-08-10T07:58:02.000Z","updated":"2022-08-10T08:01:37.107Z","comments":true,"path":"2022/08/10/泰拉瑞亚服务器搭建/","link":"","permalink":"https://gwtt.github.io/2022/08/10/%E6%B3%B0%E6%8B%89%E7%91%9E%E4%BA%9A%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647一、设置swap，防止内存不足首先查看free –m可以看到swap分区是01、删除原有的分区swapoff -a2、新建分区dd if=/dev/zero of=/root/swapfile bs=1M count=20483、格式化交换分区mkswap /root/swapfile4、启动新建的swap交换分区swapon /root/swapfile5、添加开机启动进入目录vi /etc/fstab按（英文下状态小写的）i进入编辑模式，到达文件底部添加内容/root/swapfile swap swap defaults 0 0完毕后依次按键盘 Esc-键盘shift+:+输入wq保存/退出编辑好的文件6、使用命令重启服务器reboot……等待重启7、检查输入命令free –m看Swap有了数值，表示成功添加。8、进入管理员权限sudo su root 1234567891011121314151617181920二、安装工具1、下载工具yum romove wgetyum install -y wget2、解压工具yum install -y unzip3、压缩工具yum install –y zip4、远程管理工具yum install -y screen看到Complete!表示完成 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061三、创建目录1、创建父目录mkdir /opt/terraria2、创建2个子目录,和存档目录mkdir /opt/terraria/bin keyword zip3、查看目录是否创建成功ls /opt/terraria/4、进入zip文件夹cd /opt/terraria/zip5、下载泰拉瑞亚服务器包（最新版本去官网寻找）wget https://terraria.org/api/download/pc-dedicated-server/terraria-server-1436.zip6、查看是否下载好terraria-server-1436.zipls7、解压服务器包到bin文件夹unzip terraria-server-1436.zip -d ../bin8、新建服务器的配置文件vim ../server-config按（英文状态小写的）i进入编辑模式world=/opt/terraria/keyword/存档名.wldworldname=世界的名字difficulty=0autocreate=2maxplayers=4password=设定一个密码worldpath=/opt/terraria/keyword参数解释：autocreate是地图大小1&lt;2&lt;3difficulty是难度0为普通,1为专家maxplayers 最大人数按esc-shift+:- wq保存退出 12345678910111213141516171819202122232425262728293031323334353637383940414243444546四、启动准备1、放行7777端口启动防火墙：systemctl start firewalldfirewall-cmd --permanent --add-port=7777/tcpfirewall-cmd --reload1、给文件添加最高权限chmod 777 /opt/terraria/bin/1432/Linux/TerrariaServer.bin.x86_642、进入启动目录cd /opt/terraria/bin/1432/Linux3、启动服务器使用自定义的配置文件./TerrariaServer.bin.x86_64 -config /opt/terraria/server-config等待加载……4、已经创建成功此时还差一步先退出exit编辑之前的配置文件vim /opt/terraria/server-config按（英文状态小写的）i进入编辑模式分别给worldname=xx 每段前加#号difficulty=xxautocreate=xx按esc-shift+:- wq保存退出目的是让注释代码让其失效，不然每次重开服务器都会生成新世界 12345678910111213141516171819202122232425五、创建新会话来运行服务器1、移动到服务器目录cd /opt/terraria/1432/Linux2、创建新的screen会话，用于运行服务器screen -S terrariaServer3、启动服务器使用自定义的配置文件./TerrariaServer.bin.x86_64 -config /opt/terraria/server-config创建好后,使用Ctrl+A+D退出会话，不会被关闭停止服务器输入exit重新连接screen会话screen -R terrariaServer","categories":[{"name":"兴趣","slug":"兴趣","permalink":"https://gwtt.github.io/categories/%E5%85%B4%E8%B6%A3/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://gwtt.github.io/tags/Linux/"},{"name":"游戏","slug":"游戏","permalink":"https://gwtt.github.io/tags/%E6%B8%B8%E6%88%8F/"}]},{"title":"HashMap为什么线程不安全","slug":"HashMap为什么线程不安全","date":"2022-08-08T11:50:18.000Z","updated":"2022-08-17T11:58:05.190Z","comments":true,"path":"2022/08/08/HashMap为什么线程不安全/","link":"","permalink":"https://gwtt.github.io/2022/08/08/HashMap%E4%B8%BA%E4%BB%80%E4%B9%88%E7%BA%BF%E7%A8%8B%E4%B8%8D%E5%AE%89%E5%85%A8/","excerpt":"","text":"HashMap原理数据结构上：数组+(链表和红黑树) HashMap线程不安全问题 HashMap 是线程不安全的，原因就在于 HashMap 的 rehash。rehash 是 HashMap 扩容过程种的一个步骤。 HashMap 的容量是有限的。当经过多次元素插入，使得 HashMap 达到一定饱和度时，Key 映射位置发生冲突的几率会逐渐提高。 这时候，HashMap 需要扩展它的长度，也就是进行 Resize。 影响发生 Resize 的因素有两个： 1.Capacity HashMap 的当前长度。上一期曾经说过，HashMap 的长度是 2 的幂。 2.LoadFactor HashMap 负载因子，默认值为 0.75f。 衡量 HashMap 是否进行 Resize 的 条件如下： HashMap.Size &gt;= Capacity * LoadFactor HashMap 的扩容主要分为两步： 扩容 创建一个新的 Entry 空数组，长度是原数组的 2 倍 ReHash 遍历原 Entry 数组，把所有的 Entry 重新 Hash 到新数组。为什么要重新 Hash 呢？因为长度扩大以后，Hash 的规则也随之改变。 让我们回顾一下 Hash 公式： index = HashCode（Key） &amp; （Length - 1）**(计算哈希索引)** 当原数组长度为 8 时，Hash 运算是和 111B 做与运算；新数组长度为 16，Hash 运算是和 1111B 做与运算。Hash 结果显然不同。 ReHash 的 Java 代码如下： 123456789101112131415161718/** * Transfers all entries from current table to newTable. */void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 以上过程单线程下不会出现问题，但是当两个线程同时触发resize的时候就有可能出现问题 假设一个HashMap已经到了Resize的临界点。此时有两个线程A和B，在同一时刻对HashMap进行Put操作： 此时达到Resize条件，两个线程各自进行Rezie的第一步，也就是扩容： 这时候，两个线程都走到了ReHash的步骤。让我们回顾一下ReHash的代码： 1Entry&lt;K,V&gt; next = e.next; 假如此时线程B遍历到Entry3对象，刚执行完红框里的这行代码，线程就被挂起。对于线程B来说： e = Entry3 next = Entry2 这时候线程A畅通无阻地进行着Rehash，当ReHash完成后，结果如下（图中的e和next，代表线程B的两个引用）： 直到这一步，看起来没什么毛病。接下来线程B恢复，继续执行属于它自己的ReHash。线程B刚才的状态是： e = Entry3 next = Entry2 1int i = indexFor(e.hash, newCapacity); 当执行到上面这一行时，显然 i = 3，因为刚才线程A对于Entry3的hash结果也是3。 12newTable[i] = e;e = next; 我们继续执行到这两行，Entry3放入了线程B的数组下标为3的位置，并且e指向了Entry2。此时e和next的指向如下： e = Entry2 next = Entry2 整体情况如图所示： 接着是新一轮循环，又执行到红框内的代码行： 1Entry&lt;K,V&gt; next = e.next; e = Entry2 next = Entry3 整体情况如图所示： 接下来执行下面的三行，用头插法把Entry2插入到了线程B的数组的头结点： 123e.next = newTable[i];newTable[i] = e;e = next; 整体情况如图所示： 第三次循环开始，又执行到红框的代码： e = Entry3 next = Entry3.next = null 最后一步，当我们执行下面这一行的时候，见证奇迹的时刻来临了： 1e.next = newTable[i]; newTable[i] = Entry2 e = Entry3 Entry2.next = Entry3 Entry3.next = Entry2 链表出现了环形！ 整体情况如图所示： 此时，问题还没有直接产生。当调用Get查找一个不存在的Key，而这个Key的Hash结果恰好等于3的时候，由于位置3带有环形链表，所以程序将会进入死循环！**(此问题在JDK8中已经解决，上图过程看看就好)** 主要的问题是，两个线程在同一个数组索引下标添加元素时，比如A添加key2,B添加key3,会可能导致key2或者key3丢失。 ConCurrentHashMap安全吗，是怎么保证安全的？ 目前有如下一些方式可以获得线程安全的HashMap： Collections.synchronizedMap HashTable ConcurrentHashMap 其中，前两种方式由于全局锁的问题，存在很严重的性能问题。所以，著名的并发编程大师Doug Lea在JDK1.5的java.util.concurrent包下面添加了一大堆并发工具。其中就包含ConcurrentHashMap这个线程安全的HashMap。 ConcurrentHashMap在JDK7和JDK8中的实现方式上有较大的不同。首先我们先来大概回顾一下ConcurrentHashMap在JDK7中的原理是怎样的。 1.分段锁技术(JDK7) 针对HashTable会锁整个hash表的问题，ConcurrentHashMap提出了分段锁的解决方案。 分段锁的思想就是：锁的时候不锁整个hash表，而是只锁一部分。 如何实现呢？这就用到了ConcurrentHashMap中最关键的Segment。 ConcurrentHashMap中维护着一个Segment数组，每个Segment可以看做是一个HashMap。 而Segment本身继承了ReentrantLock，它本身就是一个锁。 在Segment中通过HashEntry数组来维护其内部的hash表。 每个HashEntry就代表了map中的一个K-V，用HashEntry可以组成一个链表结构，通过next字段引用到其下一个元素。 上述内容在源码中的表示如下： 1234567891011121314151617181920212223242526272829303132333435363738394041public class ConcurrentHashMap&lt;K, V&gt; extends AbstractMap&lt;K, V&gt; implements ConcurrentMap&lt;K, V&gt;, Serializable &#123; // ... 省略 ... /** * The segments, each of which is a specialized hash table. */ final Segment&lt;K,V&gt;[] segments; // ... 省略 ... /** * Segment是ConcurrentHashMap的静态内部类 * * Segments are specialized versions of hash tables. This * subclasses from ReentrantLock opportunistically, just to * simplify some locking and avoid separate construction. */ static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; // ... 省略 ... /** * The per-segment table. Elements are accessed via * entryAt/setEntryAt providing volatile semantics. */ transient volatile HashEntry&lt;K,V&gt;[] table; // ... 省略 ... &#125; // ... 省略 ... /** * ConcurrentHashMap list entry. Note that this is never exported * out as a user-visible Map.Entry. */ static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; // ... 省略 ... &#125;&#125; 由上图可见，只要我们的hash值足够分散，那么每次put的时候就会put到不同的segment中去。 而segment自己本身就是一个锁，put的时候，当前segment会将自己锁住，此时其他线程无法操作这个segment， 但不会影响到其他segment的操作。这个就是锁分段带来的好处。 2.线程安全的putConcurrentHashMap的put方法源码如下： 1234567891011121314public V put(K key, V value) &#123; Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); int hash = hash(key); int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; // 根据key的hash定位出一个segment，如果指定index的segment还没初始化，则调用ensureSegment方法初始化 if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); // 调用segment的put方法 return s.put(key, hash, value, false);&#125; 最终会调用segment的put方法，将元素put到HashEntry数组中，这里的注释中只给出锁相关的说明 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051final V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; // 因为segment本身就是一个锁 // 这里调用tryLock尝试获取锁 // 如果获取成功，那么其他线程都无法再修改这个segment // 如果获取失败，会调用scanAndLockForPut方法根据key和hash尝试找到这个node，如果不存在，则创建一个node并返回，如果存在则返回null // 查看scanAndLockForPut源码会发现他在查找的过程中会尝试获取锁，在多核CPU环境下，会尝试64次tryLock()，如果64次还没获取到，会直接调用lock() // 也就是说这一步一定会获取到锁 HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try &#123; HashEntry&lt;K,V&gt;[] tab = table; int index = (tab.length - 1) &amp; hash; HashEntry&lt;K,V&gt; first = entryAt(tab, index); for (HashEntry&lt;K,V&gt; e = first;;) &#123; if (e != null) &#123; K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; oldValue = e.value; if (!onlyIfAbsent) &#123; e.value = value; ++modCount; &#125; break; &#125; e = e.next; &#125; else &#123; if (node != null) node.setNext(first); else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) // 扩容 rehash(node); else setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; &#125; &#125; &#125; finally &#123; // 释放锁 unlock(); &#125; return oldValue;&#125; 3.线程安全的扩容(Rehash)HashMap的线程安全问题大部分出在扩容(rehash)的过程中。 ConcurrentHashMap的扩容只针对每个segment中的HashEntry数组进行扩容。 由上述put的源码可知，ConcurrentHashMap在rehash的时候是有锁的，所以在rehash的过程中，其他线程无法对segment的hash表做操作，这就保证了线程安全。 1.JDK8中ConcurrentHashMap的初始化以无参数构造函数为例，来看一下ConcurrentHashMap类初始化的时候会做些什么。 1ConcurrentHashMap&lt;String, String&gt; map = new ConcurrentHashMap&lt;&gt;(); 首先会执行静态代码块和初始化类变量。 主要会初始化以下这些类变量： 1234567891011121314151617181920212223242526272829303132333435这里用到了Unsafe类，其中objectFieldOffset方法用于获取指定Field(例如sizeCtl)在内存中的偏移量。// Unsafe mechanicsprivate static final sun.misc.Unsafe U;private static final long SIZECTL;private static final long TRANSFERINDEX;private static final long BASECOUNT;private static final long CELLSBUSY;private static final long CELLVALUE;private static final long ABASE;private static final int ASHIFT;static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset (k.getDeclaredField(&quot;sizeCtl&quot;)); TRANSFERINDEX = U.objectFieldOffset (k.getDeclaredField(&quot;transferIndex&quot;)); BASECOUNT = U.objectFieldOffset (k.getDeclaredField(&quot;baseCount&quot;)); CELLSBUSY = U.objectFieldOffset (k.getDeclaredField(&quot;cellsBusy&quot;)); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset (ck.getDeclaredField(&quot;value&quot;)); Class&lt;?&gt; ak = Node[].class; ABASE = U.arrayBaseOffset(ak); int scale = U.arrayIndexScale(ak); if ((scale &amp; (scale - 1)) != 0) throw new Error(&quot;data type scale not a power of two&quot;); ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); &#125; catch (Exception e) &#123; throw new Error(e); &#125;&#125;这里用到了Unsafe类，其中objectFieldOffset方法用于获取指定Field(例如sizeCtl)在内存中的偏移量。 这里用到了Unsafe类，其中objectFieldOffset方法用于获取指定Field(例如sizeCtl)在内存中的偏移量。 2.内部数据结构先来从源码角度看一下JDK8中是怎么定义的存储结构。 123456789101112131415161718192021222324/** * The array of bins. Lazily initialized upon first insertion. * Size is always a power of two. Accessed directly by iterators. * * hash表，在第一次put数据的时候才初始化，他的大小总是2的倍数。 */transient volatile Node&lt;K,V&gt;[] table;/** * 用来存储一个键值对 * * Key-value entry. This class is never exported out as a * user-mutable Map.Entry (i.e., one supporting setValue; see * MapEntry below), but can be used for read-only traversals used * in bulk tasks. Subclasses of Node with a negative hash field * are special, and contain null keys and values (but are never * exported). Otherwise, keys and vals are never null. */static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next;&#125; 可以发现，JDK8与JDK7的实现由较大的不同，JDK8中不在使用Segment的概念，他更像HashMap的实现方式。 3.线程安全的hash表初始化由上文可知ConcurrentHashMap是用table这个成员变量来持有hash表的。 table的初始化采用了延迟初始化策略，他会在第一次执行put的时候初始化table。 put方法源码如下（省略了暂时不相关的代码）： 1234567891011121314151617181920212223242526272829303132/** * Maps the specified key to the specified value in this table. * Neither the key nor the value can be null. * * &lt;p&gt;The value can be retrieved by calling the &#123;@code get&#125; method * with a key that is equal to the original key. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &#123;@code key&#125;, or * &#123;@code null&#125; if there was no mapping for &#123;@code key&#125; * @throws NullPointerException if the specified key or value is null */public V put(K key, V value) &#123; return putVal(key, value, false);&#125;/** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); // 计算key的hash值 int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // 如果table是空，初始化之 if (tab == null || (n = tab.length) == 0) tab = initTable(); // 省略... &#125; // 省略...&#125; initTable源码如下 1234567891011121314151617181920212223242526272829303132333435/** * Initializes table, using the size recorded in sizeCtl. */private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; // #1 while ((tab = table) == null || tab.length == 0) &#123; // sizeCtl的默认值是0，所以最先走到这的线程会进入到下面的else if判断中 // #2 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // 尝试原子性的将指定对象(this)的内存偏移量为SIZECTL的int变量值从sc更新为-1 // 也就是将成员变量sizeCtl的值改为-1 // #3 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; // 双重检查，原因会在下文分析 // #4 if ((tab = table) == null || tab.length == 0) &#123; int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; // 默认初始容量为16 @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; // #5 table = tab = nt; // 创建hash表，并赋值给成员变量table sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; // #6 sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 成员变量sizeCtl在ConcurrentHashMap中的其中一个作用相当于HashMap中的threshold，当hash表中元素个数超过sizeCtl时，触发扩容； 他的另一个作用类似于一个标识，例如，当他等于-1的时候，说明已经有某一线程在执行hash表的初始化了，一个小于-1的值表示某一线程正在对hash表执行resize。 这个方法首先判断sizeCtl是否小于0，如果小于0，直接将当前线程变为就绪状态的线程。 当sizeCtl大于等于0时，当前线程会尝试通过CAS的方式将sizeCtl的值修改为-1。修改失败的线程会进入下一轮循环，判断sizeCtl&lt;0了，被yield住；修改成功的线程会继续执行下面的初始化代码。 在new Node[]之前，要再检查一遍table是否为空，这里做双重检查的原因在于，如果另一个线程执行完#1代码后挂起，此时另一个初始化的线程执行完了#6的代码，此时sizeCtl是一个大于0的值，那么再切回这个线程执行的时候，是有可能重复初始化的。关于这个问题会在下图的并发场景中说明。 然后初始化hash表，并重新计算sizeCtl的值，最终返回初始化好的hash表。 4 .线程安全的putput操作可分为以下两类 当前hash表对应当前key的index上没有元素时 当前hash表对应当前key的index上已经存在元素时(hash碰撞) 4.1 hash表上没有元素时对应源码如下 1234567891011121314else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin&#125;static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125;static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125; tabAt方法通过Unsafe.getObjectVolatile()的方式获取数组对应index上的元素，getObjectVolatile作用于对应的内存偏移量上，是具备volatile内存语义的。 如果获取的是空，尝试用cas的方式在数组的指定index上创建一个新的Node。 4.2 hash碰撞时对应源码如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253else &#123; V oldVal = null; // 锁f是在4.1中通过tabAt方法获取的 // 也就是说，当发生hash碰撞时，会以链表的头结点作为锁 synchronized (f) &#123; // 这个检查的原因在于： // tab引用的是成员变量table，table在发生了rehash之后，原来index上的Node可能会变 // 这里就是为了确保在put的过程中，没有收到rehash的影响，指定index上的Node仍然是f // 如果不是f，那这个锁就没有意义了 if (tabAt(tab, i) == f) &#123; // 确保put没有发生在扩容的过程中，fh=-1时表示正在扩容 if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; // 在链表后面追加元素 pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; // 如果链表长度超过8个，将链表转换为红黑树，与HashMap相同，相对于JDK7来说，优化了查找效率 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125;&#125; 不同于JDK7中segment的概念，JDK8中直接用链表的头节点做为锁。 JDK7中，HashMap在多线程并发put的情况下可能会形成环形链表，ConcurrentHashMap通过这个锁的方式，使同一时间只有有一个线程对某一链表执行put，解决了并发问题。 5 线程安全的扩容put方法的最后一步是统计hash表中元素的个数，如果超过sizeCtl的值，触发扩容。 扩容的代码略长，可大致看一下里面的中文注释，再参考下面的分析。 其实我们主要的目的是弄明白ConcurrentHashMap是如何解决HashMap的并发问题的。 带着这个问题来看源码就好。关于HashMap存在的问题，参考本文一开始说的笔者的另一篇文章即可。 其实HashMap的并发问题多半是由于put和扩容并发导致的。 这里我们就来看一下ConcurrentHashMap是如何解决的。 扩容涉及的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211/** * The array of bins. Lazily initialized upon first insertion. * Size is always a power of two. Accessed directly by iterators. * 业务中使用的hash表 */transient volatile Node&lt;K,V&gt;[] table;/** * The next table to use; non-null only while resizing. * 扩容时才使用的hash表，扩容完成后赋值给table，并将nextTable重置为null。 */private transient volatile Node&lt;K,V&gt;[] nextTable;/** * Adds to count, and if table is too small and not already * resizing, initiates transfer. If already resizing, helps * perform transfer if work is available. Rechecks occupancy * after a transfer to see if another resize is already needed * because resizings are lagging additions. * * @param x the count to add * @param check if &lt;0, don&#x27;t check resize, if &lt;= 1 only check if uncontended */private final void addCount(long x, int check) &#123; // ----- 计算键值对的个数 start ----- CounterCell[] as; long b, s; if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; // ----- 计算键值对的个数 end ----- // ----- 判断是否需要扩容 start ----- if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; // 当上面计算出来的键值对个数超出sizeCtl时，触发扩容，调用核心方法transfer while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // 如果有已经在执行的扩容操作，nextTable是正在扩容中的新的hash表 // 如果并发扩容，transfer直接使用正在扩容的新hash表，保证了不会出现hash表覆盖的情况 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; // 更新sizeCtl的值，更新成功后为负数，扩容开始 // 此时没有并发扩容的情况，transfer中会new一个新的hash表来扩容 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125; // ----- 判断是否需要扩容 end -----&#125;/** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. */private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings(&quot;unchecked&quot;) // 初始化新的hash表，大小为之前的2倍，并赋值给成员变量nextTable Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; // 扩容完成时，将成员变量nextTable置为null，并将table替换为rehash后的nextTable if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; // 接下来是遍历每个链表，对每个链表的元素进行rehash // 仍然用头结点作为锁，所以在扩容的时候，无法对这个链表执行put操作 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) &#123; int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; // setTabAt方法调用了Unsafe.putObjectVolatile来完成hash表元素的替换，具备volatile内存语义 setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 根据上述代码，对ConcurrentHashMap是如何解决HashMap并发问题这一疑问进行简要说明。 首先new一个新的hash表(nextTable)出来，大小是原来的2倍。后面的rehash都是针对这个新的hash表操作，不涉及原hash表(table)。 然后会对原hash表(table)中的每个链表进行rehash，此时会尝试获取头节点的锁。这一步就保证了在rehash的过程中不能对这个链表执行put操作。 通过sizeCtl控制，使扩容过程中不会new出多个新hash表来。 最后，将所有键值对重新rehash到新表(nextTable)中后，用nextTable将table替换。这就避免了HashMap中get和扩容并发时，可能get到null的问题。 在整个过程中，共享变量的存储和读取全部通过volatile或CAS的方式，保证了线程安全。 [ ]转载:HashMap原理及线程不安全详解 - 木子H的个人空间 - OSCHINA - 中文开源技术交流社区 []参考:Java8中ConcurrentHashMap是如何保证线程安全的 - 知乎 (zhihu.com)","categories":[{"name":"java知识","slug":"java知识","permalink":"https://gwtt.github.io/categories/java%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"自动拆箱和装箱原理","slug":"自动拆箱和装箱原理","date":"2022-08-08T11:22:37.000Z","updated":"2022-08-08T11:49:38.353Z","comments":true,"path":"2022/08/08/自动拆箱和装箱原理/","link":"","permalink":"https://gwtt.github.io/2022/08/08/%E8%87%AA%E5%8A%A8%E6%8B%86%E7%AE%B1%E5%92%8C%E8%A3%85%E7%AE%B1%E5%8E%9F%E7%90%86/","excerpt":"","text":"自动装箱和自动拆箱是两个相反的过程，自动装箱即将基本数据类型转换为对应的封装类，自动拆箱即将封装类转换为对应的基本数据类型。此外，装箱的过程会增加内存的消耗，影响性能，因为这个过程会创建对应的对象。 1234567public class Main &#123; public static void main(String[] args) &#123; Integer integerNum = 100; // 进行自动装箱，得到的是封装类 int intNum = integerNum; // 进行自动拆箱，得到基本数据类型 &#125;&#125; 通过 javap -c Main.class 查看生成的字节码文件。 12345678910111213141516171819Compiled from &quot;Main.java&quot;public class club.wadreamer.test.Main &#123; public club.wadreamer.test.Main(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return public static void main(java.lang.String[]); Code: 0: bipush 100 2: invokestatic #2 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 5: astore_1 6: aload_1 7: invokevirtual #3 // Method java/lang/Integer.intValue:()I 10: istore_2 11: return&#125; Integer#valueOf() 和 Integer#intValue() 的源码如下： 123456789101112// 自动装箱public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125;// 自动拆箱public int intValue() &#123; return value;&#125; 从上述字节码可以得出如下结论： 在进行自动装箱时，Java 虚拟机会自动调用 Integer#valueOf()。 在进行自动拆箱时，Java 虚拟机会自动调用 Integer#intValue()。 其他数据类型的自动装箱和自动拆箱的过程和 Integer 类似，都是调用类似 xxxValue()、valueOf() 等方法。 其他案例分析1.空指针异常 包装类为空，拆箱时异常 2.equals和==问题 两个包装类用==，比较的是地址 两个包装类用equals，比较的是值 一个包装类和一个基本数据类型用equals，比较的是值 一个包装类和一个基本数据类型用==，比较的是值（会自动拆包） 两个基本数据类型用==比较的是值 3.拆箱的缓存机制 对于 Integer，在 [-128, 127] 之间只有固定的 256 个值，所以为了避免多次创建对象，事先创建好一个大小为 256 的 Integer 数组 cache，所以如果值在这个范围内，就可以直接返回我们事先创建好的对象即可。 对于 Double 类型来说，我们就不能这样做，因为它在这个范围内个数是无限的。 总结一句就是：在某个范围内的整型数值的个数是有限的，而浮点数却不是。所以在 Double 里面的做法很直接，就是直接创建一个对象，所以每次创建的对象都不一样。 对于 Boolean 类型来说，在内部已经提前创建好两个对象，因为它只有两种情况，这样也是为了避免重复创建太多的对象。因此，每次执行 Boolean#valueOf() 返回的都是相同的对象。 总结: 存在拆箱操作时一定要特别注意封装类对象是否为 null。 包装类和基本数据类型在进行== 运算和算数运算时，会进行自动拆箱。 equals() 会进行自动装箱操作，且需要先判断封装类的类型是否相同，再进一步判断内容是否相同。（==是优先比较基本数据类型，是拆箱操作） Integer、Short、Byte、Character、Long 这几个类的 valueOf() 的实现是类似的，均在存在 [-128, 127] 的缓存。 Double、Float 的 valueOf() 的实现是类似的，每次都返回不同的对象。 Boolean 预先创建了两个对象，Boolean#valueOf() 每次返回的都是相同的对象。","categories":[{"name":"java知识","slug":"java知识","permalink":"https://gwtt.github.io/categories/java%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"八锁问题","slug":"八锁问题","date":"2022-07-10T12:28:04.572Z","updated":"2022-07-10T12:31:59.326Z","comments":true,"path":"2022/07/10/八锁问题/","link":"","permalink":"https://gwtt.github.io/2022/07/10/%E5%85%AB%E9%94%81%E9%97%AE%E9%A2%98/","excerpt":"","text":"八锁问题 new发短信后 睡眠100毫秒，发短信、发邮件的打印顺序 123456789101112131415161718192021222324252627public class Test01 &#123; public static void main(String[] args) throws InterruptedException &#123; Test0101 test0101 = new Test0101(); new Thread(() -&gt; &#123; test0101.sendMessage(); &#125;,&quot;AAA&quot;).start(); Thread.sleep(100); new Thread(() -&gt; &#123; test0101.sendEmail(); &#125;,&quot;BBB&quot;).start(); &#125;&#125;class Test0101&#123; public synchronized void sendMessage()&#123; System.out.println(&quot;sendMessage&quot;); &#125; public synchronized void sendEmail()&#123; System.out.println(&quot;sendEmail&quot;); &#125;&#125;结果:sendMessagesendEmail因为synchronized关键字 是对该资源类的对象上锁，因此哪个线程先拿到对象锁，就先执行 2.发短信线程中执行时睡眠4秒，发短信、发邮件的打印顺序 1234567891011121314151617181920212223242526272829303132public class Test02 &#123; public static void main(String[] args) throws InterruptedException &#123; Test0201 test0201 = new Test0201(); new Thread(() -&gt; &#123; test0201.sendMessage(); &#125;,&quot;AAA&quot;).start(); Thread.sleep(100); new Thread(() -&gt; &#123; test0201.sendEmail(); &#125;,&quot;BBB&quot;).start(); &#125;&#125;class Test0201&#123; public synchronized void sendMessage()&#123; try &#123; Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;sendMessage&quot;); &#125; public synchronized void sendEmail()&#123; System.out.println(&quot;sendEmail&quot;); &#125;&#125;结果:sendMessagesendEmail原理同上。还是上面的线程先拿到 资源类 锁对象 3.打电话线程，发短信、打电话的打印顺序 1234567891011121314151617181920212223242526272829303132333435public class Test03 &#123; public static void main(String[] args) throws InterruptedException &#123; Test0301 test0301 = new Test0301(); new Thread(() -&gt; &#123; test0301.sendMessage(); &#125;,&quot;AAA&quot;).start(); Thread.sleep(100); new Thread(() -&gt; &#123; test0301.call(); &#125;,&quot;BBB&quot;).start(); &#125;&#125;class Test0301&#123; public synchronized void sendMessage()&#123; try &#123; Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;sendMessage&quot;); &#125; public synchronized void sendEmail()&#123; System.out.println(&quot;sendEmail&quot;); &#125; public void call()&#123; System.out.println(&quot;call&quot;); &#125;&#125;结果:callsendMessagecall()为普通方法,不受同步方法的影响,不受锁的影响 4.两个资源，发短信、发邮件的打印顺序(先邮件后短信) 1234567891011121314151617181920212223242526272829303132333435public class Test04 &#123; public static void main(String[] args) throws InterruptedException &#123; Test0401 test0401 = new Test0401(); Test0401 test0402 = new Test0401(); new Thread(() -&gt; &#123; test0401.sendMessage(); &#125;,&quot;AAA&quot;).start(); Thread.sleep(100); new Thread(() -&gt; &#123; test0402.sendEmail(); &#125;,&quot;BBB&quot;).start(); &#125;&#125;class Test0401&#123; public synchronized void sendMessage()&#123; try &#123; Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;sendMessage&quot;); &#125; public synchronized void sendEmail()&#123; System.out.println(&quot;&quot;); &#125; public void call()&#123; System.out.println(&quot;call&quot;); &#125;&#125;结果:sendEmailsendMessage区别于问题1，该情况是 两个资源类对象分别开启两个线程，因此锁对象 并无互相干扰，因为线程延时的原因，打电话 先输出 5.两个同步方法变静态、一个资源，发短信、发邮件的打印顺序 12345678910111213141516171819202122232425262728293031323334public class Test05&#123; public static void main(String[] args) throws InterruptedException &#123; Test0501 test0401 = new Test0501(); new Thread(() -&gt; &#123; test0401.sendMessage(); &#125;,&quot;AAA&quot;).start(); Thread.sleep(100); new Thread(() -&gt; &#123; test0401.sendEmail(); &#125;,&quot;BBB&quot;).start(); &#125;&#125;class Test0501&#123; public static synchronized void sendMessage()&#123; try &#123; Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;sendMessage&quot;); &#125; public static synchronized void sendEmail()&#123; System.out.println(&quot;sendEmail&quot;); &#125; public void call()&#123; System.out.println(&quot;call&quot;); &#125;&#125;结果:sendMessagesendEmail加上static关键字之后，两个方法都变为静态方法。 6.两个静态同步方法、两个资源，发短信、发邮件的打印顺序 1234567891011121314151617181920212223242526272829303132333435public class Test06 &#123; public static void main(String[] args) throws InterruptedException &#123; Test0601 test0601 = new Test0601(); Test0601 test0602 = new Test0601(); new Thread(() -&gt; &#123; test0601.sendMessage(); &#125;,&quot;AAA&quot;).start(); Thread.sleep(100); new Thread(() -&gt; &#123; test0602.sendEmail(); &#125;,&quot;BBB&quot;).start(); &#125;&#125;class Test0601&#123; public static synchronized void sendMessage()&#123; try &#123; Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;sendMessage&quot;); &#125; public static synchronized void sendEmail()&#123; System.out.println(&quot;sendEmail&quot;); &#125; public void call()&#123; System.out.println(&quot;call&quot;); &#125;&#125;结果:sendMessagesendEmail原理同5，synchronized 加 静态方法 锁的是 Class 7.一个静态同步方法、一个普通同步方法、一个资源，发短信、发邮件的打印顺序 12345678910111213141516171819202122232425262728293031323334public class Test07 &#123; public static void main(String[] args) throws InterruptedException &#123; Test0701 test0701 = new Test0701(); new Thread(() -&gt; &#123; test0701.sendMessage(); &#125;,&quot;AAA&quot;).start(); Thread.sleep(100); new Thread(() -&gt; &#123; test0701.sendEmail(); &#125;,&quot;BBB&quot;).start(); &#125;&#125;class Test0701&#123; public static synchronized void sendMessage()&#123; try &#123; Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;sendMessage&quot;); &#125; public synchronized void sendEmail()&#123; System.out.println(&quot;sendEmail&quot;); &#125; public void call()&#123; System.out.println(&quot;call&quot;); &#125;&#125;结果:sendEmailsendMessagesynchronized 锁的是 类实例即对象 、synchronized 加 静态方法 锁的是 Class 8.一个静态同步方法、一个普通同步方法、两个资源，发短信、发邮件的打印顺序 1234567891011121314151617181920212223242526272829303132333435public class Test08 &#123; public static void main(String[] args) throws InterruptedException &#123; Test0801 test0801 = new Test0801(); Test0801 test0802 = new Test0801(); new Thread(() -&gt; &#123; test0801.sendMessage(); &#125;,&quot;AAA&quot;).start(); Thread.sleep(100); new Thread(() -&gt; &#123; test0802.sendEmail(); &#125;,&quot;BBB&quot;).start(); &#125;&#125;class Test0801&#123; public static synchronized void sendMessage()&#123; try &#123; Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;sendMessage&quot;); &#125; public synchronized void sendEmail()&#123; System.out.println(&quot;sendEmail&quot;); &#125; public void call()&#123; System.out.println(&quot;call&quot;); &#125;&#125;结果:sendEmailsendMessage原理同7","categories":[{"name":"java知识","slug":"java知识","permalink":"https://gwtt.github.io/categories/java%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"Juc","slug":"Juc","permalink":"https://gwtt.github.io/tags/Juc/"},{"name":"java基础","slug":"java基础","permalink":"https://gwtt.github.io/tags/java%E5%9F%BA%E7%A1%80/"}]},{"title":"lock锁和Synchronized锁","slug":"lock锁和Synchronized锁","date":"2022-07-10T08:30:15.327Z","updated":"2022-07-10T08:57:27.102Z","comments":true,"path":"2022/07/10/lock锁和Synchronized锁/","link":"","permalink":"https://gwtt.github.io/2022/07/10/lock%E9%94%81%E5%92%8CSynchronized%E9%94%81/","excerpt":"","text":"lock锁和Synchronized锁 synchronized是Java中的关键字，是一种同步锁。它修饰的对象有以下几种(一个线程访问一个对象中的synchronized(this)同步代码块时，其他试图访问该对象的线程将被阻塞)： 修饰一个代码块，被修饰的代码块称为同步语句块，其作用的范围是大括号{}括起来的代码，作用的对象是调用这个代码块的对象； 修饰一个方法，被修饰的方法称为同步方法，其作用的范围是整个方法，作用的对象是调用这个方法的对象； 修改一个静态的方法，其作用的范围是整个静态方法，作用的对象是这个类的所有对象； 修改一个类，其作用的范围是synchronized后面括号括起来的部分，作用主的对象是这个类的所有对象。 12345678910111213141516171819202122232425262728293031323334353637class Ticket &#123; private int number = 0; public synchronized void sale()&#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(Thread.currentThread().getName()+&quot;:&quot;+(number++)); &#125; &#125;&#125; public static void main(String[] args) throws InterruptedException &#123; Ticket ticket = new Ticket(); new Thread(()-&gt;&#123; ticket.sale(); &#125;,&quot;aa&quot;).start(); new Thread(()-&gt;&#123; ticket.sale(); &#125;,&quot;bb&quot;).start(); new Thread(()-&gt;&#123; ticket.sale(); &#125;,&quot;cc&quot;).start(); &#125;结果:aa:0aa:1aa:2aa:3aa:4bb:5bb:6bb:7bb:8bb:9cc:10cc:11cc:12cc:13cc:14 Lock synchronized是java中的一个关键字，也就是说是Java语言内置的特性。那么为什么会出现Lock呢？ 1）Lock不是Java语言内置的，synchronized是Java语言的关键字，因此是内置特性。Lock是一个类，通过这个类可以实现同步访问； 2）Lock和synchronized有一点非常大的不同，采用synchronized不需要用户去手动释放锁，当synchronized方法或者synchronized代码块执行完之后，系统会自动让线程释放对锁的占用；而Lock则必须要用户去手动释放锁，如果没有主动释放锁，就有可能导致出现死锁现象。 synchronized 的局限性 与 Lock 的优点 如果一个代码块被synchronized关键字修饰，当一个线程获取了对应的锁，并执行该代码块时，其他线程便只能一直等待直至占有锁的线程释放锁。事实上，占有锁的线程释放锁一般会是以下三种情况之一： 1：占有锁的线程执行完了该代码块，然后释放对锁的占有； 2：占有锁线程执行发生异常，此时JVM会让线程自动释放锁； 3：占有锁线程进入 WAITING 状态从而释放锁，例如在该线程中调用wait()方法等。 试考虑以下三种情况： Case 1 ： 在使用synchronized关键字的情形下，假如占有锁的线程由于要等待IO或者其他原因（比如调用sleep方法）被阻塞了，但是又没有释放锁，那么其他线程就只能一直等待，别无他法。这会极大影响程序执行效率。因此，就需要有一种机制可以不让等待的线程一直无期限地等待下去（比如只等待一定的时间 (解决方案：tryLock(long time, TimeUnit unit)) 或者 能够响应中断 (解决方案：lockInterruptibly())），这种情况可以通过 Lock 解决。 Case 2 ： 我们知道，当多个线程读写文件时，读操作和写操作会发生冲突现象，写操作和写操作也会发生冲突现象，但是读操作和读操作不会发生冲突现象。但是如果采用synchronized关键字实现同步的话，就会导致一个问题，即当多个线程都只是进行读操作时，也只有一个线程在可以进行读操作，其他线程只能等待锁的释放而无法进行读操作。因此，需要一种机制来使得当多个线程都只是进行读操作时，线程之间不会发生冲突。同样地，Lock也可以解决这种情况 (解决方案：ReentrantReadWriteLock) 。 Case 3 ： 我们可以通过Lock得知线程有没有成功获取到锁 (解决方案：ReentrantLock) ，但这个是synchronized无法办到的。 上面提到的三种情形，我们都可以通过Lock来解决，但 synchronized 关键字却无能为力。事实上，Lock 是 java.util.concurrent.locks包 下的接口，Lock 实现提供了比 synchronized 关键字 更广泛的锁操作，它能以更优雅的方式处理线程同步问题。也就是说，Lock提供了比synchronized更多的功能。 123456789101112131415161718192021222324252627282930313233343536373839class Ticket &#123; private ReentrantLock lock = new ReentrantLock(); private int number = 0; public void sale()&#123; lock.lock(); for (int i = 0; i &lt; 5; i++) &#123; System.out.println(Thread.currentThread().getName()+&quot;:&quot;+(number++)); &#125; lock.unlock(); &#125;&#125; public static void main(String[] args) throws InterruptedException &#123; Ticket ticket = new Ticket(); new Thread(()-&gt;&#123; ticket.sale(); &#125;,&quot;aa&quot;).start(); new Thread(()-&gt;&#123; ticket.sale(); &#125;,&quot;bb&quot;).start(); new Thread(()-&gt;&#123; ticket.sale(); &#125;,&quot;cc&quot;).start(); &#125;结果:aa:0aa:1aa:2aa:3aa:4cc:5cc:6cc:7cc:8cc:9bb:10bb:11bb:12bb:13bb:14 lock没有主动用unlock()去释放锁,会造成死锁现象,因此用lock需要在finally块中释放锁. 1234567891011121314151617// 获取锁 void lock() // 如果当前线程未被中断，则获取锁，可以响应中断 void lockInterruptibly() // 返回绑定到此 Lock 实例的新 Condition 实例 Condition newCondition() // 仅在调用时锁为空闲状态才获取该锁，可以响应中断 boolean tryLock() // 如果锁在给定的等待时间内空闲，并且当前线程未被中断，则获取锁 boolean tryLock(long time, TimeUnit unit) // 释放锁 void unlock()","categories":[{"name":"java知识","slug":"java知识","permalink":"https://gwtt.github.io/categories/java%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"java基础","slug":"java基础","permalink":"https://gwtt.github.io/tags/java%E5%9F%BA%E7%A1%80/"},{"name":"方法句柄， JVM","slug":"方法句柄，-JVM","permalink":"https://gwtt.github.io/tags/%E6%96%B9%E6%B3%95%E5%8F%A5%E6%9F%84%EF%BC%8C-JVM/"}]},{"title":"java7新特性之方法句柄","slug":"方法句柄","date":"2022-07-10T06:02:29.913Z","updated":"2022-07-10T08:30:08.808Z","comments":true,"path":"2022/07/10/方法句柄/","link":"","permalink":"https://gwtt.github.io/2022/07/10/%E6%96%B9%E6%B3%95%E5%8F%A5%E6%9F%84/","excerpt":"","text":"方法句柄JDK6之前我们会使用java反射来实现动态方法调用，多数框架用反射的比较多，例如mybatis、spring等。在JDK7中，新增了java.lang.invoke.MethodHandle（方法句柄），称之为“现代化反射”。其实反射和java.lang.invoke.MethodHandle都是间接调用方法的途径，但java.lang.invoke.MethodHandle比反射更简洁，用反射功能会写一大堆冗余代码。 12345678910111213141516171819202122232425public interface Animal &#123; void race();&#125;public class Horse implements Animal &#123; @Override public void race() &#123; System.out.println(&quot;Horse.race()&quot;); &#125;&#125;.....// 利用接口抽象方法 public static void main(String[] args) throws Exception &#123; Animal animal = new Horse(); animal.race(); &#125;// 反射调用 public static void main(String[] args) throws Exception &#123; Horse object = new Horse(); Method method = object.getClass().getMethod(&quot;race&quot;); method.invoke(object); &#125; MethodType用来描述方法的返回值类型以及入参类型。MehodHandle包含一个指向Method对象（方法在jvm内部的对等体）的指针。 12345678910111213141516171819202122232425262728public class Main &#123; public String toString(String s) &#123; return &quot;hello,&quot; + s + &quot;MethodHandle&quot;; &#125; public static void main(String[] args) throws Throwable &#123; Main main = new Main(); MethodHandle mh = getToStringMH(); System.out.println((String) mh.invokeExact(main, &quot;ss&quot;)); &#125; public static MethodHandle getToStringMH() &#123; MethodType mt = MethodType.methodType(String.class, String.class); //获取方法类型 参数为:1.返回值类型,2方法中参数类型 MethodHandle mh = null; try &#123; mh = MethodHandles.lookup().findVirtual(Main.class, &quot;toString&quot;, mt); //查找方法句柄 &#125; catch (NoSuchMethodException | IllegalAccessException e) &#123; e.printStackTrace(); &#125; return mh; &#125;&#125; 方法句柄是一个强类型的，能够被直接执行的引用。该引用可以指向常规的静态方法或者实例方法，也可以指向构造器或者字段。当指向字段时，方法句柄实则指向包含字段访问字节码的虚构方法，语义上等价于目标字段的 getter 或者 setter 方法。 方法句柄的类型（MethodType）是由所指向方法的参数类型以及返回类型组成的。它是用来确认方法句柄是否适配的唯一关键。当使用方法句柄时，我们其实并不关心方法句柄所指向方法的类名或者方法名。 MethodHandle 的创建方式： 方式一、通过反射创建 MethodHandle（不符合初衷） 123456789101112131415public class Main &#123; public static void race() &#123; System.out.println(&quot;hello&quot;); &#125; public static MethodHandles.Lookup lookup() &#123; return MethodHandles.lookup(); &#125; public static void main(String[] args) throws Throwable &#123; Method method = Main.class.getDeclaredMethod(&quot;race&quot;); MethodHandle methodHandle = Main.lookup().unreflect(method); methodHandle.invoke(); &#125;&#125;输出: hello 方式二、根据 MethodType 创建 MethodHandle 1234567891011121314151617181920212223242526public class Main &#123; public static void race()&#123; System.out.println(&quot;race&quot;); &#125; public void say()&#123; System.out.println(&quot;say&quot;); &#125; public static MethodHandles.Lookup lookup() &#123; return MethodHandles.lookup(); &#125; public static void main(String[] args) throws Throwable &#123; MethodHandles.Lookup lookup = Main.lookup(); MethodType methodType = MethodType.methodType(void.class); MethodHandle methodHandle = lookup.findStatic(Main.class, &quot;race&quot;, methodType); MethodHandle methodHandle2 = lookup.findVirtual(Main.class, &quot;say&quot;, methodType); methodHandle.invoke(); methodHandle2.invoke(new Main()); &#125;&#125;输出:racesay 小结一下： 1、方法句柄的访问权限不取决于方法句柄的创建位置，而是取决于 Lookup 对象的创建位置。 2、如果 JDK 版本大于8，那么可以在其他类中，也能够通过该 Lookup 对象对类私有属性进行赋值、取值操作。 外类中操控私有字段因为权限问题，外类中创建的 Lookup 对象无法操控私有字段，那么有什么替代方案呢？ 通过操控get和set方法 反射 关于invoke和invokeExtract方法的区别： invokeExtract要求更加精确，如下 methodHandle2.invokeExact(test1,5.1,new Integer(1));可以执行，methodHandle2.invokeExact(test1,5.1,1);会报错，因为要将1转换为integer，所以不合要求。这个方法要求不能有任何类型转换，也就是参数严格一致。invoke相对要轻松很多。 关键概念 LookupMethodHandle 的创建工厂，通过它可以创建MethodHandle，值得注意的是检查工作是在创建时处理的，而不是在调用时处理。 MethodType顾名思义，就是代表方法的签名。一个方法的返回值类型是什么，有几个参数，每个参数的类型什么？ MethodHandle方法句柄，通过它我们就可以动态访问类型信息了。 如何使用 当理解了上面几个关键概念后使用起来就比较简单了，总的来说只需要4步： 创建Lookup创建MethodType基于Lookup与MethodType获得MethodHandle调用MethodHandle","categories":[{"name":"java知识","slug":"java知识","permalink":"https://gwtt.github.io/categories/java%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"java基础","slug":"java基础","permalink":"https://gwtt.github.io/tags/java%E5%9F%BA%E7%A1%80/"},{"name":"方法句柄， JVM","slug":"方法句柄，-JVM","permalink":"https://gwtt.github.io/tags/%E6%96%B9%E6%B3%95%E5%8F%A5%E6%9F%84%EF%BC%8C-JVM/"}]},{"title":"Git面试","slug":"Git面试","date":"2022-07-02T09:55:45.230Z","updated":"2022-08-16T13:37:10.877Z","comments":true,"path":"2022/07/02/Git面试/","link":"","permalink":"https://gwtt.github.io/2022/07/02/Git%E9%9D%A2%E8%AF%95/","excerpt":"","text":"Git1.什么是git? git是一个常用的分布式版本管理工具。 2.git常用的命令 git add 添加文件到暂存区 git commit 提交文件到本地仓库 git pull 从远程仓库拉取项目到本地 git push 将本地仓库的新的改变推送到远程仓库 git clone 将远程仓库复制到本地 git fetch 抓取 git merge 合并 3.git pull、git merge、git fetch三个命令的区别？ git clone：是在本地没有版本库的情况下，从远程仓库克隆一份到本地，是一个本地版本库从无到有的过程 git pull：是在本地仓库已经存在的情况下，将远程最新的commits抓取并合并到本地版本库的过程 git fetch： 从远程版本库抓取最新的commits，不会进行合并 git merge：合并 所以git pull = git fetch + git merge 4.push之前一定要进行哪个操作？ push之前一定要进行本地更新操作。使用git pull命令或者使用git fetch和git merge的命令组合。这时候，可能会出现版本冲突，如果出现的话，需要解决完冲突再进行代码push。 5.如何解决版本冲突 版本冲突多出现在合并操作(合并远程仓库代码或者合并分支代码)中。如果出现版本冲突，需要具体分析出现冲突的代码区，手动进行代码合并，然后再进行提交。 6.别人在远程推送的新分支怎么合并在自己上面 假设别人分支是A，我是B git checkout B;//到自己分区 git merge A;//合并分支 git push;//提交","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Git","slug":"Git","permalink":"https://gwtt.github.io/tags/Git/"}]},{"title":"Springboot面试(2)","slug":"Springboot面试(2)","date":"2022-07-02T09:31:37.501Z","updated":"2022-07-02T10:22:35.985Z","comments":true,"path":"2022/07/02/Springboot面试(2)/","link":"","permalink":"https://gwtt.github.io/2022/07/02/Springboot%E9%9D%A2%E8%AF%95(2)/","excerpt":"","text":"springboot(2)1.Spring boot的核心配置文件是什么 Spring Boot 的核心配置文件是 application 和 bootstrap 配置文件。 bootstrap 配置文件是系统级别的，用来加载外部配置，如配置中心的配置信息，也可以用来定义系统不会变化的属性.bootstatp 文件的加载先于application文件application 配置文件是应用级别的，是当前应用的配置文件 2.如何使用jwt进行登录校验 （1）客户端登录成功后（必须是在登陆成功才行，与session一样的前提条件），服务器会根据用户名和签名以及其他信息加密生成唯一的token串，用来区分他们，不需要存入服务端的缓存中，但会把这个token返回给相应的主机， （2）主机收到token后会存入cookie或者localStorage中，以后主机的每一次发送其他类型的请求的操作都会携带这个token， （3）服务器会将客户端发来的这个token和服务端从数据库查询出来的并且重新计算得到的用户信息进行对比，如果匹配，则认证成功，如果用户请求的资源需要相应的权限，则校验token中的payload中存储的权限等相关信息，如果有权限则返回给对应主机所需要的资源（即做到了权限鉴权），否则拒绝 用的依赖是java-jwt，用的包是auth0.jwt 3.生成的token在服务端有哪些存储方案 （1）保存在redis，最常用，也是分布式下的验证token的解决方案， （2）数据库存储，性能比redis稍差，速度稍慢 （3）不做保存，下次验证的时候直接用jwt.decode验证(服务端为express)，存储的压力给到了客户端，但是每次从客户端传到服务器端的数据量会稍微大一些","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Springboot","slug":"Springboot","permalink":"https://gwtt.github.io/tags/Springboot/"}]},{"title":"Juc面试(2)","slug":"Juc面试(2)","date":"2022-07-02T07:35:00.409Z","updated":"2022-09-23T01:31:27.654Z","comments":true,"path":"2022/07/02/Juc面试(2)/","link":"","permalink":"https://gwtt.github.io/2022/07/02/Juc%E9%9D%A2%E8%AF%95(2)/","excerpt":"","text":"juc(2)1.线程的生命周期 线程的生命周期包含5个阶段，包括：新建、就绪、运行、阻塞、销毁。 新建：就是刚使用new方法，new出来的线程； 就绪：就是调用的线程的start()方法后，这时候线程处于等待CPU分配资源阶段，谁先抢的CPU资源，谁开始执行; 运行：当就绪的线程被调度并获得CPU资源时，便进入运行状态，run方法定义了线程的操作和功能; 阻塞：在运行状态的时候，可能因为某些原因导致运行状态的线程变成了阻塞状态，比如sleep()、wait()之后线程就处于了阻塞状态，这个时候需要其他机制将处于阻塞状态的线程唤醒，比如调用notify或者notifyAll()方法。唤醒的线程不会立刻执行run方法，它们要再次等待CPU分配资源进入运行状态; 销毁：如果线程正常执行完毕后或线程被提前强制性的终止或出现异常导致结束，那么线程就要被销毁，释放资源; 那么处于Running状态的线程能发生哪些状态转变？ 被转换成Terminated状态，比如调用 stop() 方法; 被转换成Blocked状态，比如调用了sleep, wait 方法被加入 waitSet 中； 被转换成Blocked状态，如进行 IO 阻塞操作，如查询数据库进入阻塞状态； 被转换成Blocked状态，比如获取某个锁的释放，而被加入该锁的阻塞队列中； 该线程的时间片用完，CPU 再次调度，进入Runnable状态； 线程主动调用 yield 方法，让出 CPU 资源，进入Runnable状态 Blocked状态的线程能够发生哪些状态改变？ 被转换成Terminated状态，比如调用 stop() 方法，或者是 JVM 意外 Crash; 被转换成Runnable状态，阻塞时间结束，比如读取到了数据库的数据后； 完成了指定时间的休眠，进入到Runnable状态； 正在wait中的线程，被其他线程调用notify/notifyAll方法唤醒，进入到Runnable状态； 线程获取到了想要的锁资源，进入Runnable状态； 线程在阻塞状态下被打断，如其他线程调用了interrupt方法，进入到Runnable状态； 哪些情况进入终止状态 线程正常运行结束，生命周期结束； 线程运行过程中出现意外错误； JVM 异常结束，所有的线程生命周期均被结束。 2.死锁问题 什么是死锁死锁就是有两个或者多个进程由于竞争资源而造成阻塞的现象，如果无外力作用，这种局面就会一直持续下去 死锁产生的条件死锁产生必须满足四个必要条件： 1、互斥条件：指在一段时间内某资源只能由一个进程占用。 1只有一副钥匙 2、请求和保持条件：指进程已经保持至少一个资源，但又提出了新的资源请求，且对自己已获得的其它资源保持不放。 1拿着红钥匙的人在没有归还红钥匙的情况下，又索要蓝钥匙 3、不剥夺条件：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放。 1只要人不主动归还钥匙，就可以一直占着钥匙 4、环路等待条件：指在发生死锁时，必然存在一个进程——资源的环形链。即进程集合{P0，P1，P2，···，Pn}中的P0正在等待一个P1占用的资源；P1正在等待P2占用的资源，……，Pn正在等待已被P0占用的资源。 1拿着红钥匙的人在等待蓝钥匙，而拿着蓝钥匙的人又在等待红钥匙 如何避免死锁 资源一次性分配：一次性分配所有资源，这样就不会再有请求了：（破坏请求条件） 每个进程提出申请资源前必须释放已占有的一切资源（破坏保持条件） 可剥夺资源：即当某进程获得了部分资源，但得不到其它资源，则释放已占有的资源（破坏不可剥夺条件） 资源有序分配法：系统给每类资源赋予一个编号，每一个进程按编号递增的顺序请求资源，释放则相反（破坏环路等待条件） 编程中的最佳实践： 使用 Lock 的 tryLock(long timeout, TimeUnit unit)的方法，设置超时时间，超时可以退出防止死锁 尽量使用并发工具类代替加锁 尽量降低锁的使用粒度 尽量减少同步的代码块 死锁检测工具（了解）1、Jstack命令 jstack是java虚拟机自带的一种堆栈跟踪工具。jstack用于打印出给定的java进程ID或core file或远程调试服务的Java堆栈信息。 Jstack工具可以用于生成java虚拟机当前时刻的线程快照。线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等。 线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。 2、JConsole工具 Jconsole是JDK自带的监控工具，在JDK/bin目录下可以找到。它用于连接正在运行的本地或者远程的JVM，对运行在Java应用程序的资源消耗和性能进行监控，并画出大量的图表，提供强大的可视化界面。而且本身占用的服务器内存很小，甚至可以说几乎不消耗。 3.单例模式 单例模式含义单例模式是指在内存中只会创建且仅创建一次对象的设计模式。在程序中多次使用同一个对象且作用相同时，为了防止频繁地创建对象使得内存飙升，单例模式可以让程序仅在内存中创建一个对象，让所有需要调用的地方都共享这一单例对象。 单例模式主要解决的问题是一个全局使用的类，不会被频繁的创建和销毁，从而提升代码的整体性能。 如何创建单例模式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556饿汉式优点：简单，线程安全缺点：不管有没有使用都会占据空间public class HungrySingleton &#123; private static final HungrySingleton SINGLETON = new HungrySingleton(); /** * 单例模式有一个特点，不允许外部直接创建对象，私有构造不让外部实例化 */ private HungrySingleton() &#123;&#125; public static HungrySingleton getInstance() &#123; return SINGLETON; &#125;&#125;懒汉式优点: 简单缺点: 线程不安全，要加锁才能解决public class LazySingleton &#123; private static LazySingleton singleton; private LazySingleton()&#123;&#125; public static LazySingleton getInstance()&#123; if(singleton == null)&#123; singleton = new LazySingleton() return singleton; &#125; return singleton; &#125;&#125;加锁后的public static LazySingleton getInstance() &#123; if (singleton == null) &#123; synchronized (LazySingleton.class) &#123; if (singleton == null) &#123; singleton = new LazySingleton() return new singleton(); &#125; &#125; &#125; return singleton;&#125;枚举（推荐）优点：线程安全，防止反射和反序列化public enum EnumSingleton &#123; INSTANCE; public EnumSingleton getInstance()&#123; return INSTANCE; &#125;&#125; 在单例里面定义一个全局变量或者类变量的，它线程安全的还是线程不安全的结论:有写操作的话都是线程不安全的 静态变量即类变量，只初始化一次，位于方法区，为所有对象共享，共享一份内存，一旦静态变量被修改，其他对象均对修改可见，故线程非安全。 全局变量即实例成员变量。如果线程只是读取变量的值，而不会改变变量的值，则无论是单例还是非单例都是线程安全的；如果有修改变量值的操作，则单例模式因为只有一个对象实例singleton存在，多线程同时操作时是不安全的，而非单例模式下多线程操作是安全的。 4.怎么解决高并发问题 1.优化代码 2.设置单独的图片服务器，减少访问请求服务器压力 3.使用缓存 4.使用数据库集群 5.DB优化（索引优化，字段类型恰当） 6.实现负载均衡 7.限流","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"juc","slug":"juc","permalink":"https://gwtt.github.io/tags/juc/"}]},{"title":"Java面试基础（2）","slug":"Java面试基础(2)","date":"2022-07-02T06:37:00.000Z","updated":"2022-07-09T14:38:29.285Z","comments":true,"path":"2022/07/02/Java面试基础(2)/","link":"","permalink":"https://gwtt.github.io/2022/07/02/Java%E9%9D%A2%E8%AF%95%E5%9F%BA%E7%A1%80(2)/","excerpt":"","text":"java基础(2)1..java你用过的jdk版本 jdk1.8或者jdk11 关键问: 你知道jdk1.8相较于1.7有什么新特性吗 新增Lambda表达式 ：Lambda允许把函数作为一个方法的参数（函数作为参数传递到方法中）。 新增Stream API:对容器功能加强，可以对容器进行高效操作 新增方法引用:通过方法的名字来指向一个方法，用:: 新增Optional类:解决空指针异常 新增default:接口可以有具体方法 新增日期API (接下来可能会问Lambda表达式和StreamAPI) 你知道jdk11有什么新特性吗 新增本地类型推断:用var直接定义变量，编译器能自动推断类型 Optianal类增强，可以转换成流使用 字符类增加一些功能API 2.用过final关键字吗？它有什么作用 final关键字表示不可变，它可以修饰在类、方法、成员变量中。 如果修饰在类上，则表示该类不允许被继承 修饰在方法上，表示该方法无法被重写 修饰在变量上，表示该变量无法被修改，而且JVM会隐性定义为一个常量。(但是可以通过反射去破坏，但是反射无法修改被final和static同时修饰的变量) 另外，final修饰的关键字，还可以避免因为指令重排序带来的可见性问题，原因是，final遵循两个重排序规则 构造函数内，对一个 final 变量的写入，与随后把这个被构造对象的引用赋值给一个变量，这两个操作之间不可重排序。 首次读一个包含 final 变量的对象，与随后首次读这个 final 变量，这两个操作之间不可以重排序。 3.反射知道吗 通过反射可以获取到一些什么东西 反射是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为 Java 语言的反射机制。 ① 通过反射获取类中的属性和属性值 ② 通过反射获取反射类中的构造方法 ③ 通过反射获取反射类的父类（超类）和接口 ④通过反射获取反射类的方法 4.序列化ID有什么作用？怎么生成的？ 作用:验证版本一致性 反序列化时，程序会比较磁盘中的序列化版本号ID是否与当前类结构生成的版本号ID一致，如果一致，则反序列化成功 下图就是反序列化异常情况 生成方式： 首先，相关的类要实现Serializable接口 手动生成 12private static final long serialVersionUID = 1L;好处就是即使在原来类上加字段，也不会发生上图的反序列化失败，老版本兼容 代码调用 12ObjectStreamClass c = ObjectStreamClass.lookup(Student.class);long serialVersionUID = c.getSerialVersionUID(); ide快捷工具 5.为什么重写 equals 的同时必须重写 hashCode 程序先进行 hashcode 的比较，如果不同，那没就不必在进行 equals 的比较了，这样就大大减少了 equals 比较的次数，这对比需要比较的数量很大的效率提高是很明显的，一个很好的例子就是在集合中的使用 hashCode 和 equals 两个方法是用来协同判断两个对象是否相等的，采用这种方式的原因是可以提高程序插入和查询的速度，如果在重写 equals 时，不重写 hashCode，就会导致在某些场景下，例如将两个相等的自定义对象存储在 Set 集合时，就会出现程序执行的异常，为了保证程序的正常执行，所以我们就需要在重写 equals 时，也一并重写 hashCode 方法才行。 回答:因为程序会先进行hashcode，我们希望Set集合能过滤相同数值的对象，因为涉及对象的值比较，我们重写equals，但我们要保证两个对象 equals 相等，那么hashcode也要相同，所以要重写hashcode，否则还是不能去重。（hashcode不是内存地址，跟内存地址相关）","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"java基础","slug":"java基础","permalink":"https://gwtt.github.io/tags/java%E5%9F%BA%E7%A1%80/"}]},{"title":"消息队列面试","slug":"消息队列面试","date":"2022-06-12T15:15:44.249Z","updated":"2022-08-02T14:57:20.569Z","comments":true,"path":"2022/06/12/消息队列面试/","link":"","permalink":"https://gwtt.github.io/2022/06/12/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E9%9D%A2%E8%AF%95/","excerpt":"","text":"消息队列1.消息队列有哪些作用 1．解耦:使用消息队列来作为两个系统直接的通讯方式，两个系统不需要相互依赖了 2．异步:系统A给消费队列发送完消息之后，就可以继续做其他事情了 3．流量削峰:如果使用消息队列的方式来调用某个系统，那么消息将在队列中排队，有消费者自己控制消费速度 2.死信队列和延迟队列是什么 1．死信队列也是一个消息队列，它是用来存放那些没有成功消费的消息的，通常可以用来作为消息重试2．延时队列就是用来存放需要在指定时间被处理的元素的队列，通常可以用来处理一些具有过期性操作的业务，比如十分钟内未支付则取消订单 3.Kafka的Pull和Push分别有什么优缺点 pull表示消费者主动拉取，可以批量拉取，也可以单条拉取，所以pull可以由消费者自己控制，根据自己的消息处理能力来进行控制，但是消费者不能及时知道是否有消息，可能会拉到的消息为空 push表示Broker主动给消费者推送消息，所以肯定是有消息时才会推送，但是消费者不能按自己的能力来消费消息，推过来多少消息，消费者就得消费多少消息，所以可能会造成网络堵塞，消费者压力大等问题 4.Kafka为什么比RocketMQ的吞吐量要高 Kafka的生产者采用的是异步发送消息机制，当发送一条消息时，消息并没有发送到Broker而是缓存起来，然后直接向业务返回成功，当缓存的消息达到一定数量时再批量发送给Broker。这种做法减少了网络io，从而提高了消息发送的吞吐量，但是如果消息生产者宕机，会导致消息丢失，业务出错，所以理论上kafka利用此机制提高了性能却降低了可靠性。 5.RocketMQ的底层实现原理 RocketMQ由NameServer集群、Producer集群、Consumer集群、Broker集群组成，消息生产和消费的大致原理如下: Broker在启动的时候向所有的NameServer注册，并保持长连接，每30s发送一次心跳 Producer在发送消息的时候从NameServer获取Broker服务器地址，根据负载均衡算法选择一台服务器来发送消息 Conusmer消费消息的时候同样从NameServer获取Broker地址，然后主动拉取消息来消费 6.消息队列如何保证消息可靠传输 为了保证消息不多，也就是消息不能重复，也就是生产者不能重复生产消息，或者消费者不能重复消费消息 首先要确保消息不多发，这个不常出现，也比较难控制，因为如果出现了多发，很大的原因是生产者自己的原因，如果要避免出现问题，就需要在消费端做控制 要避免不重复消费，最保险的机制就是消费者实现幂等性，保证就算重复消费，也不会有问题，通过幂等性，也能解决生产者重复发送消息的问题 消息不能少，意思就是消息不能丢失，生产者发送的消息，消费者一定要能消费到，对于这个问题，就要考虑两个方面 生产者发送消息时，要确认broker确实收到并持久化了这条消息，比如RabbitMQ的confirm机制，Kafka的ack机制都可以保证生产者能正确的将消息发送给broker broker要等待消费者真正确认消费到了消息时才删除掉消息，这里通常就 是消费端ack机制，消费者接收到一条消息后，如果确认没问题了，就可以给broker发送一个ack，broker接收到ack后才会删除消息 7.消息丢失怎么办 丢失我们从三个角度出发(以下是RabbitMQ案例 ) 消费者丢数据 此时可以选择用 RabbitMQ 提供的事务功能，就是生产者发送数据之前开启 RabbitMQ 事务channel.txSelect，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务 channel.txRollback，然后重试发送消息；如果收到了消息，那么可以提交事务channel.txCommit。 缺点:吞吐量减少 所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。 事务机制和 confirm 机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是confirm 机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用 confirm 机制的。 消息队列丢数据 就是 RabbitMQ 自己弄丢了数据，这个你必须开启 RabbitMQ 的持久化，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，可能导致少量数据丢失，但是这个概率较小。 消费端丢数据 RabbitMQ 如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。 这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 ack，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。 8.消息重复消费怎么办? 重复消费出现场景（消费完后没有返回状态然后重启重新消费）: Kafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后consumer 消费了数据之后，每隔一段时间（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。 但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset，尴尬了。重启之后，少数消息会再次消费一次。 如果消费者干的事儿是拿一条数据就往数据库里写一条，会导致说，你可能就把数据 1/2 在数据库里插入了 2 次，那么数据就错啦。 其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性。 举个例子吧。假设你有个系统，消费一条消息就往数据库里插入一条数据，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？但是你要是消费到第二次的时候，自己判断一下是否已经消费过了，若是就直接扔了，这样不就保留了一条数据，从而保证了数据的正确性。 一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性。 幂等性，通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。 如何保证幂等性 数据库实现主键唯一 redis天然幂等性 唯一键和redis结合","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"消息队列","slug":"消息队列","permalink":"https://gwtt.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]},{"title":"分布式面试","slug":"分布式面试","date":"2022-06-12T15:14:14.872Z","updated":"2022-07-04T11:47:11.923Z","comments":true,"path":"2022/06/12/分布式面试/","link":"","permalink":"https://gwtt.github.io/2022/06/12/%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95/","excerpt":"","text":"分布式1.什么是CAP理论 CAP理论是分布式领域中非常重要的一个指导理论，C(Consistency)表示强一致性，A(Availability)表示可用性，P(Partition Tolerance)表示分区容错性，CAP理论指出在目前的硬件条件下，一个分布式系统是必须要保证分区容错性的，而在这个前提下，分布式系统要么保证C，要么保证AP，无法同时保证CAP。 分区容错性表示，一个系统虽然是分布式的，但是对外看上去应该是一个整体，不能由于分布式系统内部的某个结点挂点，或网络出现了故障，而导致系统对外出现异常。所以，对于分布式系统而言是一定要保证分区容错性的。 强一致性表示，一个分布式系统中各个结点之间能及时的同步数据，在数据同步过程中，是不能对外提供服务的，不然就会造成数据不一致，所以强一致性和可用性是不能同时满足的。 可用性表示，一个分布式系统对外要保证可用。 2.什么是BASE理论 由于不能同时满足CAP，所以出现了BASE理论:1.BA: Basically Available，表示基本可用，表示可以允许一定程度的不可用，比如由于系统故障，请求时间变长，或者由于系统故障导致部分非核心功能不可用，都是允许的 2.S: Soft state:表示分布式系统可以处于一种中间状态，比如数据正在同步 3.E:Eventually consistent，表示最终一致性，不要求分布式系统数据实时达到一致，允许在经过一段时间后再达到一致，在达到一致过程中系统也是可用的 3.什么是RPC 什么是 RPC ？ RPC (Remote Procedure Call)即远程过程调用，是分布式系统常见的一种通信方法。它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。 除 RPC 之外，常见的多系统数据交互方案还有分布式消息队列、HTTP 请求调用、数据库和分布式缓存等。 其中 RPC 和 HTTP 调用是没有经过中间件的，它们是端到端系统的直接数据交互。 简单的说 RPC就是从一台机器（客户端）上通过参数传递的方式调用另一台机器（服务器）上的一个函数或方法（可以统称为服务）并得到返回的结果。 RPC会隐藏底层的通讯细节（不需要直接处理Socket通讯或Http通讯）。 客户端发起请求，服务器返回响应（类似于Http的工作方式）RPC在使用形式上像调用本地函数（或方法）一样去调用远程的函数（或方法）。 4.什么是分布式ID 在分布式系统中，经常需要一些全局唯一的ID对数据、消息、http请求等进行唯一标识。那么这个全局唯一ID就叫分布式ID 5.为什么需要分布式ID 1.如果id我们使用的是数据库的自增长类型，在分布式系统中需要分库和分表时，会有两个相同的表，有可能产生主键冲突。 2.电商订单号，采用自增方式，是最简单的生成规则。但是！这种与流水号相同的订单号很容易就被竞争对手看出你公司真实的运营信息。 6.分布式ID解决方案 1.uuid，这种方案复杂度最低，但是会影响存储空间和性能 ⒉利用单机数据库的自增主键，作为分布式ID的生成器，复杂度适中，ID长度较之uuid更短，但是受到单机数据库性能的限制，并发量大的时候此方案也不是最优方案 3.利用redis、zookeeper的特性来生成id，比如redis的自增命令、zookeeper的顺序节点，这种方案和单机数据库(mysql)相比，性能有所提高,可以适当选用 4.雪花算法，一切问题如果能直接用算法解决，那就是最合适的，利用雪花算法也可以生成分布式ID，底层原理就是通过某台机器在某一毫秒内对某一个数字自增，这种方案也能保证分布式架构中的系统id唯一，但是只能保证趋势递增。业界存在tinyid、 leaf等开源中间件实现了雪花算法 7.分布式锁的使用场景和实现方案 在单体架构中，多个线程都是属于同一个进程的，所以在线程并发执行时，遇到资源竞争时，可以利用ReentrantLock、synchronized等技术来作为锁，来控制共享资源的使用。 而在分布式架构中，多个线程是可能处于不同进程中的，而这些线程并发执行遇到资源竞争时，利用ReentrantLock、synchronized等技术是没办法来控制多个进程中的线程的，所以需要分布式锁，意思就是，需要一个分布式锁生成器，分布式系统中的应用程序都可以来使用这个生成器所提供的锁，从而达到多个进程中的线程使用同一把锁。 目前主流的分布式锁的实现方案有两种: 1.zookeeper:利用的是zookeeper的临时节点、顺序节点、watch机制来实现的，zookeeper分布式锁的特点是高一致性，因为zookeeper保证的是CP，所以由它实现的分布式锁更可靠，不会出现混乱 2.redis:利用redis的setnw、lua脚本、消费订阅等机制来实现的，redis分布式锁的特点是高可用，因为redis保证的是AP，所以由它实现的分布式锁可能不可靠，不稳定(一旦redis中的数据出现了不一致)，可能会出现多个客户端同时加到锁的情况 8.分布式事务和实现实现方案 在分布式系统中，一次业务处理可能需要多个应用来实现，比如用户发送一次下单请求，就涉及到订单系统创建订单、库存系统减库存，而对于一次下单，订单创建与减库存应该是要同时成功或同时失败的，但在分布式系统中，如果不做处理，就很有可能出现订单创建成功，但是减库存失败，那么解决这类问题，就需要用到分布式事务。常用解决方案有: 1.本地消息表:创建订单时，将减库存消息加入在本地事务中，一起提交到数据库存入本地消息表，然后调用库存系统，如果调用成功则修改本地消息状态为成功，如果调用库存系统失败，则由后台定时任务从本地消息表中取出未成功的消息，重试调用库存系统 2.消息队列:目前RocketMQ中支持事务消息，它的工作原理是: a.生产者订单系统先发送一条half消息到Broker，half消息对消费者而言是不可见的 b.再创建订单，根据创建订单成功与否，向Broker发送commit或rollback c.并且生产者订单系统还可以提供Broker回调接口，当Broker发现一段时间half消息没有收到任何操作命令，则会主动调此接口来查询订单是否创建成功 d.一旦half消息commit了，消费者库存系统就会来消费，如果消费成功，则消息销毁，分布式事务成功结束 e.如果消费失败，则根据重试策略进行重试，最后还失败则进入死信队列，等待进一步处理 3.Seata:阿里开源的分布式事务框架，支持AT、TCC等多种模式，底层都是基于两阶段提交理论来实现的 9.简述zab协议 ZAB协议（Zookeeper Atomic Broadcast原子广播）是Zookeeper用来实现一致性的原子广播协议，该协议描述了Zookeeper是如何实现一致性的，分为三个阶段: 1.领导者选举阶段:从Zookeeper集群中选出一个节点作为Leader，所有的写请求都会由Leader节点来处理 ⒉数据同步阶段:集群中所有节点中的数据要和Leader节点保持一致，如果不一致则要进行同步 3.请求广播阶段:当Leader节点接收到写请求时，会利用两阶段提交来广播该写请求，使得写请求像事务一样在其他节点上执行，达到节点上的数据实时一致 但值得注意的是，Zookeeper只是尽量的在达到强一致性，实际上仍然只是最终一致性的。 10.Spring Cloud和Dubbo有哪些区别? Spring Cloud是一个微服务框架，提供了微服务领域中的很多功能组件，Dubbo一开始是一个RPC调用框架，核心是解决服务调用间的问题，Spring Cloud是一个大而全的框架，Dubbo则更侧重于服务调用，所以Dubbo所提供的功能没有SpringCloud全面，但是Dubbo的服务调用性能比Spring Cloud高，不过Spring Cloud和Dubbo并不是对立的，是可以结合起来一起使用的。 11.什么是服务雪崩?什么是服务限流? 1.当服务A调用服务B，服务B调用C，此时大量请求突然请求服务A，假如服务A本身能抗住这些请求，但是如果服务C抗不住，导致服务C请求堆积，从而服务B请求堆积，从而服务A不可用，这就是服务雪崩，解决方式就是服务降级和服务熔断。2．服务限流是指在高并发请求下，为了保护系统，可以对访问服务的请求进行数量上的限制，从而防止系统不被大量请求压垮，在秒杀中，限流是非常重要的。 12.什么是服务熔断?什么是服务降级?区别是什么? 1.服务熔断是指，当服务A调用的某个服务B不可用时，上游服务A为了保证自己不受影响，从而不再调用服务B，直接返回一个结果，减轻服务A和服务B的压力，直到服务B恢复。 2.服务降级是指，当发现系统压力过载时，可以通过关闭某个服务，或限流某个服务来减轻系统压力，这就是服务降级。 相同点: 1．都是为了防止系统崩溃 2．都让用户体验到某些功能暂时不可用 不同点:熔断是下游服务故障触发的，降级是为了降低系统负载","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"分布式","slug":"分布式","permalink":"https://gwtt.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"Mybatis面试(1)","slug":"Mybatis面试(1)","date":"2022-06-12T15:12:55.815Z","updated":"2022-07-02T09:39:28.585Z","comments":true,"path":"2022/06/12/Mybatis面试(1)/","link":"","permalink":"https://gwtt.github.io/2022/06/12/Mybatis%E9%9D%A2%E8%AF%95(1)/","excerpt":"","text":"Mybatis(1)1.Mybatis的优缺点 Mybaits 的优点： 1234567891.基于 SQL 语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任 何影响，SQL 写在 XML 里，解除 sql 与程序代码的耦合，便于统一管理；提供 XML 标签，支持编写动态 SQL 语句，并可重用。2.与 JDBC 相比，减少了 50%以上的代码量，消除了 JDBC 大量冗余的代码，不 需要手动开关连接；3.很好的与各种数据库兼容（因为 MyBatis 使用 JDBC 来连接数据库，所以只要 JDBC 支持的数据库 MyBatis 都支持）。4.能够与 Spring 很好的集成；5.提供映射标签，支持对象与数据库的 ORM 字段关系映射；提供对象关系映射 标签，支持对象关系组件维护。 MyBatis 框架的缺点： 1231.SQL 语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写 SQL 语句的功底有一定要求。2.SQL 语句依赖于数据库，导致数据库移植性差，不能随意更换数据库。 2.Mybatis中的#{}和${}区别是什么 1.#{}是预编译处理、是占位符，${}是字符串替换、是拼接符 2.Mybatis在处理#{}时，会将sql中的#替换为?号，调用PreparedStatement来赋值 3.Mybatis在处理${}时，就是把$替换成变量的值，调用Statement来赋值 4.使用#{}可以很大程度上防止SQL注入，提高系统安全性 5.#号防止sql注入的原理就是对参数及参数中的特殊字符进行了转义，从而达到防止sql注入的效果。 3.ORM框架是什么 对象关系映射（Object—Relational Mapping，简称ORM） 是一种为了解决面向对象与面向关系数据库存在的互不匹配的现象的技术；简单的说，ORM 是通过使用描述对象和数据库之间映射的元数据，将java 程序中的对象自动持久化到关系数据库中；本质上就是将数据从一种形式转换到另外一种形式。 常见的比如Hibernate, Mybatis,TopLink等框架","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Mybatis","slug":"Mybatis","permalink":"https://gwtt.github.io/tags/Mybatis/"}]},{"title":"Redis面试","slug":"Redis面试","date":"2022-06-12T15:11:30.900Z","updated":"2022-08-17T12:28:18.886Z","comments":true,"path":"2022/06/12/Redis面试/","link":"","permalink":"https://gwtt.github.io/2022/06/12/Redis%E9%9D%A2%E8%AF%95/","excerpt":"","text":"Redis1.redis 的持久化有哪几种方式？不同的持久化机制都有什么优缺点？持久化机制具体底层是如何实现的？ redis 持久化的两种方式 RDB：RDB 持久化机制，是对 redis 中的数据执行周期性的持久化。 AOF（append only file）：AOF 机制对每条写入命令作为日志，以 append-only 的模式写入一个日志文件中，在 redis 重启的时候，可以通过回放 AOF 日志中的写入指令来重新构建整个数据集。 RDB 优缺点 RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 redis 的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说 Amazon 的 S3 云服务上去，在国内可以是阿里云的 ODPS 分布式存储上，以预定好的备份策略来定期备份 redis 中的数据。 RDB 对 redis 对外提供的读写服务，影响非常小，可以让 redis 保持高性能，因为 redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 redis 进程，更加快速。 如果想要在 redis 故障时，尽可能少的丢失数据，那么 RDB 没有 AOF 好。一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，这个时候就得接受一旦 redis 进程宕机，那么会丢失最近 5 分钟的数据。 RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。 AOF 优缺点 AOF 可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次fsync操作，最多丢失 1 秒钟的数据。 AOF 日志文件以 append-only 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。 AOF 日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。 AOF 日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用 flushall 命令清空了所有数据，只要这个时候后台 rewrite 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 flushall 命令给删了，然后再将该 AOF 文件放回去，就可以通过恢复机制，自动恢复所有数据。 对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大。 AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 fsync 一次日志文件，当然，每秒一次 fsync，性能也还是很高的。（如果实时写入，那么 QPS 会大降，redis 性能会大大降低） 以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志 / merge / 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 RDB 和 AOF 到底该如何选择 不要仅仅使用 RDB，因为那样会导致你丢失很多数据； 也不要仅仅使用 AOF，因为那样有两个问题：第一，你通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug； redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。 2.说说缓存雪崩、缓存击穿、缓存穿透吧，解决办法？ 缓存雪崩出现过程 假设有如下一个系统，高峰期请求为5000次/秒，4000次走了缓存，只有1000次落到了数据库上，数据库每秒1000的并发是一个正常的指标，完全可以正常工作，但如果缓存宕机了，或者缓存设置了相同的过期时间，导致缓存在同一时刻同时失效，每秒5000次的请求会全部落到数据库上，数据库立马就死掉了，因为数据库一秒最多抗2000个请求，如果DBA重启数据库，立马又会被新的请求打死了，这就是缓存雪崩。 解决方法 事前：redis高可用，主从+哨兵，redis cluster，避免全盘崩溃 事中：本地ehcache缓存 + hystrix限流&amp;降级，避免MySQL被打死 事后：redis持久化RDB+AOF，快速恢复缓存数据 缓存的失效时间设置为随机值，避免同时失效 缓存穿透出现过程 假如客户端每秒发送5000个请求，其中4000个为黑客的恶意攻击，即在数据库中也查不到。举个例子，用户id为正数，黑客构造的用户id为负数，如果黑客每秒一直发送这4000个请求，缓存就不起作用，数据库也很快被打死。 解决方法 对请求参数进行校验，不合理直接返回 查询不到的数据也放到缓存，value为空，如 set -999 “” 使用布隆过滤器，快速判断key是否在数据库中存在，不存在直接返回 缓存击穿出现过程 设置了过期时间的key，承载着高并发，是一种热点数据。从这个key过期到重新从MySQL加载数据放到缓存的一段时间，大量的请求有可能把数据库打死。缓存雪崩是指大量缓存失效，缓存击穿是指热点数据的缓存失效 解决方法 设置key永远不过期，或者快过期时，通过另一个异步线程重新设置key 当从缓存拿到的数据为null，重新从数据库加载数据的过程上锁，下面写个分布式锁实现的demo 1231，缓存雪崩:如果缓存中某一时刻大批热点数据同时过期，那么就可能导致大量请求直接访问Mysql了，解决办法就是在过期时间上增加一点随机值，另外如果搭建一个高可用的Redis集群也是防止缓存雪崩的有效手段2．缓存击穿:和缓存雪崩类似，缓存雪崩是大批热点数据失效，而缓存击穿是指某一个热点key突然失效，也导致了大量请求直接访问Mysql数据库，这就是缓存击穿，解决方案就是考虑这个热点key不设过期时间3．缓存穿透:假如某一时刻访问redis的大量key都在redis中不存在(比如黑客故意伪造一些乱七八糟的key)，那么也会给数据造成压力，这就是缓存穿透，解决方案是使用布隆过滤器，它的作用就是如果它认为一个key不存在，那么这个key就肯定不存在，所以可以在缓存之前加一层布隆过滤器来拦截不存在的key 3.redis为什么快 1、Redis 是一款纯内存结构，避免了磁盘 I/O 等耗时操作。 2、Redis 命令处理的核心模块为单线程，减少了锁竞争，以及频繁创建线程和销毁线程的代价，减少了线程上下文切换的消耗。 3、采用了 I/O 多路复用机制，大大提升了并发效率。 4.redis缓存淘汰策略 Redis内存不足的缓存淘汰策略提供了8种。noeviction：当内存使用超过配置的时候会返回错误，不会驱逐任何键allkeys-lru：加入键的时候，如果过限，首先通过LRU算法驱逐最久没有使用的键volatile-lru：加入键的时候如果过限，首先从设置了过期时间的键集合中驱逐最久没有使用的键allkeys-random：加入键的时候如果过限，从所有key随机删除volatile-random：加入键的时候如果过限，从过期键的集合中随机驱逐volatile-ttl：从配置了过期时间的键中驱逐马上就要过期的键volatile-lfu：从所有配置了过期时间的键中驱逐使用频率最少的键allkeys-lfu：从所有键中驱逐使用频率最少的键 12345678这八种大体上可以分为4中，lru、lfu、random、ttl。lru：Least Recently Used)，最近最少使用lfu：Least Frequently Used，最不经常使用法ttl：Time To Live，生存时间random：随机默认是noeviction。对于写请求不再提供服务，直接返回错误（DEL请求和部分特殊请求除外eviction：“逐出；赶出；收回”。volatile：“不稳定的”。 Redis默认的过期策略是noeviction, 最暴力那个, 如果内存满了那就是一场“华丽”的故事了。 5.Redis有哪些数据结构？分别有哪些典型的应用场景? Redis的数据结构有: 字符串:可以用来做最简单的数据缓存，可以缓存某个简单的字符串，也可以缓存某个json格式的字符串，Redis分布式锁的实现就利用了这种数据结构，还包括可以实现计数器、Session共享、分布式ID 哈希表:可以用来存储一些key-value对，更适合用来存储对象 列表: Redis的列表通过命令的组合，既可以当做栈，也可以当做队列来使用，可以用来缓存类似微信公众号、微博等消息流数据 集合:和列表类似，也可以存储多个元素，但是不能重复，集合可以进行交集、并集、差集操作，从而可以实现类似,我和某人共同关注的人、朋友圈点赞等功能 有序集合:集合是无序的，有序集合可以设置顺序，可以用来实现排行榜功能 6.Redis分布式锁底层是如何实现的 首先利用setnx来保证:如果key不存在才能获取到锁，如果key存在，则获取不到锁 然后还要利用lua脚本来保证多个redis操作的原子性 同时还要考虑到锁过期，所以需要额外的一个看门狗定时任务来监听锁是否需要续约 同时还要考虑到redis书点挂掉后的情况，所以需要采用红锁的方式来同时向N/2+1个节点申请锁，都申请到了才证明获取锁成功，这样就算其中某个redis节点挂掉了，锁也不能被其他客户端获取到 7.Redis和Mysql如何保证数据一致 1．先更新Mysql，再更新Redis，如果更新Redis失败，可能仍然不一致 2．先删除Redis缓存数据，再更新Mysql，再次查询的时候在将数据添加到缓存中，这种方案能解决1方案的问题，但是在高并发下性能较低，而且仍然会出现数据不一致的问题，比如线程1删除了Redis缓存数据，正在更新Mysql，此时另外一个查询再查询，那么就会把Mysql中老数据又查到Redis中 3，延时双删，步骤是:先删除Redis缓存数据，再更新Mysql，延迟几百毫秒再删除Redis缓存数据，这样就算在更新Mysql时，有其他线程读了Mysql，把老数据读到了Redis中，那么也会被删除掉，从而把数据保持一致 8.redis的应用场景？ 1.缓存 缓存现在几乎是所有中大型网站都在用的必杀技，合理的利用缓存不仅能够提升网站访问速度，还能大大降低数据库的压力。Redis提供了键过期功能，也提供了灵活的键淘汰策略，所以，现在Redis用在缓存的场合非常多。 2.排行榜 很多网站都有排行榜应用的，如淘宝的月度销量榜单、商品按时间的上新排行榜等。Redis提供的有序集合数据类构能实现各种复杂的排行榜应用。 3.计数器什么是计数器，如电商网站商品的浏览量、视频网站视频的播放数等。为了保证数据实时效，每次浏览都得给+1，并发量高时如果每次都请求数据库操作无疑是种挑战和压力。Redis提供的incr命令来实现计数器功能，内存操作，性能非常好，非常适用于这些计数场景。 4.分布式会话集群模式下，在应用不多的情况下一般使用容器自带的session复制功能就能满足，当应用增多相对复杂的系统中，一般都会搭建以Redis等内存数据库为中心的session服务，session不再由容器管理，而是由session服务及内存数据库管理。 5.分布式锁在很多互联网公司中都使用了分布式技术，分布式技术带来的技术挑战是对同一个资源的并发访问，如全局ID、减库存、秒杀等场景，并发量不大的场景可以使用数据库的悲观锁、乐观锁来实现，但在并发量高的场合中，利用数据库锁来控制资源的并发访问是不太理想的，大大影响了数据库的性能。可以利用Redis的setnx功能来编写分布式的锁，如果设置返回1说明获取锁成功，否则获取锁失败，实际应用中要考虑的细节要更多。 6.朋友圈点赞 点赞、踩、关注/被关注、共同好友等是社交网站的基本功能，社交网站的访问量通常来说比较大，而且传统的关系数据库类型不适合存储这种类型的数据，Redis提供的哈希、集合等数据结构能很方便的的实现这些功能。 7.最新消息 Redis列表结构，LPUSH可以在列表头部插入一个内容ID作为关键字，LTRIM可用来限制列表的数量，这样列表永远为N个ID，无需查询最新的列表，直接根据ID去到对应的内容页即可。 8.消息队列 消息队列是大型网站必用中间件，如ActiveMQ、RabbitMQ、Kafka等流行的消息队列中间件，主要用于业务解耦、流量削峰及异步处理实时性低的业务。Redis提供了发布/订阅及阻塞队列功能，能实现一个简单的消息队列系统。另外，这个不能和专业的消息中间件相比","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Redis","slug":"Redis","permalink":"https://gwtt.github.io/tags/Redis/"}]},{"title":"MySQL面试(1)","slug":"Mysql面试(1)","date":"2022-06-12T15:09:55.412Z","updated":"2022-09-05T10:51:51.765Z","comments":true,"path":"2022/06/12/Mysql面试(1)/","link":"","permalink":"https://gwtt.github.io/2022/06/12/Mysql%E9%9D%A2%E8%AF%95(1)/","excerpt":"","text":"MySQL(1) 1.$跟#的区别 #相当于对数据 加上 双引号，$相当于直接显示数据。 2.MySQL的事务隔离级别有哪些？默认隔离级别是？ READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。(默认) SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 12345脏读：一个事务读取另外一个事务还没有提交的数据。不可重复读：事务 T1 读到某行；事务 T2 修改或删除这行，提交事务；T1 重新读取发现这行数据已经被修改或删除。幻读：事务 T1 读取了 N 行；事务 T2 在事务 T1 读取的条件范围内生成了一行或多行数据；T1 重新读取获得与之前不同集合的行数据。 3.explain语句结果各个字段分别表示什么 Column 含义 id 查询序号 select_type 查询类型 table 表名 partitions 匹配的分区 type join类型 prossible_keys 可能会选择的索引 key 实际选择的索引 key_len 索引的长度 ref 与索引作比较的列 rows 要检索的行数(估算值) filtered 查询条件过滤的行数的百分比 Extra 额外信息 id: SQL查询中的序列号。 select_type: 查询的类型，可以是下表的任何一种类型： select_type 类型说明 SIMPLE 简单SELECT(不使用UNION或子查询) PRIMARY 最外层的SELECT UNION UNION中第二个或之后的SELECT语句 DEPENDENT UNION UNION中第二个或之后的SELECT语句取决于外面的查询 UNION RESULT UNION的结果 SUBQUERY 子查询中的第一个SELECT DEPENDENT SUBQUERY 子查询中的第一个SELECT, 取决于外面的查询 DERIVED 衍生表(FROM子句中的子查询) MATERIALIZED 物化子查询 UNCACHEABLE SUBQUERY 结果集无法缓存的子查询，必须重新评估外部查询的每一行 UNCACHEABLE UNION UNION中第二个或之后的SELECT，属于无法缓存的子查询 table 表名或者表的别名。 partitions 分区信息，非分区表为null。 type 访问类型，表示找到所查询数据的方法，也是本文重点介绍的属性。该属性的常见值如下，性能从好到差： NULL：无需访问表或者索引，比如获取一个索引列的最大值或最小值。 system/const：当查询最多匹配一行时，常出现于where条件是＝的情况。system是const的一种特殊情况，既表本身只有一行数据的情况。 eq_ref：多表关联查询时，根据唯一非空索引进行查询的情况。 ref：多表查询时，根据非唯一非空索引进行查询的情况。 range：在一个索引上进行范围查找。 index：遍历索引树查询，通常发生在查询结果只包含索引字段时。 ALL：全表扫描，没有任何索引可以使用时。这是最差的情况，应该避免。 possible_keys 表示mysql此次查询中可能使用的索引。 key 表示mysql实际在此次查询中使用的索引。 key_len 表示mysql使用的索引的长度。该值越小越好。 ref如果是使用的常数等值查询，这里会显示const，如果是连接查询，被驱动表的执行计划这里会显示驱动表的关联字段，如果是条件使用了表达式或者函数，或者条件列发生了内部隐式转换，这里可能显示为func rows 也是一个重要的字段。 这是mysql估算的需要扫描的行数（不是精确值）。这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. extra（重要） explain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: distinct：在select部分使用了distinc关键字 Using filesort：当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. Using index“覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化. 4.什么是覆盖索引？ 覆盖索引（covering index ，或称为索引覆盖）即从非主键索引中就能查到的记录，而不需要查询主键索引中的记录，避免了回表的产生减少了树的搜索次数，显著提升性能。 5.最左前缀原则是什么 当一个SQL想要利用索引是，就一定要提供该索引所对应的字段中最左边的字段，也就是排在最前面的字段，比如针对a,b,c三个字段建立了一个联合索引，那么在写一个sql时就一定要提供a字段的条件，这样才能用到联合索引，这是由于在建立a,b,c三个字段的联合索引时，底层的B+树是按照a,b.c三个字段从左往右去比较大小进行排序的，所以如果想要利用B+树进行快速查找也得符合这个规则 6.lnnodb是如何实现事务的 Innodb通过Buffer Pool，LogBuffer，Redo Log, Undo Log来实现事务，以一个update语句为例: Innodb在收到一个update语句后，会先根据条件找到数据所在的页，并将该页缓存在Buffer Pool中 执行update语句，修改Buffer Pool中的数据，也就是内存中的数据 针对update语句生成一个RedoLog对象，并存入LogBuffer中 针对update语句生成undolog日志，用于事务回滚 如果事务提交，那么则把RedoLog对象进行持久化，后续还有其他机制将Buffer Pool中所修改的数据页持久化到磁盘中6．如果事务回滚，则利用undolog日志进行回滚 7.B树和B+树的区别，为什么Mysql使用B+树 B树的特点: 1．节点排序2.一个节点了可以存多个元素，多个元素也排序了 B+树的特点: 1．拥有B树的特点2．叶子节点之间有指针3．非叶子节点上的元素在叶子节点上都冗余了，也就是叶子节点中存储了所有的元素，并且排好顺序 Mysql索引使用的是B+树，因为索引是用来加快查询的，而B+树通过对数据进行排序所以是可以提高查询速度的，然后通过一个节点中可以存储多个元素，从而可以使得B+树的高度不会太高，在Mysql中一个Innodb页就是一个B+树节点，一个Innodb页默认16kb，所以一般情况下一颗两层的B+树可以存2000万行左右的数据，然后通过利用B+树叶子节点存储了所有数据并且进行了排序，并且叶子节点之间有指针，可以很好的支持全表扫描，范围查找等SQL语句。 为什么索引采用B+树 1.索引用来加快查询速度，B+树对数据排序可以加快查询速度 2.一个节点存储多个元素，B+树不会很高，一个innodb页是一个节点，两层B+树可以存储200万条数据 3.B+树叶子节点有指针，可以支持全表扫描，范围查找等SQL语句 8.Mysql锁有哪些，如何理解 按锁粒度分类: 行锁:锁某行数据，锁粒度最小，并发度高 表锁:锁整张表，锁粒度最大，并发度低 间隙锁:锁的是一个区间 还可以分为: 共享锁:也就是读锁，一个事务给某行数据加了读锁，其他事务也可以读，但是不能写- 排它锁:也就是写锁，一个事务给某行数据加了写锁，其他事务不能读，也不能写 还可以分为: 乐观锁:并不会真正的去锁某行记录，而是通过一个版本号来实现的 悲观锁:上面所的行锁、表锁等都是悲观锁 在事务的隔离级别实现中，就需要利用锁来解决幻读 9.Mysql慢查询该如何优化? 检查是否走了索引，如果没有则优化SQL利用索引 检查所利用的索引，是否是最优索引 检查所查字段是否都是必须的，是否查询了过多字段，查出了多余数据 检查表中数据是否过多，是否应该进行分库分表了 检查数据库实例所在机器的性能配置，是否太低，是否可以适当增加资源 10.mysql有哪些引擎 所谓的存储引擎是指用于存储、处理和保护数据的核心服务。也就是存储引擎是数据库的底层软件组织。在 MySQL 中可以使用“show engines”来查询数据库的所有存储引擎 InnoDB 存储引擎InnoDB是一种兼顾高可靠性和高性能的通用存储引擎，在 MySQL 5.5 之后，InnoDB是默认的 MySQL 存储引擎。 特点(1) 支持自动增长列AUTO_INCREMENT。自动增长列的值不能为空，且值必须唯一。MySQL中规定自增列必须为主键。(2) 支持外键，保证数据的完整性和正确性。外键所在表为子表，外键所依赖的表为父表。父表中被子表外键关联的字段必须为主键。(3) DML(数据库操作)操作遵循ACID模型，支持事务。(4) 行级锁 ，提高并发访问性能。 MyISAM 存储引擎MyISAM是MySQL早期的默认存储引擎。 特点(1) 不支持事务，不支持外键(2) 支持表锁，不支持行锁(3) 占用空间小，访问速度快 Memory 存储引擎Memory引擎的表数据时存储在内存中的，由于受到硬件问题、或断电问题的影响，只能将这些表作为临时表或缓存使用。 特点(1) 内存存放(2) hash索引（默认） 11.索引的作用 创建索引可以大大提高系统的性能。 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 12.索引的类型 存储方式区分1. B-树索引目前大多数索引都是采用B-树来存储，其包含组件有： 叶子节点：包含的条目直接指向表里的数据行。叶子节点之间彼此相连，一个叶子节点有一个指向下一个叶子节点的指针。 分支节点：包含的条目指向索引里其他的分支节点或者叶子节点。 根节点：一个 B-树索引只有一个根节点，实际上就是位于树的最顶端的分支节点。 2. 哈希索引哈希索引也称为散列索引或 HASH 索引。MySQL 目前仅有 MEMORY 存储引擎和 HEAP 存储引擎支持这类索引。其中，MEMORY 存储引擎可以支持 B-树索引和 HASH 索引，且将 HASH 当成默认索引。 哈希索引的最大特点是访问速度快，但也存在下面的一些缺点： MySQL 需要读取表中索引列的值来参与散列计算，散列计算是一个比较耗时的操作。也就是说，相对于 B-树索引来说，建立哈希索引会耗费更多的时间。 不能使用 HASH 索引排序。 HASH 索引只支持等值比较，如”=” “IN()”或”&lt;=&gt;”。 HASH 索引不支持键的部分匹配，因为在计算 HASH 值的时候是通过整个索引值来计算的。 逻辑区分1. 普通索引普通索引是 MySQL 中最基本的索引类型，它没有任何限制，唯一任务就是加快系统对数据的访问速度。允许重复值和空值。 关键字是 INDEX 或 KEY。 2. 唯一索引唯一索引列的值必须唯一，允许有空值。如果是组合索引，则列值的组合必须唯一。 关键字是 UNIQUE。 3. 主键索引主键索引是一种特殊的唯一索引，不允许值重复或者值为空。 关键字是 PRIMARY KEY。 4. 空间索引空间索引是对空间数据类型的字段建立的索引，不允许空值，只能在存储引擎为 MyISAM 的表中创建。 关键字是 SPATIAL。 5. 全文索引全文索引主要用来查找文本中的关键字，只能在 CHAR、VARCHAR 或 TEXT 类型的列上创建。只有 MyISAM 存储引擎支持，允许重复值和空值。 关键字是 FULLTEXT。 实际使用区分1. 单列索引单列索引可以是普通索引，也可以是唯一性索引，还可以是全文索引。只要保证该索引只对应一个字段即可。 2. 组合索引组合索引也称为复合索引或多列索引。相对于单列索引来说，组合索引是将原表的多个列共同组成一个索引。 查询时，字段顺序需与索引顺序一致；LIKE时，首字符不能是 ‘%’，否则会影响索引使用。 13.Mysql事务是什么，介绍一下 事务：一个最小的不可再分的单元；可以理解为一个事务对应的是一组完整的业务，并且在这个事务中所作的一切操作要么全部成功，要么全部失败，只要有一个操作没成功，整个事务都将回滚到事务开始前。 事务的四大特征：原子性：每一个事务都是一个不可再分的工作单位，事务中包括的操作要么都做，要么 都不做。 一致性：对于数据的操作从一个一致的状态转变成另一个一致转态。 隔离性：指一个事务的执行不能被其他事务干扰，即一个事务内部的操作对并发的其他事务是具有隔离的，并发执行的各个事务之间不能互相干扰。 持久性：一个事务一旦提交，他对数据库中的数据的改变就应该是永久性的。提交后的其他操作或故障不会对其具有任何影响。 14.Mysql主从了解吗，说说过程和原理? MySQL 主从复制概念MySQL 主从复制是指数据可以从一个MySQL数据库服务器主节点复制到一个或多个从节点。MySQL 默认采用异步复制方式，这样从节点不用一直访问主服务器来更新自己的数据，数据的更新可以在远程连接上进行，从节点可以复制主数据库中的所有数据库或者特定的数据库，或者特定的表。 为什么需要主从复制？ 在业务复杂的系统中，有这么一个情景，有一句sql语句需要锁表，导致暂时不能使用读的服务，那么就很影响运行中的业务，使用主从复制，让主库负责写，从库负责读，这样，即使主库出现了锁表的情景，通过读从库也可以保证业务的正常运作。 做数据的热备，当主数据库有问题，可以切换从数据库 架构的扩展。业务量越来越大，I/O访问频率过高，单机无法满足，此时做多库的存储，降低磁盘I/O访问的频率，提高单个机器的I/O性能。 MySQL主从复制原理 从上图可以看出，MySQL主从同步实现主要有以下三个过程：1、当有数据更改语句执行时，MySQL主库要在更新数据的同时，写二进制日志，将数据修改的内容记录进入日志中。2、MySQL从库上运行这一些I/O进程，这个进程会监视MySQL主库上的二进制日志，当发现修改时，会立即同步到自身的中继日志。3、MySQL从库上还会运行一个SQL进程，该进程用于监视自身的中继日志，当发现自身的中继日志发生改变时，立即将该中继日志改变对应的数据更改操作写入自身的数据库。 MySQL主从复制模式异步复制 在异步复制（async replication）中，Master不用关心Slave是否接收到二进制日志，所以Master与Slave没有任何的依赖关系。你可以认为Master和Slave是分别独自工作的两台服务器，数据最终会通过二进制日志达到一致。 异步复制的性能最好，因为它对数据库本身几乎没有任何开销，除非主从延迟非常大，Dump Thread需要读取大量二进制日志文件。 如果业务对于数据一致性要求不高，当发生故障时，能容忍数据的丢失，甚至大量的丢失，推荐用异步复制，这样性能最好（比如像微博这样的业务，虽然它对性能的要求极高，但对于数据丢失，通常可以容忍）。但往往核心业务系统最关心的就是数据安全，比如监控业务、告警系统。 半同步复制 主库在执行完客户端提交的事务后不是立刻返回给客户端，而是等待至少一个从库接收到并写到 relay log 中才返回给客户端。相对于异步复制，半同步复制提高了数据的安全性，同时它也造成了一定程度的延迟。半同步复制的出现，就是为了保证在任何时刻主备数据一致的问题。相对于异步复制，半同步复制要求执行的每一个事务，都要求至少有一个备库成功接收后，才返回给用户。实现原理也很简单，主库本地执行完毕后，等待备库的响应消息（包含最新备库接收到的binlog（file，pos）），接收到备库响应消息后，再返回给用户，这样一个事务才算真正完成。在主库实例上，有一个专门的线程（ack_receiver）接收备库的响应消息，并以通知机制告知主库备库已经接收的日志，可以继续执行。 增强半同步复制 解决半同步复制中途数据不一致问题 半同步的问题是因为等待ACK的点是Commit之后，此时Master已经完成数据变更，用户已经可以看到最新数据，当Binlog还未同步到Slave时，发生主从切换，那么此时从库是没有这个最新数据的，用户又看到老数据。 增强半同步将等待ACK的点放在提交Commit之前，此时数据还未被提交，外界看不到数据变更，此时如果发送主从切换，新库依然还是老数据，不存在数据不一致的问题。 全同步复制 当主库提交事务之后，所有的从库节点必须收到、APPLY并且提交这些事务，然后主库线程才能继续做后续操作。因为需要等待所有从库执行完该事务才能返回，所以全同步复制的性能必然会收到严重的影响。 15.char和varchar区别 1、最大长度： char最大长度是255字符，varchar最大长度是65535个字节。 2、定长： char是定长的，不足的部分用隐藏空格填充，varchar是不定长的。 3、空间使用： char会浪费空间，varchar会更加节省空间。 4、查找效率： char查找效率会很高，varchar查找效率会更低。 5、尾部空格： char插入时可省略，vaechar插入时不会省略，查找时省略。 ps(int 长度是11位)","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"MySQL","slug":"MySQL","permalink":"https://gwtt.github.io/tags/MySQL/"}]},{"title":"Jvm面试","slug":"jvm面试","date":"2022-06-12T15:08:16.733Z","updated":"2022-09-14T08:51:32.182Z","comments":true,"path":"2022/06/12/jvm面试/","link":"","permalink":"https://gwtt.github.io/2022/06/12/jvm%E9%9D%A2%E8%AF%95/","excerpt":"","text":"jvm1.JVM运行时划分哪几个区域？哪些区域是线程共享的？哪些区域是线程独占的？ JVM运行时一共划分：程序计数器、虚拟机栈、堆、本地方法栈、方法区。 线程共享的数据区域：堆、方法区。 线程独享的数据区域区域：程序计数器、虚拟机栈、本地方法栈。 连问（1）这几个内存区域分别存放什么数据？ 程序计数器: 记录当前线程执行的位置 虚拟机栈: 存储基本数据类型以及对象的引用等 堆: 存储对象实例 本地方法栈: 与虚拟机栈类似，它为Native方法服务 方法区: 存储被JVM加载的类信息、常量、静态变量等。 2.JVM内存怎么分配的 方法区: 有时候也称为永久代（Permanent Generation） 注意: 在Java8中，永久代已经被移除，被一个称为“元数据区”(元空间)的区域所取代。 方法区和永久代的关系很像Java中接口和类的关系，类实现了接口，而永久代就是HotSpot虚拟机对虚拟机规范中方法区的一种实现方式。 在方法区中，存储了每个类的信息（包括类的名称、修饰符、方法信息、字段信息）、类中静态变量、类中定义为final类型的常量、类中的Field信息、类中的方法信息以及编译器编译后的代码等。当开发人员在程序中通过Class对象中的getName、isInterface等方法来获取信息时，这些数据都来源于方法区域，同时方法区域也是全局共享的，在一定的条件下它也会被GC，在这里进行的GC主要是方法区里的常量池和类型的卸载。当方法区域需要使用的内存超过其允许的大小时，会抛出OutOfMemory的错误信息。 在方法区中有一个非常重要的部分就是运行时常量池，用于存放静态编译产生的字面量和符号引用。运行时生成的常量也会存在这个常量池中，比如String的intern方法。它是每一个类或接口的常量池的运行时表示形式，在类和接口被加载到JVM后，对应的运行时常量池就被创建出来。 JVM堆分代 1、JVM堆被分为了年轻代和老年代。年轻代的GC过程称为Yong GC，速度快较频繁。老年代的GC过程称为Full GC，速度较慢应该尽量避免。 2、对象被创建后，除了少部分大对象会在老年代分配内存外，大部分的对象首先都是在年轻代进行内存分配，而且大部分的对象都是“朝生夕死”，很快就会被年轻代的Yong GC回收掉。 3、老年代的内存空间一般会比年轻代的内存空间大，能存放的对象多，老年代的空间不足后会进行Full GC操作，比Yong GC耗时，所以应尽量避免频繁的Full GC操作。 年轻代的分区 1、年轻代中分为一个Eden区和两个Surviver区，比例为8：1：1，两个Surviver区分别称为“From”区和“To”区。对象在Eden区创建，经过一次Yong GC后，还存活的对象将会被复制到Surviver区的“From”区，此时“To”区是空的。到了下一次GC的时候，Eden区还存活的对象会复制到Surviver区的“To”区，而“Form”区的对象有两个去处，“From”区的对象会根据经过的GC次数计算年龄，如果年龄到达了阈值（默认15），则会被移动到老年代中，否则就复制到“To”区，此时“From”区变成了空的，然后“From”区和“To”区进行角色互换，到下一次进行GC时，还是有一块空的“To”区，用来存放从eden区和“From”区移动过来的对象。 2、那这种分区有什么好处呢？ a、在年轻代新增Surviver区，有利于减轻老年代的负担，尽可能的让大部分对象在年轻代通过较高效的Yong GC回收掉，不至于老年代里存放的对象过多导致内存不足而进行频繁的Full GC操作。 b、这种分区有利于减少内存碎片的产生。 首先我们来看看，如果年轻代只分为Eden区和Surviver区两个区域并且比例是8:2的时候，内存的回收和分配情况会怎么样。第一次Yong GC后，Eden区还存活的对象移动到Surviver区，Surviver区还存活的对象保留在Surviver区，而这些对象的内存是不连续的，Surviver区里就会产生很多内存碎片，这就会导致有些大对象要移动到Surviver区的时候，没有足够的连续内存进行分配，而不得不移动到老年代中，增加老年代的负担，降低效率。 然后我们看看Eden区和Surviver区的比例是8:1:1时会有什么样的效果。第一次Yong GC后，Eden区还存活的对象复制到Surviver区的“To”区，“From”区还存活的对象也复制到“To”区，再清空Eden区和From区，这样就等于“From”区完全是空的了，而“To”区也不会有内存碎片产生，等到第二次Yong GC时，“From”区和“To”区角色互换，很好的解决了内存碎片的问题。 详细的过程：1.当系统创建一个对象的时候，总是在Eden区操作，当这个区满了，那么就会触发一次YoungGC，也就是年轻代的垃圾回收。 一般来说这时候不是所有的对象都没用了，所以就会把还能用的对象复制到From区。 2.这样整个Eden区就被清理干净了，可以继续创建新的对象，当Eden区再次被用完，就再触发一次YoungGC，然后呢，注意，这个时候跟刚才稍稍有点区别。这次触发YoungGC后，会将Eden区与From区还在被使用的对象复制到To区， 3.再下一次YoungGC的时候，则是将Eden区与To区中的还在被使用的对象复制到From区。 4.经过若干次YoungGC后，有些对象在From与To之间来回游荡，这时候From区与To区亮出了底线（阈值），这些家伙要是到现在还没挂掉，对不起，一起滚到（复制）老年代吧。 3.JVM怎么回收内存，gc机制是什么? 垃圾收集需要完成的三件事情： 哪些内存需要回收？ 程序计数器，虚拟机栈和本地方法栈都是随线程而生，随线程而灭。 栈中的栈帧随着方法的进入和退出而有条不紊地执行着出栈和入栈操作。 每一个栈帧中分配多少内存基本上是在类结构确定下来时就已知的因此这几个区域的内存分配和回收都具备确定性，在这几个区域内就不需要过多考虑如何回收的问题， 当方法结束或者线程结束时， 内存自然就跟随着回收了。 而Java堆和方法区这两个区域则有着很显著的不确定性： 一个接口的多个实现类需要的内存可能会不一样， 一个方法所执行的不同条件分支所需要的内存也可能不一样， 只有处于运行期间， 我们才能知道程序究竟会创建哪些对象， 创建多少个对象， 这部分内存的分配和回收是动态的。 因此，垃圾收集器所关注的正是这部分内存该如何管理。 什么时候回收？ 哪些还“存活”着， 哪些已经“死去”了。 判断对象是否死去通常有两种方法：引用计数算法和可达性分析算法。 引用计数算法引用计数算法：在对象中添加一个引用计数器， 每当有一个地方引用它时， 计数器值就加一； 当引用失效时， 计数器值就减一； 任何时刻计数器为零的对象就是不可能再被使用的。 优点: 原理简单，判断效率高 实时性，任何内存，一旦没有指向它的引用，就会立即被回收。 缺点: 内存分配和释放次数变多，维护引用计数代价越高（执行效率低） 循环引用不能去使用（关键缺点） 在Java领域，至少主流的Java虚拟机里面都没有选用引用计数算法来管理内存，主要原因是这个看似简单的算法有很多例外情况要考虑，必须要配合大量额外处理才能保证正确地工作。 可达性分析算法可达性分析算法：通过一系列称为“GC Roots”的根对象作为起始节点集， 从这些节点开始， 根据引用关系向下搜索， 搜索过程所走过的路径称为“引用链”（Reference Chain） ，如果某个对象到GC Roots间没有任何引用链相连，或者用图论的话来说就是从GC Roots到这个对象不可达时， 则证明此对象是不可能再被使用的。 如下图所示， 对象object 5、 object 6、 object 7虽然互有关联， 但是它们到GC Roots是不可达的，因此它们将会被判定为可回收的对象。 在Java技术体系里面， 固定可作为GC Roots的对象包括以下几种： 1234567在虚拟机栈（栈帧中的本地变量表）中引用的对象， 譬如，各个线程被调用的方法堆栈中使用到的参数、 局部变量、 临时变量等。在方法区中类静态属性引用的对象， 譬如，Java类的引用类型静态变量。在方法区中常量引用的对象， 譬如，字符串常量池（String Table）里的引用。在本地方法栈中JNI（即通常所说的Native方法）引用的对象。Java虚拟机内部的引用，如，基本数据类型对应的Class对象， 一些常驻的异常对象（比如NullPointExcepiton、 OutOfMemoryError）等， 还有系统类加载器。所有被同步锁（synchronized关键字）持有的对象。反映Java虚拟机内部情况的JMXBean、 JVMTI中注册的回调、本地代码缓存等。 如何回收？ 即使在可达性分析算法中判定为不可达的对象，也不是“非死不可”的，这时候它们暂时还处于“缓刑”阶段。 这边就要用到垃圾收集算法 标记-清除算法算法分为标记和清除两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。它是最基础的收集算法，因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。 缺点: 标记和清除过程的效率都不高 标记清除之后会产生大量的不连续的内存碎片，分配较大对象无法找到连续内存不得不触发另一次垃圾回收 复制算法为了解决效率问题，一种称为“复制”的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用一块。当这块内存用完了。就将还活着的对象复制到另一块上面，然后再把已经用过的内存空间一次清理掉。这样使得每次都是对其中一块内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只需要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，未免也太高了一点。 如图所示，“半区域复制”这样实现的垃圾回收算法缺点显而易见（1）内存利用效率太低，只能利用一半的内存（2）如果内存中出现对象大都是存活的情况，将会产生大量内存间复制的开销 在1989年，Andrew Appel针对具备“朝生夕灭”特点的对象，提出了一种更为优化的半区复制分代策略，现在称为“Appel式回收”。HotSpot虚拟机的Serial、ParNew等新生代收集器均采用了这种策略来设计新生代的内存布局。 具体做法就是，把新生代分为一块较大的Eden空间和两块较小的Survivor空间，每次分配内存只使用Eden和其中一块Survivor。发生内存收集时，将Eden和Survivor中仍然存活的对象一次性复制到另外一块Survivor空间上，然后直接清理掉Eden和已使用过的那一块Survivor空间。HotSpot虚拟机默认的Eden和Survivor的大小默认时8:1，也即每次新生代中可用内存空间为整个新生代容量的90%，只有另一个Survivor空间，即10%的新生代是“浪费”的。如图： 标记-整理算法复制算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对内存中被使用的所有的对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。根据老年代的特点，有人提出了一种“标记-整理“算法，标记过程仍然与”标记-清除“算法一样，但是后续步骤不是直接对可回收的对象进行清理，而是让所有存活的对象都向一端移动，然后清理掉端边界以外的内存。 分代收集算法当前商业虚拟机的垃圾回收都采用”分代收集“算法，这种算法并没有什么新的思想，只是根据对象的存活周期的不同将内存划分为几块。一般是把Java堆分为老年代和新生代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批的对象死去，只有少量存活，那就选用复制算法，只需要付出少量的存活对象的复制成本就可以完成收集。而老年代中因为对象的存活率高、没有额外的空间对他进行分配担保，就必须使用”标记-整理“或者”标记-清理“算法来进行回收。 （1） 年轻代(Young Gen) 年轻代特点是区域相对老年代较小，对像存活率低。 这种情况复制算法的回收整理，速度是最快的。复制算法的效率只和当前存活对像大小有关，因而很适用于年轻代的回收。而复制算法内存利用率不高的问题，通过hotspot中的两个survivor的设计得到缓解。 （2） 老年代(Tenure Gen) 老年代的特点是区域较大，对像存活率高。 这种情况，存在大量存活率高的对像，复制算法明显变得不合适。一般是由标记清除或者是标记清除与标记整理的混合实现。 Mark阶段的开销与存活对像的数量成正比，这点上说来，对于老年代，标记清除或者标记整理有一些不符，但可以通过多核/线程利用，对并发、并行的形式提标记效率。 Sweep阶段的开销与所管理区域的大小形正相关，但Sweep“就地处决”的特点，回收的过程没有对像的移动。使其相对其它有对像移动步骤的回收算法，仍然是效率最好的。但是需要解决内存碎片问题。 Compact阶段的开销与存活对像的数据成开比，如上一条所描述，对于大量对像的移动是很大开销的，做为老年代的第一选择并不合适。 基于上面的考虑，老年代一般是由标记清除或者是标记清除与标记整理的混合实现。以hotspot中的CMS回收器为例，CMS是基于Mark-Sweep实现的，对于对像的回收效率很高，而对于碎片问题，CMS采用基于Mark-Compact算法的Serial Old回收器做为补偿措施：当内存回收不佳（碎片导致的Concurrent Mode Failure时），将采用Serial Old执行Full GC以达到对老年代内存的整理。 总结：没有最好的算法，只有最合适的算法 4.static变量的初始化前后在jvm内存中的位置？ 成员变量数据存储在堆内存的对象中，所以也叫对象的特有数据。 静态变量数据存储在方法区（共享数据区）的静态区，所以也叫对象的共享数据。 5.Java方法调用在jvm中是怎样的过程(方法栈、入参、返回值) 方法（Java中称为方法，其他语言一般称为函数）调用主要是通过栈来存储相关的数据，系统就方法调用者和方法如何使用栈做了约定，返回值可以简单认为是通过一个专门的返回值存储器来存储的。 123456789101112public class Sum&#123; public static int sum(int a, int b)&#123; int c = a * b; return c; &#125; public static void main(String[] args)&#123; int d = Sum.sum(1, 2); System.out.println(d); &#125;&#125; 当程序在 main 方法调用 Sum.sum 之前，栈的情况大概如图所示。 在 main 方法调用 Sum.sum 时，首先将参数 1 和 2 入栈，然后将返回地址（也就是调用方法结束后要执行的指令地址）入栈，接着跳转到 sum 函数，在 sum 函数内部，需要为局部变量 c 分配一个空间，而参数变量 a 和 b 则直接对应于入栈的数据 1 和 2，在返回之前，返回值保存到了专门的返回值存储器中。 在调用 return 后，程序会跳转到栈中保存的返回地址，即 main 的一条指令地址，而 sum 函数相关的数据会出栈，从而又变回上图中的样子。 main 的下一条指令是根据方法返回值给变量 d 赋值，返回值从专门的返回值存储器中获得。 程序执行的基本原理CPU有一个指令指示器，指向下一条要执行的指令，要么顺序执行，要么进行跳转（条件跳转或无条件跳转）。 具体到Java程序来说就是，程序从 main 方法开始顺序执行，方法调用可以看作一个无条件跳转，跳转到对应方法的指令处开始执行，碰到 return 语句或者方法结尾的时候，再执行一次无条件跳转， 跳转回调用方，执行调用方法后的下一条指令。 6.如果一个程序频繁触发Full GC，原因可能是什么？ （1）System.gc()方法的调用。此方法的调用是建议JVM进行Full GC,虽然只是建议而非一定，但很多情况下它会触发 Full GC,从而增加Full GC的频率，也即增加了间歇性停顿的次数。强烈建议能不使用此方法就别使用，让虚拟机自己去管理它的内存，可通过通过-XX:+ DisableExplicitGC来禁止RMI（Java远程方法调用）调用System.gc。 （2）旧生代空间不足。旧生代空间只有在新生代对象转入及创建为大对象、大数组时才会出现不足的现象，当执行Full GC后空间仍然不足，则抛出错误：java.lang.OutOfMemoryError: Java heap space 。为避免以上两种状况引起的FullGC，调优时应尽量做到让对象在Minor GC阶段被回收、让对象在新生代多存活一段时间及不要创建过大的对象及数组。 （3）Permanet Generation空间满了。Permanet Generation中存放的为一些class的信息等，当系统中要加载的类、反射的类和调用的方法较多时，Permanet Generation可能会被占满，在未配置为采用CMS GC的情况下会执行Full GC。如果经过Full GC仍然回收不了，那么JVM会抛出错误信息：java.lang.OutOfMemoryError: PermGen space 。为避免Perm Gen占满造成Full GC现象，可采用的方法为增大Perm Gen空间或转为使用CMS GC。 （4）通过Minor GC后进入老年代的平均大小大于老年代的可用内存。如果发现统计数据说之前Minor GC的平均晋升大小比目前old gen剩余的空间大，则不会触发Minor GC而是转为触发full GC。 （5）由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 123451.java远程调用System.gc()方法2.老年代空间不足3.永久代空间满了（java8被元空间替代，使用本地内存，所以这个现在不太可能发生）4.假如young gc后，进入老年代的平均大小大于老年代可用内存会触发full gc5.由Eden区，from区向to区复制时（或者颠倒顺序），对象大于to区内存，则移动到老年代，但是老年代内存小于该对象大小 7.jvm如何知道new了一个对象要多大内存 对象在堆内存中的存储布局可以划分为三个部分： 对象头,实例数据,对齐填充 对象头对象的对象头包括两类信息。第一类是存储对象自身的运行时数据，第二类是类型指针 Mark World存储对象自身的运行时数据，如哈希码，GC分代年龄、锁状态标志、线程持有的锁等等。这部分的数据长度在32位虚拟机中为4个字节，在64位虚拟机中是8个字节，官方称之为 Mark Word 类型指针指向它的类型元数据的指针（指向它的Class对象的指针），大小是4个字节。Java虚拟机通过这个指针来确定该对象是哪个类的实例 实例数据实例数据部分是对象真正存储的有效信息，即我们在程序代码里面所定义的各种类型的字段内容。计算方式是累加，如下对象： 1234public class O &#123; private int o1; private long o2&#125; 实例数据部分长度就是 int(4字节)+long(8字节)=12字节 对齐填充第三部分就是对齐填充。HotSpot虚拟机的自动内存管理要求对象起始地址必须是8字节的整数倍，换句话说就是任何对象的大小都必须是8字节的整数倍。如果对象大小没到8字节的整数倍，那就需要通过对齐填充来补全。 实战已经知道了对象的内存布局，我们就可以来尝试计算一个类对象占用的内存：我们就来计算 String类的内存：我们先来查看String类里面的实例数据有哪些; 12345678@Stableprivate final byte[] value;private final byte coder;private int hash; // Default to 0private boolean hashIsZero; // Default to false;private static final long serialVersionUID = -6849794470754667710L;static final boolean COMPACT_STRINGS;可以得到实例数据部分的字节是 数组对象 + byte（1字节）+ int（4字节）+ boolean(1字节) + long（8字节） + Boolean（1字节） 那么数组对象也是一个对象，它的占用内存是 对象头（8字节）+ 引用（4字节）+ 记录长度的int（4字节）=16字节。 所以空的String对象占用内存是 8+16+1+4+1+8+1+ 1字节（字节填充）=40字节 非空的String对象 非空的string对象比空的string对象只有在数组对象里的 实例数据部分变化了，其他都没变，所以非空的String对象占用内存是 8+16+1+4+1+8+1+ 1字节（字节填充）=40+ n字节（n是byte数组的长度） 8.堆，方法区，永久代，元空间 之间的关系 方法区和永久代的关系方法区是 《Java虚拟机规范 》中的规范，在 HotSpot 虚拟机中，JDK 6.0 及以前由永久代实现，就如同 Java 中的接口和实现一样。 永久代并不是很好的方法区实现，《Java虚拟机规范》中方法区不需要连续的内存20)并且可以选择固定大小或可扩展，而永久代有 -XX:MaxPermSize 上限，即使不设置也有默认大小，很容易造成内存溢出。 堆和方法区的关系《Java虚拟机规范》把方法区描述为堆的逻辑部分，但它却有一个名字 Non-Heap 非堆，两者物理上并无联系。 两者的共同点就是都可以选择固定大小或可扩展。 元空间：JDK 8 把永久代迁移到由本地内存实现的元空间当中 永久代数据迁移 1.6 及以前永久代是方法区的实现，其中包括：已加载的类型信息，常量，静态变量，即时编译器编译过后的代码缓存等。 1.7 将字符串常量池，静态变量移出到堆中。 1.8 将老年代剩余内容（主要是类型信息）全部移到元空间中。 方法区的实现永久代真的永久吗？垃圾回收在这个区域出现的比较少，但是并不是没有，主要是常量池的回收和类型的卸载。主要是动态类的生成：CGLib字节码增强，JSP 等。 永久代和元空间JDK 8 以后，永久代完全退出历史舞台，由元空间替代 常量池在 JDK 6,7,8 中存在位置证明1234567891011121314import java.util.HashSet;import java.util.Set;public class RuntimeConstantPoolOOM &#123; public static void main(String[] args) &#123; Set&lt;String&gt; set = new HashSet&lt;&gt;(); int i = 0; while (true)&#123; // String.intern();去字符串常量池中查找有没有这个字符串，有的话指向引用，没有将字符串拷贝进字符串常量池当中 set.add(String.valueOf(i++).intern()); &#125; &#125;&#125;12345678910111213 JDK 6：-XX:MaxPermSize 限制永久代大小，异常 OOM:PermGen space ，代表永久代空间不足，此时字符串常量池在永久代当中。 JDK 7及以上：-XX:MaxPermSize 限制永久代大小，循环一直下去，不会爆异常。-Xmx512M 限制最大堆就可以看到 OOM:heap space堆内存不足，代表字符串常量池在堆中。 指令补充 -Xmssize 设置初始化堆内存大小，这个值的大小必须是1024的倍数，并且大于1M，可以指定单位k(K),m(M),g(G)。例如 -Xms6m。如果没有设置这个值，那么它的初始化大小就是年轻代和老年代的和。等价于-XX:InitialHeapSize。 -Xmxsize 设置最大堆内存大小，这个值的大小必须是1024的倍数，并且大于2M，可以指定单位k(K),m(M),g(G)。默认值是根据运行时的系统配置来确定的。一般服务器部署时，把-Xms和-Xmx的值设置成相同的大小。-Xmx选项和-XX:MaxHeapSize相同。 -Xmnsize 设置年轻代大小。可以指定单位k(K),m(M),g(G) .例如-Xmn256m。还可以通过其他两个选项来代替这个选项来指定年轻代最小和最大内存：-XX:NewSize指定初始化大小,-XX:MaxNewSize指定最大内存大小 -XX:NewSize=&lt; n &gt;[g|m|k] 年轻代的初始值。 -XX:MaxNewSize=&lt; n &gt;[g|m|k] 年轻代的最大值。 -Xsssize 设置线程栈的大小。可以指定单位k(K),m(M),g(G)。默认值根据内存而定。 这个选项和-XX:ThreadStackSize相同 -XX:ThreadStackSize=size 设置线程栈大小。默认值依赖于机器内存。这个选项和-Xss选项的功能相同。 -XX:MetaspaceSize=size 设置元数据空间初始大小。 -XX:MaxMetaspaceSize=size 设置元数据空间最大值。 -XX:NewRatio 设置老生代和新生代大小的比例，比如-XX:NewRatio=2表示1/3的Heap是新生代，2/3的Heap是老生代。 -XX:SurvivorRatio 用来设置新生代中Eden和Survivor空间大小的比例，需要注意的是有两个Survivor。比如-XX:SurvivorRatio=8表示Eden区域在新生代的8/10，两个Survivor分别占1/10。","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Jvm","slug":"Jvm","permalink":"https://gwtt.github.io/tags/Jvm/"}]},{"title":"计算机网络面试","slug":"计算机网络面试(1)","date":"2022-06-12T15:00:11.369Z","updated":"2022-09-11T11:43:03.040Z","comments":true,"path":"2022/06/12/计算机网络面试(1)/","link":"","permalink":"https://gwtt.github.io/2022/06/12/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95(1)/","excerpt":"","text":"计算机网络相关(1)1.get和post的区别 GET POST 后退按钮/刷新 无害 数据会被重新提交（浏览器应该告知用户数据会被重新提交）。 书签 可收藏为书签 不可收藏为书签 缓存 能被缓存 不能缓存 编码类型 application/x-www-form-urlencoded application/x-www-form-urlencoded 或 multipart/form-data。为二进制数据使用多重编码。 历史 参数保留在浏览器历史中。 参数不会保存在浏览器历史中。 对数据长度的限制 是的。当发送数据时，GET 方法向 URL 添加数据；URL 的长度是受限制的（URL 的最大长度是 2048 个字符）。 无限制。 对数据类型的限制 只允许 ASCII 字符。 没有限制。也允许二进制数据。 安全性 与 POST 相比，GET 的安全性较差，因为所发送的数据是 URL 的一部分。在发送密码或其他敏感信息时绝不要使用 GET ！ POST 比 GET 更安全，因为参数不会被保存在浏览器历史或 web 服务器日志中。 可见性 数据在 URL 中对所有人都是可见的。 数据不会显示在 URL 中。 从标准上来看，GET 和 POST 的区别如下： GET 用于获取信息，是无副作用的，是幂等的，且可缓存 POST 用于修改服务器上的数据，有副作用，非幂等，不可缓存 2.http和https的区别 HTTP协议传输的数据都是未加密的，也就是明文的，因此使用HTTP协议传输隐私信息非常不安全，为了保证这些隐私数据能加密传输，于是网景公司设计了SSL（Secure Sockets Layer）协议用于对HTTP协议传输的数据进行加密，从而就诞生了HTTPS。 简单来说，HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全。 HTTPS和HTTP的区别主要如下： 1、https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。 2、http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。 3、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 4、http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。 3.TCP和udp的区别 TCP面向连接（如打电话要先拨号建立连接）；UDP是无连接的，即发送数据之前不需要建立连接 TCP要求的系统资源较多，UDP较少 TCP提供可靠的服务。也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达；UDP尽最大努力交付，即不保证可靠交付 TCP面向字节流，实际上是TCP把数据看成一连串无结构的字节流；UDP是面向报文的 UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等） 每一条TCP连接只能是点到点的；UDP支持一对一，一对多，多对一和多对多的交互通信 TCP首部开销20字节；UDP的首部开销小，只有8个字节 TCP的逻辑通信信道是全双工的可靠信道；UDP则是不可靠信道 4.说说浏览器输入网址到返回页面的过程？ 12345基础版本：浏览器根据请求的 URL 交给 DNS 域名解析，找到真实 IP ，向服务器发起请求；服务器交给后台处理完成后返回数据，浏览器接收⽂件（ HTML、JS、CSS 、图象等）；浏览器对加载到的资源（ HTML、JS、CSS 等）进⾏语法解析，建立相应的内部数据结构 （如 HTML 的 DOM）；载⼊解析到的资源⽂件，渲染页面，完成。 1、用户在输入栏输入地址 (1) 如果有 beforeunload 事件会先执行判断继续还是跳出操作 (2) 浏览器进程识别是 地址还是关键字检索 (3) 将正确的 url 地址发送给网络进程 2、网络进程查看有没有缓存 (1) 有缓存直接返回数据 (2) 没有缓存直接进入网络进程请求流程 3、浏览器进程查询 DNS 获取服务器 ip 地址 (1) 如果请求协议是 https 那么还需要建立TLS连接 (2) 利用ip地址和服务器建立 tcp 连接 (3) 如果浏览器的 tcp 连接请求有超过浏览器设定限额，则需要排队等待 4、tcp 连接协议通过与服务器进行3次握手确立连接 (1) 浏览器即用户端发起第一次握手 (2) 服务器接收到用户端的消息发起接收到信息 (3) 用户端接收到信息后向服务器发起第三次握手表示已经接收到信息可以连接 (4) 如果第二次握手发送数据包丢失或者用户端发起第三次握手数据包丢失，服务器在长时间未接收到信息会再次发起，总共会尝试6次 5、tcp 接受信息 6、网络进程接收到数据后将返回头和返回体发送给浏览器 7、浏览器进程读取返回头查看返回信息，是否重定向或者响应数据类型处理 (1) 重定向即回到第3步 (2) 会根据 Content-Type 的值来决定如何显示响应体的内容。 (3) Content-Type 为下载类型就会把请求交给浏览器的下载管理器，导航结束 (4) 如果是 HTML，那么浏览器则会继续进行导航流程 8、准备渲染进程 9、浏览器进程接收到网络进程的响应头数据之后，便向渲染进程发起“提交文档”的消息 10、渲染进程就和网络进程直接建立连接进行管道数据传输 11、数据传输完成后tcp发起4次挥手断开连接并且渲染进程同时向浏览器进程发起确认提交 12、浏览器进程收到确认提交后会更新浏览器界面状态，包括了安全状态、地址栏的 URL、前进后退的历史状态。 13、渲染进程开始页面解析和子资源加载 相关的题目浏览器发出一个请求到收到响应经历了哪些步骤?1234561.浏览器解析用户输入的URL，生成一个HTTP格式的请求⒉先根据URL域名从本地hosts文件查找是否有映射IP，如果没有就将域名发送给电脑所配置的DNS进行域名解析，得到IP地址 3.浏览器通过操作系统将请求通过四层网络协议发送出去4.途中可能会经过各种路由器、交换机，最终到达服务器5.服务器搜到请求后，根据请求所指定的端口，将请求传递给绑定了该端口的应用程序，比如8080被tomcat占用了 6.tomcat接收到请求数据后，按照http协议的格式进行解析，解析得到所要访问的servlet7.然后servlet来处理这个请求，如果是SpringMVC中的DispatcherSservlet，那么则会找到对应的Controller中的方法，并执行该方法得到结果 8.Tomcat得到响应结果后封装成HTTP响应的格式，并再次通过网络发送给浏览器所在的服务器9.浏览器所在的服务器拿到结果后再传递给浏览器，浏览器则负责解析并渲染 5.cookie做什么的？session又是什么？它们直接的关系？ 什么是Cookie？Cookie实际上是一小段的文本信息。客户端请求服务器，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。客户端会把Cookie保存起来。 当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器。服务器检查该Cookie，以此来辨认用户状态。服务器还可以根据需要修改Cookie的内容。信息保存的时间可以根据需要设置. 如果没有设置Cookie失效日期,它们仅保存到关闭浏览器程序为止.如果将Cookie对象的Expires属性设置为Minvalue,则表示Cookie永远不会过期. Cookie存储的数据量很受限制,大多数浏览器支持最大容量为4K,因此不要用来保存数据集及其他大量数据. 由于并非所有的浏览器都支持Cookie,并且数据信息是以明文文本的形式保存在客户端的计算机中,因此最好不要保存敏感的,未加密的数据,否则会影响网站的安全性 什么是Session？Session是另一种记录客户状态的机制，不同的是Cookie保存在客户端浏览器中，而Session保存在服务器上。客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上。这就是Session。客户端浏览器再次访问时只需要从该Session中查找该客户的状态就可以了。 每个用户访问服务器都会建立一个session，那服务器是怎么标识用户的唯一身份呢？事实上，用户与服务器建立连接的同时，服务器会自动为其分配一个SessionId。 Session和Cookie的区别？ 1、数据存储位置：cookie数据存放在客户的浏览器上，session数据放在服务器上。 2、安全性：cookie不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗，考虑到安全应当使用session。 3、服务器性能：session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能，考虑到减轻服务器性能方面，应当使用cookie。 4、数据大小：单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。 5、信息重要程度：可以考虑将登陆信息等重要信息存放为session，其他信息如果需要保留，可以放在cookie中。 6.跨域请求是什么?有什么问题?怎么解决? 跨域是指浏览器在发起网络请求时，会检查该请求所对应的协议、域名、端口和当前网页是否一致，如果不一致则浏览器会进行限制，比如在www.baidu.com的某个网页中，如果使用ajax去访问www.jd.com是不行的，但是如果是img、iframe、scipt等标签的sc属性去访问则是可以的，之所以浏览器要做这层限制，是为了用户信息安全。但是如果开发者想要绕过这层限制也是可以的: 1.response添加header，比如resp.setHeader(“Access-Control-Allow-Origin”*);表示可以访问所有网站，不受是否同源的限制 2.jsonp的方式，该技术底层就是基于script标签来实现的，因为script标签是可以跨域的 3.后台自己控制，先访问同域名下的接口，然后在接口中再去使用HTTPClient等工具去调用目标接口 4.网关，和第三种方式类似，都是交给后台服务来进行跨域访问 7.Netty是什么？和Tomcat有什么区别？特点是什么？ Netty是一个基于NIO的异步网络通信框架，性能高，封装了原生NIO编码的复杂度，开发者可以直接使用Netty来开发高效率的各种网络服务器，并且编码简单。Tomcat是一个Web服务器，是一个Servlet容器，基本上Tomcat内部只会运行Servlet程序，并处理HTTP请求，而Netty封装的是底层IO模型，关注的是网络数据的传输，而不关心具体的协议，可定制性更高。 Netty的特点: 异步、NIO的网络通信框架 高性能 高扩展，高定制性 易用性 8.TCP的三次握手和四次挥手过程 TCP协议是7层网络协议中的传输层协议，负责数据的可靠传输。在建立TCP连接时，需要通过三次握手来建立，过程是: 客户端向服务端发送一个SYN 服务端接收到SYN后，给客户端发送一个SYN_ACK 客户端接收到SYN_ACK后，再给服务端发送一个ACK 在断开TCP连接时，需要通过四次挥手来断开，过程是: 客户端向服务端发送FIN 服务端接收FIN后，向客户端发送ACK，表示我接收到了断开连接的请求，客户端你可以不发数据了，不过服务端这边可能还有数据正在处理 服务端处理完所有数据后，向客户端发送FIN，表示服务端现在可以断开连接 客户端收到服务端的FIN，向服务端发送ACK，表示客户端也会断开连接了 SYN表示建立连接，FIN表示关闭连接，ACK表示响应，PSH表示有 DATA数据传输，RST表示连接重置。 额外问题: 为什么要三次握手？两次握手不行吗？ 三次握手的目的是为了确认客户端和服务端的接收能力和发送能力。 第一次握手成功时，服务端确认客户端的发送能力和服务端的接收能力； 第二次握手成功时，客户端确认服务端的发送能力和接收能力和客户端的发送能力和接收能力，但此时服务端不能确认客户端的接收能力和服务端的发送能力； 第三次握手成功时，服务端和客户端都确认了双方的发送能力和接收能力。 如果是两次握手还会出现一个问题，客户端的第一次SYN请求在网络中阻塞时，客户端重新发送第二次SYN请求，服务器收到第二次SYN请求后，成功与客户端两次握手，双方建立连接，在数据传输结束后，双方断开链接，这时，第一次的SYN请求在服务端到来，服务端会认为客户端想要重新建立链接，给客户端发出确认建立连接，会一直等待客户端发送数据，而客户端已经完成了自己的数据传输任务，不会再给服务端发信息，于是服务端就一直等待，造成了资源的浪费。 9.大文件上传如何做断点续传？ 不管怎样简单的需求，在量级达到一定层次时，都会变得异常复杂 文件上传简单，文件变大就复杂 上传大文件时，以下几个变量会影响我们的用户体验 服务器处理数据的能力 请求超时 网络波动 上传时间会变长，高频次文件上传失败，失败后又需要重新上传等等 为了解决上述问题，我们需要对大文件上传单独处理 这里涉及到分片上传及断点续传两个概念分片上传 分片上传分片上传，就是将所要上传的文件，按照一定的大小，将整个文件分隔成多个数据块（Part）来进行分片上传 如下图 上传完之后再由服务端对所有上传的文件进行汇总整合成原始的文件 大致流程如下： 将需要上传的文件按照一定的分割规则，分割成相同大小的数据块； 初始化一个分片上传任务，返回本次分片上传唯一标识； 按照一定的策略（串行或并行）发送各个分片数据块； 发送完成后，服务端根据判断数据上传是否完整，如果完整，则进行数据块合成得到原始文件 断点续传断点续传指的是在下载或上传时，将下载或上传任务人为的划分为几个部分 每一个部分采用一个线程进行上传或下载，如果碰到网络故障，可以从已经上传或下载的部分开始继续上传或下载未完成的部分，而没有必要从头开始上传下载。用户可以节省时间，提高速度 一般实现方式有两种： 服务器端返回，告知从哪开始 浏览器端自行处理 上传过程中将文件在服务器写为临时文件，等全部写完了（文件上传完），将此临时文件重命名为正式文件即可 如果中途上传中断过，下次上传的时候根据当前临时文件大小，作为在客户端读取文件的偏移量，从此位置继续读取文件数据块，上传到服务器从此偏移量继续写入文件即可 10.为什么Https用两种加密算法？ HTTPS 在内容传输的加密上使用的是对称加密，非对称加密只作用在证书验证阶段。 HTTPS的整体过程分为证书验证和数据传输阶段，具体的交互过程如下： ① 证书验证阶段： 123451）浏览器发起 HTTPS 请求；2）服务端返回 HTTPS 证书；3）客户端验证证书是否合法，如果不合法则提示告警。 ② 数据传输阶段： 1234567891）当证书验证合法后，在本地生成随机数；2）通过公钥加密随机数，并把加密后的随机数传输到服务端；3）服务端通过私钥对随机数进行解密；4）服务端通过客户端传入的随机数构造对称加密算法，对返回结果内容进行加密后传输。 5) 客户端再用生成的随机值对返回内容进行对应的对称解密，获取内容 为什么数据传输是用对称加密？首先：非对称加密的加解密效率是非常低的，而 http 的应用场景中通常端与端之间存在大量的交互，非对称加密的效率是无法接受的。 另外：在 HTTPS 的场景中只有服务端保存了私钥，一对公私钥只能实现单向的加解密，所以 HTTPS 中内容传输加密采取的是对称加密，而不是非对称加密 内容使用对称加密是为了传输效率 非对称加密是为了验证证书合法性","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://gwtt.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"Springboot面试(1)","slug":"Springboot面试(1)","date":"2022-06-12T14:52:33.501Z","updated":"2022-07-09T14:40:37.958Z","comments":true,"path":"2022/06/12/Springboot面试(1)/","link":"","permalink":"https://gwtt.github.io/2022/06/12/Springboot%E9%9D%A2%E8%AF%95(1)/","excerpt":"","text":"springboot(1)1.说说对springmvc的理解 什么是MVCmvc是一种设计模式（设计模式就是日常开发中编写代码的一种好的方法和经验的总结）。模型（model）-视图（view）-控制器（controller），三层架构的设计模式。用于实现前端页面的展现与后端业务数据处理的分离。 mvc设计模式的好处 1.分层设计，实现了业务系统各个组件之间的解耦，有利于业务系统的可扩展性，可维护性。 2.有利于系统的并行开发，提升开发效率。 SpringMVC流程 1、 用户发送请求至前端控制器DispatcherServlet。 2、 DispatcherServlet收到请求调用HandlerMapping处理器映射器。 3、 处理器映射器找到具体的处理器(可以根据xml配置、注解进行查找)，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet。 4、 DispatcherServlet调用HandlerAdapter处理器适配器。 5、 HandlerAdapter经过适配调用具体的处理器(Controller，也叫后端控制器)。 6、 Controller执行完成返回ModelAndView。 7、 HandlerAdapter将controller执行结果ModelAndView返回给DispatcherServlet。 8、 DispatcherServlet将ModelAndView传给ViewReslover视图解析器。 9、 ViewReslover解析后返回具体View。 10、DispatcherServlet根据View进行渲染视图（即将模型数据填充至视图中）。 11、 DispatcherServlet响应用户。 核心组件DispatcherServlet（前端控制器）说明：Spring MVC 的入口函数，接收请求，响应结果，相当于转发器，中央处理器，它就相当于mvc模式中DispatcherServlet的存在降低了组件之间的耦合性。HandlerMapping(处理器映射器）说明：根据请求的url查找Handler（即处理器Controller），映射方式有配置文件方式，实现接口方式，注解方式等。HandlerAdapter(处理器适配器)说明：HandlerAdapter是适配器模式的应用，按照HandlerAdapter要求的规则去执行Handler。Handler（处理器）说明：Handler需要开发工程师按照HandlerAdapter的要求去做，是后端控制器，处理具体的业务逻辑。View resolver（视图解析器）说明：进行视图解析，首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。视图View说明：View是一个接口，实现类支持不同的View类型（jsp、freemarker、pdf等） springMVC是一个MVC的开源框架，springMVC=struts2+spring，springMVC就相当于是Struts2加上sring的整合，但是这里有一个疑惑就是，springMVC和spring是什么样的关系呢？这个在百度百科上有一个很好的解释：意思是说，springMVC是spring的一个后续产品，其实就是spring在原有基础上，又提供了web应用的MVC模块，可以简单的把springMVC理解为是spring的一个模块（类似AOP，IOC这样的模块），网络上经常会说springMVC和spring无缝集成，其实springMVC就是spring的一个子模块，所以根本不需要同spring进行整合。 2.说说ioc和aop理解和原理 IOCIOC叫做控制反转，指的是通过Spring来管理对象创建、配置和生命周期，这样相当于把控制权交给了Spring，不需要人工来管理对象之间的复杂的依赖关系，这样做的好处就是解耦！ 在Spring里面，主要提供BeanFactory和ApplicationContext两种IOC容器，通过它们来实现对Bean的管理！ AOP叫做面向切面编程，他是一个编程范式，目的就是提高代码的模块性。Spring AOP基于动态代理的方式实现，如果是实现了接口的话就会使用JDK动态代理，反之则使用CGLIB代理。Spring中AOP的应用主要体现在事务、日志、异常处理等方面，通过在代码的前后做一些增强处理，可以实现对业务的逻辑隔离，提高代码的模块化能力，同时也是解耦。Spring主要提供了Aspect切面，JoinPoint连接点、PointCut切入点、Advice增强等实现方式。 3.说说springboot的理解 在使用spring框架的时候,我们需要配置applicationContext.xml的配置文件,并且在pom文件中,我们需要导入相关的依赖jar包以及版本,这样在使用的过程中,就显得有些复杂. 而springboot利用起步依赖和自动装配就很好的解决了一个问题,首先,我们来说说起步依赖,在springboot的pom文件中,我们依赖父工程 ,在父工程中,定义了相关的坐标以及版本信息,这样我们继承了这个父工程的时候,就不需要指定版本信息,在我们的项目中.依赖了start-web这个坐标,而这个坐标依赖了spring以及springmvc的坐标,这样,根据依赖传递我们的项目就间接依赖了spring和mvc坐标,所有我们只需要导入start-web这一个坐标就可以了 接下来我们说说自动装配,在springboot的启动类上有一个@SpringBootApplication注解,这个注解也是springboot项目的核心注解,在@SpringBootApplication注解中,有一个@EnableAutoConfiguration注解,也就是开启自动配置的意思,这个注解的关键功能由@Import提供,其导入的AutoConfigurationImportSelector中的selectImports()方法能够找到META-INF/spring.factories配置文件中的所有启动配置类,并对其进行加载,而这些自动配置类都是以AtuoConfiguration结尾来命名的,它实际上就是一个JavaConfig形式的Spring容器配置类,通过@ConfigurationProperties注解,绑定到对应的以properties结尾的配置实体类上封装为一个bean,然后在通过@EnableAutoConfiguration注解导入到Spring容器中,这样自动装配就完成了. 4.使用过过滤器或者拦截器吗，说说拦截器的方法和实现的接口 使用过 用过滤器主要做过全局网关配置请求token和跨域问题，实现Filter接口 用拦截器主要做日志输出，实现HandlerInterceptor 接口，有三个方法，preHandle，postHandle，afterCompletion，分别是请求之前调用，之后调用，结束后调用，然后相比过滤器，它需要在springmvc注入，在启动类添加配置代码。实现WebMvcConfigurer接口添加拦截器。 5.aop在什么情况下会使用到 AOP的应用场景：1.日志处理2.用户登录3.权限（Authentication ）4.性能优化（Performance optimization）5.事务（Transactions ）6.记录跟踪 优化 校准（logging, tracing, profiling and monitoring）7.调试（Debugging）8.懒加载（Lazy loading）9.错误处理（Error handling）10.资源池（Resource pooling）11.同步（Synchronization） 6.如果有过滤器和拦截器和aop，他们的执行顺序是什么 执行顺序依次是过滤器、拦截器、切面 7.Spring 中的bean 是线程安全的吗 Spring本身并没有针对Bean做线程安全的处理，所以: 1.如果Bean是无状态的，那么Bean则是线程安全的 2.如果Bean是有状态的，那么Bean则不是线程安全的 另外，Bean是不是线程安全，跟Bean的作用域没有关系，Bean的作用域只是表示Bean的生命周期范围，对于任何生命周期的Bean都是一个对象，这个对象是不是线程安全的，还是得看这个Bean对象本身。 (有状态会话bean ：每个用户有自己特有的一个实例，在用户的生存期内，bean保持了用户的信息，即“有状态”；一旦用户灭亡（调用结束或实例结束），bean的生命期也告结束。即每个用户最初都会得到一个初始的bean。简单来说，有状态就是有数据存储功能。有状态对象(Stateful Bean)，就是有实例变量的对象 ，可以保存数据，是非线程安全的。无状态会话bean ：bean一旦实例化就被加进会话池中，各个用户都可以共用。即使用户已经消亡，bean 的生命期也不一定结束，它可能依然存在于会话池中，供其他用户调用。由于没有特定的用户，那么也就不能保持某一用户的状态，所以叫无状态bean。但无状态会话bean 并非没有状态，如果它有自己的属性（变量），那么这些变量就会受到所有调用它的用户的影响，这是在实际应用中必须注意的。简单来说，无状态就是一次操作，不能保存数据。无状态对象(Stateless Bean)，就是没有实例变量的对象 .不能保存数据，是不变类，是线程安全的。) 8.BeanFactory和ApplicationContext的区别 BeanFactory： 是Spring里面最低层的接口，提供了最简单的容器的功能，只提供了实例化对象和拿对象的功能； ApplicationContext： 应用上下文，继承BeanFactory接口，它是Spring的一各更高级的容器，提供了更多的有用的功能； 国际化（MessageSource） 访问资源，如URL和文件（ResourceLoader） 载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的web层 消息发送、响应机制（ApplicationEventPublisher） AOP（拦截器） 9.@Transactional 注解的失效场景 1、@Transactional 应用在非 public 修饰的方法上 如果Transactional注解应用在非public 修饰的方法上，Transactional将会失效。 注意：protected、private 修饰的方法上使用 @Transactional 注解，虽然事务无效，但不会有任何报错，这是我们很容犯错的一点。 2、@Transactional 注解属性 propagation 设置错误 这种失效是由于配置错误，若是错误的配置以下三种 propagation，事务将不会发生回滚。 TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。 3、@Transactional 注解属性 rollbackFor 设置错误 rollbackFor 可以指定能够触发事务回滚的异常类型。Spring默认抛出了未检查unchecked异常（继承自 RuntimeException 的异常）或者 Error才回滚事务，其他异常不会触发回滚事务。如果在事务中抛出其他类型的异常，但却期望 Spring 能够回滚事务，就需要指定rollbackFor属性。 4、同一个类中方法调用，导致@Transactional失效 开发中避免不了会对同一个类里面的方法调用，比如有一个类Test，它的一个方法A，A再调用本类的方法B（不论方法B是用public还是private修饰），但方法A没有声明注解事务，而B方法有。当外界调用方法A之后，方法B的事务是不会起作用的。这也是经常犯错误的一个地方。那为什么会出现这种情况？其实这还是由于使用Spring AOP代理造成的，因为只有当事务方法被当前类以外的代码调用时，才会由Spring生成的代理对象来管理。 10.spring运用了哪些设计模式 工厂模式Spring使用工厂模式可以通过 BeanFactory 或 ApplicationContext 创建 bean 对象。 代理模式Spring AOP就是基于动态代理的，如果要代理的对象，实现了某个接口，那么Spring AOP会使用JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用JDK Proxy去进行代理了，这时候Spring AOP会使用Cglib，这时候Spring AOP会使用Cglib生成一个被代理对象的子类来作为代理。 模板方法模式Spring中jdbcTemplate、hibernateTemplate等以Template结尾的对数据库操作的类，它们就使用到模板模式。一般情况下，我们都是使用继承的方式来实现模板模式，但是Spring并没有使用这种方式，而是使用Callback模式与模板方法配合，既达到了代码复用的效果，同时增加了灵活性。 观察者模式观察者设计模式是一种对象行为模式。它表示的是一种对象与对象之间具有依赖关系，当一个对象发生改变时，这个对象锁依赖的对象也会做出反应。Spring事件驱动模型就是观察者模式很经典的应用。 适配器 模式适配器设计模式将一个接口转换成客户希望的另一个接口，适配器模式使得接口不兼容的那些类可以一起工作，其别名为包装器。在Spring MVC中，DispatcherServlet根据请求信息调用HandlerMapping，解析请求对应的Handler，解析到对应的Handler（也就是我们常说的Controller控制器）后，开始由HandlerAdapter适配器处理。为什么要在Spring MVC中使用适配器模式？Spring MVC中的Controller种类众多不同类型的Controller通过不同的方法来对请求进行处理，有利于代码的维护拓展。 装饰者模式装饰者设计模式可以动态地给对象增加些额外的属性或行为。相比于使用继承，装饰者模式更加灵活。Spring 中配置DataSource的时候，DataSource可能是不同的数据库和数据源。我们能否根据客户的需求在少修改原有类的代码下切换不同的数据源？这个时候据需要用到装饰者模式。 策略模式Spring 框架的资源访问接口就是基于策略设计模式实现的。该接口提供了更强的资源访问能力，Spring框架本身大量使用了Resource接口来访问底层资源。Resource接口本身没有提供访问任何底层资源的实现逻辑，针对不同的额底层资源，Spring将会提供不同的Resource实现类，不同的实现类负责不同的资源访问类型。 11.Springboot中常用注解有哪些，简述一下注解的作用 @SpringBootApplication 这个注解很常见了，每次在启动SpringBoot项目的时候，都会见到它，它作用在类上，标识该类为SpringBoot项目启动类。并且让SpringBoot自动给程序进行必要的配置，等同于@Configuration、@EnableAutoConfiguration、@ComponentScan这三个注解。 @Configuration 相当于传统Spring的xml配置文件。 如果第三方库需要用到xml文件，建议通过@Configuration类作为项目的配置主类，可以使用@ImportResource注解加载xml配置文件。 @EnableAutoConfiguration 自动配置。 SpringBoot自动配置(auto-configuration)，尝试根据你添加的启动器(starter)自动配置你的SpringBoot应用。 你可以将@EnableAutoConfiguration注解或者@SpringBootApplication注解添加到一个@Configuration类上来选择自动配置。 如果发现应用了你不想要的特定自动配置类，你可以使用@EnableAutoConfiguration注解的排除属性来禁用它们。 @ComponentScan 如果某个类加上@ComponentScan注解，则该类会自动发现扫描组件。 也就是说，如果扫描到有@Component、@Controller、@Service等这些注解的类，并注册为Bean，可以自动收集所有的Spring组件，包括@Configuration类。 我们经常使用@ComponentScan注解搜索beans，并结合@Autowired注解导入。如果没有配置的话，SpringBoot会扫描启动类所在包下以及子包下的使用了@Component、@Controller、@Service、@Repository等注解的类。 @Controller @Controller注解用于定义控制器类，在SpringBoot中由控制器负责将用户发来的URL请求转发到对应的服务接口(Controller层)。 一般这个注解用在类上，控制器方法需要加上@RequestMapping注解。 @ResponseBody 如果控制器方法加上@ResponseBody注解，该方法的返回结果将直接写入HTTP response body中，一般在异步获取数据时使用，用于构建RESTful的api。 在使用@RequestMapping后，返回值通常解析为跳转路径，加上@esponsebody后返回结果不会被解析为跳转路径，而是直接写入HTTP response body中。 比如异步获取json数据，加上@Responsebody后，会直接返回json数据。该注解一般会配合@RequestMapping一起使用。 @RestController @RestController注解是@Controller和@ResponseBody的合集。 使用在类上，表示该类是控制器，并且类中所有控制器方法的返回值直接填入HTTP响应体中，是RESTful风格的控制器，控制器方法返回JSON数据。 @RequestMapping 提供路由信息，负责URL到Controller中的具体方法的映射。 @Import 用来导入其他配置类（加了@Configuration注解的类）。 @ImportResource 用来加载xml配置文件。 @PathVariable 获取URL上的参数（Restful风格接口）。 @Service 一般用于修饰service层的组件 @Repository 使用@Repository注解可以确保DAO或者repositories提供异常转译，这个注解修饰的DAO或者repositories类会被ComponetScan发现并配置，同时也不需要为它们提供XML配置项。 @Bean 用@Bean标注方法等价于XML中配置的bean，意思是产生一个bean，并交给SpringBoot管理。 @Value 注入SpringBoot中的配置文件——application.properties配置的属性的值。 @Inject 等价于默认的@Autowired，只是没有required属性。 @Component 泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。 @AutoWired 自动导入依赖的bean。byType方式。把配置好的Bean拿来用，完成属性、方法的组装，它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作。当加上（required=false）时，就算找不到bean也不报错。 @Qualifier 当有多个同一类型的Bean时，可以用@Qualifier(“name”)来指定。与@Autowired配合使用。@Qualifier限定描述符除了能根据名字进行注入，但能进行更细粒度的控制如何选择候选者。 @Resource(name=”name”,type=”type”) 没有括号内内容的话，默认byName。与@Autowired干类似的事。 附页 注解 含义 @Component 最普通的组件，可以被注入到spring容器进行管理 @Repository 作用于持久层 @Service 作用于业务逻辑层 @Controller 作用于表现层（spring-mvc的注解） 这几个注解几乎可以说是一样的：因为被这些注解修饰的类就会被Spring扫描到并注入到Spring的bean容器中。 @Autowired是根据类型进行自动装配的，如果找到多个类型相同的，会按照名称进行匹配，如果名称相同，会报错，如果需要按指定名称进行装配，则需要配合@Qualifier； @Inject是根据类型进行自动装配的，如果需要按名称进行装配，则需要配合@Named； @Resource是根据名称进行自动装配的，一般会指定一个name属性 总结: 1、@Autowired是spring自带的，@Inject是JSR330规范实现的，@Resource是JSR250规范实现的，需要导入不同的包 2、@Autowired、@Inject用法基本一样，不同的是@Autowired有一个request属性 3、@Autowired、@Inject是默认按照类型匹配的，@Resource是按照名称匹配的 4、@Autowired如果需要按照名称匹配需要和@Qualifier一起使用，@Inject和@Name一起使用","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Springboot","slug":"Springboot","permalink":"https://gwtt.github.io/tags/Springboot/"}]},{"title":"Juc面试(1)","slug":"Juc面试(1)","date":"2022-06-11T03:22:48.894Z","updated":"2022-09-21T11:21:57.582Z","comments":true,"path":"2022/06/11/Juc面试(1)/","link":"","permalink":"https://gwtt.github.io/2022/06/11/Juc%E9%9D%A2%E8%AF%95(1)/","excerpt":"","text":"juc(1)1.了解线程吗，他和进程有什么区别 线程的基本概念 线程是进程中执行运算的最小单位，是进程中的一个实体，是被系统独立调度和分派的基本单位，线程基本上自己不拥有系统资源，只拥有一点在运行中必不可少的资源，但它可与同属一个进程的其它线程共享进程所拥有的全部资源。一个线程可以创建和撤消另一个线程，同一进程中的多个线程之间可以并发执行。 好处 ： ​ （1）易于调度。 ​ （2）提高并发性。通过线程可方便有效地实现并发性。进程可创建多个线程来执行同一程序的不同部分。 ​ （3）开销少。创建线程比创建进程要快，所需开销很少。 ​ （4）利于充分发挥多处理器的功能。通过创建多线程进程，每个线程在一个处理器上运行，从而实现应用程序的并发性，使每个处理器都得到充分运行 线程和进程的区别一 简单地讲,任何的一个程序都必须有且有一个以上的进程,而相对于一个进程而言也必须要有且有一个以上的线程。相对于进程而言，对线程进行划分的尺度一般要小很多，这就导致了多线程的一些程序能够出现更高的并发性。 （线程相比进程划分尺度小，迸发性更高） 线程和进程的区别二 在执行进程的时候，一般会具有相互独立的多个内存单元。但是多个线程是可以共享内存的，这样运行效率就很大的程度上被提高了。相对于单个的独立进程而言都会有相应程序的运行入口以及一些程序等出口。线程就不一样了，它不能独立的去执行而必须要依附在相应的应用程序里面。这样的话应用程序就可以执行多个线程并进行相应的控制。 （同一进程的线程共享内存，运行效率高） 线程和进程的区别三 通过了解逻辑角度我们可以得知，多线程这样的意义是相对于在一个应用程序里面的，能够同时的执行。而操作系统不会认为多个线程就是多个独立应用，因此也就不会使其调度以及管理实现资源的分配。 (线程没有资源分配) 123456789了解的，线程时进程中执行运算的最小单位，是被系统独立调度和分派的基本单位。可以创建线程或者撤销另一个线程，可以迸发执行。区别： （1）调度：线程作为调度和分配的基本单位，进程作为拥有资源的基本单位（2）并发性：不仅进程之间可以并发执行，同一个进程的多个线程之间也可并发执行（3）拥有资源：进程是拥有资源的一个独立单位，线程不拥有系统资源，但可以访问隶属于进程的资源.（4）系统开销：在创建或撤消进程时，由于系统都要为之分配和回收资源，导致系统的开销明显大于创建或撤消线程时的开销。 进程和线程的区别是什么？ 进程是执行着的应用程序，而线程是进程内部的一个执行序列。一个进程可以有多个线程。线程又叫做轻量级进程。 2.线程和进程的通信方式了解吗 进程间通信方式进程间通信主要包括管道、系统IPC（包括消息队列、信号量、信号、共享内存等）、以及套接字socket。 （1）管道（Pipe）：管道可用于具有亲缘关系进程间的通信，允许一个进程和另一个与它有共同祖先的进程之间进行通信。 （2）命名管道（named pipe）：命名管道克服了管道没有名字的限制，除具有管道所具有的功能外，它还允许无亲缘关系进程间的通信。 （3）信号（Signal）：信号是比较复杂的通信方式，用于通知接受进程有某种事件发生，除了用于进程间通信外，进程还可以发送 信号给进程本身。 （4）消息（Message）队列：消息队列是消息的链接表，包括Posix消息队列system V消息队列。 （5）共享内存：使得多个进程可以访问同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。 （6）内存映射（mapped memory）：内存映射允许任何多个进程间通信，每一个使用该机制的进程通过把一个共享的文件映射到自己的进程地址空间来实现它。 （7）信号量（semaphore）：主要作为进程间以及同一进程不同线程之间的同步手段。 （8）套接口（Socket）：更为一般的进程间通信机制，可用于不同机器之间的进程间通信。 线程间通信的方式 临界区：通过多线程的串行化来访问公共资源或一段代码，速度快，适合控制数据访问；互斥量: Synchronized/Lock：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问信号量: Semphare：为控制具有有限数量的用户资源而设计的，它允许多个线程在同一时刻去访问同一个资源，但一般需要限制同一时刻访问此资源的最大线程数目。事件(信号)，Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较 3.线程的创建方式 四种方式 （1）通过继承Thread类来创建新的线程。 （2）通过实现Runnable接口来创建线程。 Thread和Runnable区别： Thread 是类，需要继承，但是一个类只能继承一个类，一旦继承 Thread 之后，无法在继承其他的类。 Runnable 是接口，一个类可以实现多个接口，所以实现Runnable接口可拓展性更好 Thread是Runnable接口的实现类 （3）通过实现Callable接口来创建线程。 Runnable和Callable的区别 Runnable没有返回值。 Callable可以返回值。 （4）自定义线程池（推荐）,从中获取线程。 4.sleep和wait的区别 (1)限制 使用 sleep 方法可以让让当前线程休眠，时间一到当前线程继续往下执行，在任何地方都能使用，但需要捕获 InterruptedException 异常。 而使用 wait 方法则必须放在 synchronized 块里面，同样需要捕获 InterruptedException 异常，并且需要获取对象的锁。而且 wait 还需要额外的方法 notify/ notifyAll 进行唤醒，它们同样需要放在 synchronized 块里面，且获取对象的锁。 (2)使用场景 sleep 一般用于当前线程休眠，或者轮循暂停操作，wait 则多用于多线程之间的通信。 (3)类不同 sleep 是 Thread 类的静态本地方法，wait 则是 Object 类的本地方法。 (4)释放锁 wait 可以释放当前线程对 lock 对象锁的持有，而 sleep 则不会。 (5)线程切换 sleep 会让出 CPU 执行时间且强制上下文切换，而 wait 则不一定，wait 后可能还是有机会重新竞争到锁继续执行的。 5.run和start的区别 1.线程中的start()方法和run()方法的主要区别在于，当程序调用start()方法，将会创建一个新线程去执行run()方法中的代码。但是如果直接调用run()方法的话，会直接在当前线程中执行run()中的代码，注意，这里不会创建新线程。这样run()就像一个普通方法一样。 2.另外当一个线程启动之后，不能重复调用start()，否则会报IllegalStateException异常。但是可以重复调用run()方法。 总结起来就是run()就是一个普通的方法，而start()会创建一个新线程去执行run()的代码。 6.了解synchronize吗 修饰实例方法，对于普通同步方法，锁是当前的实例对象 修饰静态方法，对于静态同步方法，锁是当前的Class对象 修饰方法代码块，对于同步方法块，锁是synchronized括号里面配置的对象！ 7.创建线程池的核心参数有哪些？多讲几个? 123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; corePoolSize：线程池的核心线程数。核心线程会一直存活，即便没有任务需要执行，当线程数小于核心线程数时，即使有线程空闲，线程池也会优先创建新线程处理。设置allowCoreThreadTimeout=true（默认false）时，核心线程会超时关闭。 maximumPoolSize：线程池允许的最大线程数。当线程数&gt;=corePoolSize，且任务队列已满时。线程池会创建新线程来处理任务。当线程数=maxPoolSize，且任务队列已满时，线程池会拒绝处理任务而抛出异常。 keepAliveTime：线程空闲时间。当线程空闲时间达到keepAliveTime时，线程会退出，直到线程数量=corePoolSize。如果allowCoreThreadTimeout=true，则会直到线程数量=0。 unit：keepAliveTime的时间单位。 workQueue：当核心线程数达到最大时，新任务会放在队列中排队等待执行。 threadFactory：线程工厂，ThreadFactory是一个接口，只有一个方法，即newThread(Runnable r)。从这个方法名字就可以知道，这接口是用来创建新的线程的。其使用也很简单，仅仅只需要实现newThread方法，根据自己的需要进行线程的创建即可。 handler：任务拒绝处理器。两种情况会拒绝处理任务： 1）、当线程数已经达到maxPoolSize，切队列已满，会拒绝新任务； 2）、当线程池被调用shutdown()后，会等待线程池里的任务执行完毕，再shutdown。如果在调用shutdown()和线程池真正shutdown之间提交任务，会拒绝新任务线程池会调用rejectedExecutionHandler来处理这个任务。如果没有设置默认是AbortPolicy，会抛出异常。拒绝策略有如下几种： AbortPolicy 丢弃任务，抛运行时异常。 CallerRunsPolicy 如果任务被拒绝了，则由调用线程（提交任务的线程）直接执行此任务 DiscardPolicy 丢弃任务，不抛出异常 DiscardOldestPolicy 从队列中踢出最先进入队列（最后一个执行）的任务，重新提交被拒绝的任务。 9.说说你对volatile关键字的理解 被volatile修饰的共享变量，就具有了以下两点特性： 1 . 保证了不同线程对该变量操作的内存可见性; 2 . 禁止指令重排序 连环问(1)能不能详细说下什么是内存可见性，什么又是重排序呢？ 内存可见性是指当一个线程修改了某个变量的值，其它线程总是能知道这个变量变化。 也就是说，如果线程 A 修改了共享变量 V 的值，那么线程 B 在使用 V 的值时，能立即读到 V 的最新值。 重排序是指编译器和处理器为了优化程序性能而对指令序列进行重新排序的一种手段。 10.说一下公平锁和非公平锁区别 1234公平锁：多个线程按照申请锁的顺序去获得锁，线程会直接进入队列去排队，永远都是队列的第一位才能得到锁。优点：所有的线程都能得到资源，不会饿死在队列中。缺点：吞吐量会下降很多，队列里面除了第一个线程，其他的线程都会阻塞，cpu唤醒阻塞线程的开销会很大。 1234非公平锁：多个线程去获取锁的时候，会直接去尝试获取，获取不到，再去进入等待队列，如果能获取到，就直接获取到锁。优点：可以减少CPU唤醒线程的开销，整体的吞吐效率会高点，CPU也不必取唤醒所有线程，会减少唤起线程的数量。缺点：你们可能也发现了，这样可能导致队列中间的线程一直获取不到锁或者长时间获取不到锁，导致饿死。 11.ReentrantLock的trylock和lock区别 1: lock拿不到锁会一直等待，并且没有返回值。tryLock是去尝试，拿不到就返回false，拿到返回true。 2: tryLock是可以被打断的，被中断的，lock是不可以。 12.CountDownLatch和Semaphore的区别和底层原理 CountDownLatch表示计数器，可以给CountDownLatch设置一个数字，一个线程调用CountDownLatch的await()将会阻塞，其他线程可以调用CountDownLatch的countDown()方法来对CountDownLatch中的数字减一，当数字被减成o后，所有await的线程都将被唤醒. 对应的底层原理就是，调用await()方法的线程会利用AQS排队，一旦数字被减为O，则会将AQS中排队的线程依次唤醒。 Semaphore表示信号量，可以设置许可的个数，表示同时允许最多多少个线程使用该信号量，通过acquire()来获取许可，如果没有许可可用则线程阻塞，并通过AQS来排队，可以通过release()方法来释放许可，当某个线程释放了某个许可后，会从AQS中正在排队的第一个线程开始依次唤醒，直到没有空闲许可。 13.Sychronized的偏向锁、轻量级锁、重量级锁 1.偏向锁:在锁对象的对象头中记录一下当前获取到该锁的线程ID，该线程下次如果又来获取该锁就可以直接获取到了 ⒉轻量级锁:由偏向锁升级而来，当一个线程获取到锁后，此时这把锁是偏向锁，此时如果有第二个线程来竞争锁，偏向锁就会升级为轻量级锁,之所以叫轻量级锁，是为了和重量级锁区分开来，轻量级锁底层是通过自旋来实现的，并不会阻塞线程 3.如果自旋次数过多仍然没有获取到锁，则会升级为重量级锁，重量级锁会导致线程阻塞 4.自旋锁:自旋锁就是线程在获取锁的过程中，不会去阻塞线程，也就无所谓唤醒线程。阻塞和唤醒这两个步骤都是需要操作系统去进行的，比较消耗时间，自旋锁是线程通过CAS获取预期的一个标记，如果没有获取到，则继续循环获取，如果获取到了则表示获取到了锁，这个过程线程一直在运行中，相对而言没有使用太多的操作系统资源，比较轻量。 14.Sychronized和ReentrantLock的区别 sychronized是一个关键字，ReentrantLock是一个类 sychronized会自动的加锁与释放锁,ReentrantLock需要程序员手动加锁与释放锁 sychronized的底层是JVM层面的锁，ReentrantLock是APl层面的锁 sychronized是非公平锁，ReentrantLock可以选择公平锁或非公平锁 sychronized锁的是对象，锁信息保存在对象头中，ReentrantLock通过代码中int类型的state标识来标识锁的状态 sychronized底层有一个锁升级的过程 15.线程池的底层工作原理 线程池内部是通过队列+线程实现的，当我们利用线程池执行任务时: 1.如果此时线程池中的线程数量小于corePoolSize，即使线程池中的线程都处于空闲状态，也要创建新的线程来处理被添加的任务。 ⒉.如果此时线程池中的线程数量等于corePoolSize，但是缓冲队列workQueue未满，那么任务被放入缓冲队列。 3.如果此时线程池中的线程数量大于等于corePoolSize，缓冲队列workQueue满，并且线程池中的数量小于maximumPoolSize，建新的线程来处理被添加的任务。 4.如果此时线程池中的线程数量大于corePoolSize，缓冲队列workQueue满，并且线程池中的数量等于maximumPoolSize，那么通过 handler所指定的策略来处理此任务。 5.当线程池中的线程数量大于corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终止。这样，线程池可以动态的调整池中的线程数 16.BIO,NIO,AIO分别是什么 1.BIO:同步阻塞lO，使用BIO读取数据时，线程会阻塞住，并且需要线程主动去查询是否有数据可读，并且需要处理完一个Socket之后才能处理下一个Socket 2.NIO:同步非阻塞lO，使用NIO读取数据时，线程不会阻塞，但需要线程主动的去查询是否有IO事件 3.AlO:也叫做NIO 2.0，异步非阻塞lO，使用AIO读取数据时，线程不会阻塞，并且当有数据可读时会通知给线程，不需要线程主动去查询","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"juc","slug":"juc","permalink":"https://gwtt.github.io/tags/juc/"}]},{"title":"Java面试基础（1）","slug":"Java面试基础(1)","date":"2022-06-11T03:07:00.000Z","updated":"2022-07-02T06:37:19.100Z","comments":true,"path":"2022/06/11/Java面试基础(1)/","link":"","permalink":"https://gwtt.github.io/2022/06/11/Java%E9%9D%A2%E8%AF%95%E5%9F%BA%E7%A1%80(1)/","excerpt":"","text":"java基础(1)1.int和interger的区别 Integer是int的包装类；int是基本数据类型； Integer变量必须实例化后才能使用；int变量不需要； Integer实际是对象的引用，指向此new的Integer对象；int是直接存储数据值 ； Integer的默认值是null；int的默认值是0。 延伸：关于Integer和int的比较1、由于Integer变量实际上是对一个Integer对象的引用，所以两个通过new生成的Integer变量永远是不相等的（因为new生成的是两个对象，其内存地址不同）。 123Integer i = new Integer(100);Integer j = new Integer(100);System.out.print(i == j); //false 2、Integer变量和int变量比较时，只要两个变量的值是相等的，则结果为true（因为包装类Integer和基本数据类型int比较时，java会自动拆包装为int，然后进行比较，实际上就变为两个int变量的比较） 123Integer i = new Integer(100);int j = 100；System.out.print(i == j); //true 3、非new生成的Integer变量和new Integer()生成的变量比较时，结果为false。（因为 ①当变量值在-128到127之间时，非new生成的Integer变量指向的是java常量池中的对象，而new Integer()生成的变量指向堆中新建的对象，两者在内存中的地址不同；②当变量值不在-128到127之间时，非new生成Integer变量时，java API中最终会按照new Integer(i)进行处理（参考下面第4条），最终两个Interger的地址同样是不相同的） 123Integer i = new Integer(100);Integer j = 100;System.out.print(i == j); //false 4、对于两个非new生成的Integer对象，进行比较时，如果两个变量的值在区间-128到127之间，则比较结果为true，如果两个变量的值不在此区间，则比较结果为false 123456Integer i = 100;Integer j = 100;System.out.print(i == j); //trueInteger i = 128;Integer j = 128;System.out.print(i == j); //false 对于第4条的原因：java在编译Integer i = 100 ;时，会翻译成为Integer i = Integer.valueOf(100)；，而java API中对Integer类型的valueOf的定义如下： 1234567public static Integer valueOf(int i)&#123; assert IntegerCache.high &gt;= 127; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)&#123; return IntegerCache.cache[i + (-IntegerCache.low)]; &#125; return new Integer(i);&#125; java对于-128到127之间的数，会进行缓存，Integer i = 127时，会将127进行缓存，下次再写Integer j = 127时，就会直接从缓存中取，就不会new了 2.什么时候用list什么时候用数组，linklist和arraylist的区别以及什么时候用link什么时候用array 当数据量固定时或者多维度时，我们可以使用数组，而当我们可以根据需要自动扩充，修改数据时，应该使用list linklist和arraylist的区别 ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。 对于随机访问get和set，ArrayList优于LinkedList，因为LinkedList要移动指针。 对于新增和删除操作add和remove，LinkedList比较占优势，因为ArrayList要移动数据。 3.集合有哪些接口，他们有什么区别 Collection接口：单列集合，两个子接口 ①List接口：有序可重复 LinkedList：基于链表实现，每个元素储存本身内存地址还储存下一个元素的地址。（增删快，查找慢） ArrayList：基于数组实现，每次增删都要重新创建新的数组，但数组有索引。（增删慢，查找快） Vector：基于数组，线程安全相关，效率低。 ②Set接口：不可重复 HashSet: 储存的元素无序，不可重复，底层是哈希表 LinkedHashSet：储存元素有序，不可重复，底层是哈希表和链表的结合 TreeSet：可以指定一个顺序，对象存入之后会按照指定的顺序排序。 2 . Map接口：双列集合 HashMap：非线程安全，高效，支持null LinkedHashMap：是HashMap的一个子类，保存了记录的插入顺序 HashTable：线程安全，低效，不支持null TreeMap：能够把他保存的记录根据键排序，默认是键值的升序排序 4.重载和重写的区别 方法重载Overload： 1、同一个类中 2、方法名相同，参数列表不同（参数顺序、个数、类型） 3、方法返回值、访问修饰符任意 4、与方法的参数名无关 1234567891011121314151617181920//重载public void eat()&#123; System.out.println(&quot;我是干饭人&quot; ); &#125;// public int eat()&#123; 会报错// return 4;// &#125; //报错原因：参数类型和个数一样,返回值类型不同是不算重载的 //因为在调用方法的时候,我们还不知道方法的返回值类型,所以编译器无法区分你调用的是哪个方法。 public void eat(String name)&#123; System.out.println(&quot;我是干饭人:&quot;+name ); &#125; public void eat(String name,int age)&#123; System.out.println(&quot;我是干饭人:&quot;+name+&quot;我今年&quot;+age ); &#125; 方法重写Override： 1、有继承关系的子类中 2、方法名相同，参数列表相同（参数顺序、个数、类型），方法返回值相同 3、访问修饰符，访问范围需要大于等于父类的访问范围 4、与方法的参数名无关 1234567891011121314//重载public class Father &#123; public void walk()&#123; System.out.println(&quot;我是父亲&quot;); &#125;&#125;public class Son extends Father &#123; @Override//方法重写 public void walk() &#123; System.out.println(&quot;我是儿子&quot;); &#125;&#125; 5.接口和抽象类的区别 A:成员的区别抽象类：构造方法：有构造方法，用于子类实例化使用。成员变量：可以是变量，也可以是常量。成员方法：可以是抽象的，也可以是非抽象的。 接口：构造方法：没有构造方法成员变量：只能是常量。默认修饰符：public static final成员方法：jdk1.7只能是抽象的。默认修饰符：public abstract (推荐：默认修饰符请自己永远手动给出)jdk1.8可以写以default和static开头的具体方法 B:类和接口的关系区别类与类：继承关系,只能单继承。可以多层继承。 类与接口：实现关系,可以单实现,也可以多实现。类还可以在继承一个类的同时实现多个接口。 接口与接口：继承关系,可以单继承,也可以多继承。 C:体现的理念不同抽象类里面定义的都是一个继承体系中的共性内容。接口是功能的集合,是一个体系额外的功能，是暴露出来的规则。 6.字符串转为json用什么接口 JSONObject jsonObject = JSON.parseObject(tt); fastjson 7.hashmap和hashset的区别及原理 HashSet是通过HasMap来实现的，HashMap的输入参数有Key、Value两个组成，在实现HashSet的时候，保持HashMap的Value为常量，相当于在HashMap中只对Key对象进行处理。HashMap的底层是一个数组结构，数组中的每一项对应了一个链表，这种结构称“链表散列”的数据结构，即数组和链表的结合体；也叫散列表、哈希表。 HahMap存储对象的过程如下①、对HahMap的Key调用hashCode()方法，返回int值，即对应的hashCode； ②、把此hashCode作为哈希表的索引，查找哈希表的相应位置，若当前位置内容为NULL，则把hashMap的Key、Value包装成Entry数组，放入当前位置； ③、若当前位置内容不为空，则继续查找当前索引处存放的链表，利用equals方法，找到Key相同的Entry数组，则用当前Value去替换旧的Value； ④、若未找到与当前Key值相同的对象，则把当前位置的链表后移（Entry数组持有一个指向下一个元素的引用），把新的Entry数组放到链表表头； HashSet存储对象的过程往HashSet添加元素的时候，HashSet会先调用元素的hashCode方法得到元素的哈希值 ， 然后通过元素 的哈希值经过移位等运算，就可以算出该元素在哈希表中 的存储位置。 情况1： 如果算出元素存储的位置目前没有任何元素存储，那么该元素可以直接存储到该位置上。 情况2： 如果算出该元素的存储位置目前已经存在有其他的元素了，那么会调用该元素的equals方法与该位置的元素再比较一次 ，如果equals返回的是true，那么该元素与这个位置上的元素就视为重复元素，不允许添加，如果equals方法返回的是false，那么该元素运行添加。 注意:现在Hashmap不是数组加链表实现，而是数组，链表，红黑树，链表节点数大于8时会从链表结构变成树结构。 123&gt;ConcurrentHashMap线程安全，锁部分Hashtable线程安全，锁全部上者效率更高 8.io有哪些类型 Java IO方式大体上可以分为三类，基于不同的io模型可以简单分为同步阻塞的BIO,同步非阻塞的NIO和异步非阻塞的AIO. BIO：Block IO 同步阻塞式 IO，就是咱们日常使用的传统 IO，它的特色是模式简单使用方便，并发处理能力低。 NIO：Non IO 同步非阻塞 IO，是传统 IO 的升级，客户端和服务器端经过 Channel(通道)通信，实现了多路复用。 AIO：Asynchronous IO 是 NIO 的升级，也叫 NIO2，实现了异步非堵塞 IO ，异步 IO 的操做基于事件和回调机制。 9.说说对耦合的理解 耦合就是模块与模块之间的联系程度 123高内聚，低耦合有什么好处呢？事实上，你会发现，短期来看，并没有很明显的好处，甚至会有一些人抱怨，“我这样写不行吗？有舍么问题？” “我写了十几年代码都是这样写的也没见有什么问题。”等等；还有个最重要的，这样甚至在短期内会影响系统的开发进度，因为高内聚，低耦合的系统对开发设计人员提出了更高的要求。高内聚，低耦合的好处体现在系统持续发展的过程中，高内聚，低耦合的系统具有更好的重用性，维护性，扩展性，可以更高效的完成系统的维护开发，持续的支持业务的发展，而不会成为业务发展的障碍。 123456789高内聚，低耦合是否意味着内聚越高越好，耦合越低越好？(内聚性，又称块内联系，指模块的功能强度的度量，即一个模块内部各个元素彼此结合的紧密程度的度量。)（1）并不是内聚越高越好，耦合越低越好，真正好的设计是在高内聚和低耦合间进行平衡，也就是说高内聚和低耦合是冲突的。（2）最强的内聚莫过于一个类只写一个函数，这样内聚性绝对是最高的。但这会带来一个明显的问题：类的数量急剧增多，这样就导致了其它类的耦合特别多，于是整个设计就变成了“高内聚高耦合”了。由于高耦合，整个系统变动同样非常频繁。（3）对于耦合来说，最弱的耦合是一个类将所有的函数都包含了，这样类完全不依赖其它类，耦合性是最低的。但这样会带来一个明显的问题：内聚性很低，于是整个设计就变成了“低耦合低内聚”了。由于低内聚，整个类的变动同样非常频繁。（4）真正做到高内聚、低耦合是很难的，很多时候未必一定要这样，更多的时候“最适合”的才是最好的，不过、审时度势、融会贯通、人尽其才、物尽其用，才是设计的王道。 10.8大基本数据类型 1234567891011121314151617181920212223242526272829303132333435bytebyte属于Java中的整型，长度为1字节8bit，取值10000000（-128）到 01111111（127），变量初始化默认值为0，包装类Byteshortshort属于Java中的整型，长度为2字节16bit，取值10000000 00000000（-32768）到 01111111 11111111（32767），变量初始化默认值为0，包装类Shortintint属于Java中的整型，长度为4字节32bit，取值-2^31 （-2,147,483,648）到 2^31-1（2,147,483,647），变量初始化默认值为0，包装类Integerlonglong属于Java中的整型，长度为8字节64bit，取值-2^63 （-9,223,372,036,854,775,808‬）到 2^63-1（9,223,372,036,854,775,8087），变量初始化默认值为0或0L，包装类Longfloatfloat属于Java中的浮点型，也叫单精度浮点型，长度为4字节32bit，变量初始化默认值0.0f，包装类Floatdoubledouble属于Java中的浮点型，也叫双精度浮点型，长度为8字节64bit，变量初始化默认值0.0d，包装类Doublecharchar属于java中的字符型，占2字节16bit，可以赋值单字符以及整型数值, 变量初始化无默认值，包装类Character。boolean在JVM中并没有提供boolean专用的字节码指令，而boolean类型数据在经过编译后在JVM中会通过int类型来表示，此时boolean数据4字节32位，而boolean数组将会被编码成Java虚拟机的byte数组，此时每个boolean数据1字节占8bit.--Java虚拟机规范仅有两个值true, false，变量初始化默认值false 11.==和equal的区别 equal与== 1.最大的区别是，==是运算符，equal是方法 简述几种情况下的equal与== java基本类型比较包装类型比较String类型比较对象1.java基本类型（short，int，long，byte，char，float，double，boolean） 比较基本类型，只能用==，不能用equal，这里的==比较的是两个变量的值 2.比较包装类型 ==比较的是内存地址，因为a和b是new出来的，是两个不同的对象，所以地址肯定是不同的，而equal比较的是值. 123456public boolean equals(Object obj) &#123; if (obj instanceof Integer) &#123; return value == ((Integer)obj).intValue(); &#125; return false;&#125; 3.比较String类型 ==比较的是内存地址，equal比较的是值 4.比较对象 ==和equal比较的都是内存地址，因为equal没有被重写，没有被重写的equal都是object的equal方法 12.String 与 StringBuffer 和 StringBuilder 的区别 String 是不可变的，而 StringBuffer 和 StringBuilder 是可变类。 StringBuffer 是线程安全和同步的，而 StringBuilder 不是。这就是 StringBuilder 比 StringBuffer 快的原因。 字符串连接运算符 (+) 在内部使用 StringBuilder 类。 对于非多线程环境中的字符串操作，我们应该使用 StringBuilder 否则使用 StringBuffer 类。 13.CopyOnWriteArrayList的底层原理 CopyOnWriteArrayList容器允许并发读，读操作是无锁的，性能较高。至于写操作，比如向容器中添加一个元素，则首先将当前容器复制一份，然后在新副本上执行写操作，结束之后再将原容器的引用指向新容器。 优点： 读操作性能很高，因为无需任何同步措施，比较适用于读多写少的并发场景。Java的list在遍历时，若中途有别的线程对list容器进行修改，则会抛出ConcurrentModificationException异常。而CopyOnWriteArrayList由于其”读写分离”的思想，遍历和修改操作分别作用在不同的list容器，所以在使用迭代器进行遍历时候，也就不会抛出ConcurrentModificationException异常了 缺点： 缺点也很明显，一是内存占用问题，毕竟每次执行写操作都要将原容器拷贝一份，数据量大时，对内存压力较大，可能会引起频繁GC；二是无法保证实时性，Vector对于读写操作均加锁同步，可以保证读和写的强一致性。而CopyOnWriteArrayList由于其实现策略的原因，写和读分别作用在新老不同容器上，在写操作执行过程中，读不会阻塞但读取到的却是老容器的数据。 12345678910111213141516171819public boolean add(E e) &#123; //ReentrantLock加锁，保证线程安全 final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; //拷贝原容器，长度为原容器长度加一 Object[] newElements = Arrays.copyOf(elements, len + 1); //在新副本上执行添加操作 newElements[len] = e; //将原容器引用指向新副本 setArray(newElements); return true; &#125; finally &#123; //解锁 lock.unlock(); &#125; &#125; 14.深拷贝和浅拷贝的区别对于基本数据类型，两者都是值传递 对于引用数据类型，浅拷贝是地址拷贝，而深拷贝是创建一个新的对象，将值复制进去","categories":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"java基础","slug":"java基础","permalink":"https://gwtt.github.io/tags/java%E5%9F%BA%E7%A1%80/"}]}],"categories":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/categories/Java/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/categories/%E9%9D%A2%E8%AF%95/"},{"name":"java知识","slug":"java知识","permalink":"https://gwtt.github.io/categories/java%E7%9F%A5%E8%AF%86/"},{"name":"数据库","slug":"数据库","permalink":"https://gwtt.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"算法","slug":"算法","permalink":"https://gwtt.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"兴趣","slug":"兴趣","permalink":"https://gwtt.github.io/categories/%E5%85%B4%E8%B6%A3/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gwtt.github.io/tags/Java/"},{"name":"个人理解","slug":"个人理解","permalink":"https://gwtt.github.io/tags/%E4%B8%AA%E4%BA%BA%E7%90%86%E8%A7%A3/"},{"name":"Springboot","slug":"Springboot","permalink":"https://gwtt.github.io/tags/Springboot/"},{"name":"AOP","slug":"AOP","permalink":"https://gwtt.github.io/tags/AOP/"},{"name":"Juc","slug":"Juc","permalink":"https://gwtt.github.io/tags/Juc/"},{"name":"框架","slug":"框架","permalink":"https://gwtt.github.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"面试","slug":"面试","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"函数接口","slug":"函数接口","permalink":"https://gwtt.github.io/tags/%E5%87%BD%E6%95%B0%E6%8E%A5%E5%8F%A3/"},{"name":"Redis","slug":"Redis","permalink":"https://gwtt.github.io/tags/Redis/"},{"name":"Mysql","slug":"Mysql","permalink":"https://gwtt.github.io/tags/Mysql/"},{"name":"MySQL","slug":"MySQL","permalink":"https://gwtt.github.io/tags/MySQL/"},{"name":"SQL","slug":"SQL","permalink":"https://gwtt.github.io/tags/SQL/"},{"name":"运维","slug":"运维","permalink":"https://gwtt.github.io/tags/%E8%BF%90%E7%BB%B4/"},{"name":"Linux","slug":"Linux","permalink":"https://gwtt.github.io/tags/Linux/"},{"name":"IO","slug":"IO","permalink":"https://gwtt.github.io/tags/IO/"},{"name":"java基础","slug":"java基础","permalink":"https://gwtt.github.io/tags/java%E5%9F%BA%E7%A1%80/"},{"name":"索引","slug":"索引","permalink":"https://gwtt.github.io/tags/%E7%B4%A2%E5%BC%95/"},{"name":"动态代理","slug":"动态代理","permalink":"https://gwtt.github.io/tags/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"},{"name":"面试， 动态规划","slug":"面试，-动态规划","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95%EF%BC%8C-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"Jvm","slug":"Jvm","permalink":"https://gwtt.github.io/tags/Jvm/"},{"name":"面试， 排序","slug":"面试，-排序","permalink":"https://gwtt.github.io/tags/%E9%9D%A2%E8%AF%95%EF%BC%8C-%E6%8E%92%E5%BA%8F/"},{"name":"指令","slug":"指令","permalink":"https://gwtt.github.io/tags/%E6%8C%87%E4%BB%A4/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://gwtt.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"游戏","slug":"游戏","permalink":"https://gwtt.github.io/tags/%E6%B8%B8%E6%88%8F/"},{"name":"方法句柄， JVM","slug":"方法句柄，-JVM","permalink":"https://gwtt.github.io/tags/%E6%96%B9%E6%B3%95%E5%8F%A5%E6%9F%84%EF%BC%8C-JVM/"},{"name":"Git","slug":"Git","permalink":"https://gwtt.github.io/tags/Git/"},{"name":"juc","slug":"juc","permalink":"https://gwtt.github.io/tags/juc/"},{"name":"消息队列","slug":"消息队列","permalink":"https://gwtt.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"分布式","slug":"分布式","permalink":"https://gwtt.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Mybatis","slug":"Mybatis","permalink":"https://gwtt.github.io/tags/Mybatis/"}]}